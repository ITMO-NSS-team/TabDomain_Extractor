{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06615fa1-6024-48b9-9c95-e19e8bf82c49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install openml pandas requests pydantic langchain tqdm retrying sentence_transformers ipywidgets rapidfuzz matplotlib numpy scikit-learn gensim seaborn faiss-cpu chardet langchain-core langchain-openai openai pymfe dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4748ec55-41b6-4262-ad05-bb8bace82b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import gc\n",
    "import shutil\n",
    "import warnings\n",
    "import requests\n",
    "import chardet\n",
    "import csv\n",
    "import pickle\n",
    "import joblib\n",
    "from typing import Dict, List, Any, Callable, Set, Tuple, Optional, DefaultDict\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from itertools import combinations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openml\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import HDBSCAN, KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, davies_bouldin_score, classification_report, \n",
    "    confusion_matrix, f1_score, accuracy_score, precision_score, \n",
    "    recall_score\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    KFold, train_test_split, StratifiedKFold, cross_val_predict, \n",
    "    cross_validate\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    LabelEncoder, PowerTransformer, StandardScaler\n",
    ")\n",
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from pymfe.mfe import MFE\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import faiss\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a3a068-1292-48a9-8422-9958c7090df5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d0ac03-23e4-405e-aa71-6ec6a9163191",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Embeddings of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee45725b-b31c-4d34-abf8-1e9ba481fa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удален кэш-файл: baseline_embeddings_cache.pkl\n",
      "Удален кэш-файл: classification_embeddings_cache.pkl\n",
      "Удален кэш-файл: domain_embeddings_cache.pkl\n",
      "Загрузка информации о датасетах...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 14:22:34,369 - INFO - Use pytorch device_name: cpu\n",
      "2025-08-14 14:22:34,369 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего папок в all_datasets: 258\n",
      "Успешно загружено датасетов: 258\n",
      "Уникальные домены:\n",
      "Computer Science & Artificial Intelligence    41\n",
      "Medical Science                               30\n",
      "Engineering & Technology                      22\n",
      "Sports & Recreation                           19\n",
      "Social Sciences                               18\n",
      "Finance & Economics                           17\n",
      "Synthetic Systems                             16\n",
      "Transportation Systems                        16\n",
      "Environmental Science                         16\n",
      "Computational Biology                         16\n",
      "Biology & Genetics                            15\n",
      "Physics & Mathematics                         14\n",
      "Materials Science                             14\n",
      "Veterinary Science                             4\n",
      "Name: count, dtype: int64\n",
      "Количество уникальных доменов: 14\n",
      "Загружено 258 датасетов\n",
      "Инициализация модели для создания эмбеддингов...\n",
      "Создание эмбеддингов для метода голосование...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6da601e74ac4a9c8bde3f5c64dc4e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944d8c56765f4713b5ebd8caf5919590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e48365cb7a840e99a1a4805ccdfbf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969fa46abe7d4319973ac040ffa94c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f01d15bc1f244b5a209e7866e89aee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ece5a967ce4f1dadde8a6c97178e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f80261d04034ce0b182e8277089823f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae85f2b3bdd5462a90a7b788473b2dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c52c891b814870a963edb935c83609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160b44c4eb2d42b6b7c9dc230a9e060a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6924cdc2ebfa4c3abf2b7d2eff8bfcdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec013f68dac479289225e8ae04e578c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa319270ffda46d887a2f4e7db4a1e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e363a23cd66443d4a69e8a264df3aee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d9d33f22fd4932b9876c2ef0d4bb27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909052b73e70430c88e8e771fae0ed9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f2cc1c52674c9591f14442b8802032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4295f08f7b4c3fb3e939f4be527426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4101b02a576b46438fbf62a93b8bcb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d154c5558d8e47a3b5880e2a8ba6c5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a32289d9fd8460daf54322dabb65f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783042089252427892794107bb25528a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e66834d6340416da264ee617dc5e32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d39315662494cceb11896440288f0ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec67a16e7133428abacfeffa3d7af2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ae047e42da4187b87bb0439838b5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7770ab2323e540f49e5e8698ce2ad6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5a564637004907bc45ea83b4d3adf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a42f74a11374039a52f538a87a4bb72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d53700c57e34565bd94e2ea2b5bea93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ce4a0d2c18478caac82979ee9e7882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6f821fe9824de89b5b090d0a1f3a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f66bce62cf1448baf09b545e95d886f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08aa9d96c8654bb5a79371182dd9f9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7b4bdcd6e84e46ba0316fceb94e032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7d0dcbf49240fd8d4b1984cf3945bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6079e4bfd64b4e55b635d9da7aa006ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01024dad2190442e9e6182f8e5ae4d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557c396306964d6cab90d0d21defd381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0be5425f804dbcbaccd410b381e1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e838e9ded74f6085b3465b8fee9607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655098acd851466d98515cabe02f3d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36fc6cd8426433daef2c16d8f01ca2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d521db945344bf9965371b015548b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c1d07a2ab0484d8c804d649f6f5da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bde08ed0834558b1566a0811b28e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec8a35b0a49486f8f8c13318fc2d572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c4079f764a46c0bd7f25a187b457f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33cc64b4766487fabd7ace867239ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d20bf9f57af44789157d8f48b6524db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127872833ed1475c9f669a4fea2eddcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e128bbd1dce44c6c83e045a63dad99cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159818505b47439d8c0ff87aeb25c4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f16bb49b4a405ebd04c55c278d3375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d8b36d99724bf7860cef537cb1511b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eee6dd249d04b0d93bf378d1e5df458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee3d4ec35e046c5920816a56a56242e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea7f3ac4ad0469ba8df3f36186c18cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3deb799ca9eb4d8da047852a6d84a2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65235ccf9c4549cfa6fd409bfadd2c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e47d5a5d724204b98f9cdd2584c3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bac8c41faef4b9ebcea0a14f336bec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dad49c5b7ca4b43b09c0596b47225e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b35f224f93a4f4dba91167cf3e9b234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc30fbdd17334a0d89e5b5f3b72a0917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5c3b98ade246d395335a5eb25bef43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd505fdf8e5a4eb9a2729a6597462212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7288f8812e64a44968e9c1cc91615fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d66d96f15547adbdf88469c83827d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5fa7a3b61cf4226a93c9d5ad1a5fb8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f7da2340b0417395fbc71ab958bec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc5c0ef562548b989344522c6bc3600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3fd9296a254bc8b1fb6aa1d4420497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce022617fc747c6b4ada0955522ea85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ff254ddbc5435fa644ef19878ed914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c405a3acf2449c81d23de371834527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62f3ab8cdbf4bd591310041fb226782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb15a07e7db749298207891a5edd89d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc8872c78f74a4ab064b26a143c3373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07420180c1754869a8b3e32ac32736ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214158ef41d84fca93db85dbc1794b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2551a0ab6cce4d4ba8b63288c0e51840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17862e5832be4a93b4db166fd6c6af05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35b37201f22480cb31d95775d43b172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48851ca11b88471fad46987e962a9193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e9bbbc99f24ebc9f8df3b378053fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad15841105943cb8e8f129a3fe98059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4f0a0c66824b8883b7fd941fdbecbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c24b2b9272a43e38dc0b207007cb00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ee3239ac1c4f63863b858815e3b9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6b1d6f9e9e46e89b545d2c9392bb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0b230e96344c318cb7dea78c141773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fa4b9213c848d4b22b5de8ffe26fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84de88f314b54e1192b0d455c492d0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521cd9b16ff24d8eb18603da4d5e4a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e652b30ae14beca601d6b09d076bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be14bb7526b442738ec853f1cbcedef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a91e1b91094b58b46fa07282979d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eec2720289e4b51b074e8d6fc51b316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d7da54967946dc9add818aa573bd9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e0a146c528400e91b7521333f1edb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c7eb3b1a3146bdb8b7069b159b1a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2c9753eaf24f95971616e32c4a063c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8db36d00384b14ad4baee0d9455e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe9f17f4bf04817982a7c2e9b9e2556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b86b61d1c4485abb71a86a6a55ab50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6192ab8db37d46b19005b6611f050619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696ff1a31da449c59940a9b71c5bbfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526ff11b90ca4d709f9d57193d7f4e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21d868137994403974c65bffdba6c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be98ea87bc5e453693ca3a139f78bb47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7273dc5f576340bb905817bc7db6b9be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c196275067924ccda36fbad7f18e2e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca919cf08cb64292b3c2efced25186f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2788f0f2ac2a43cf8813e6420193b4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff2305ad1b847a88ba241a910766f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1366722f43ef47f388b6ba33abdf9faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b87938ade0743d5af3160bc9520fad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3274797e89e4c72b61ce7b2eae84d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb26e09639f4d4e9b57629781e018e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec5df58db5f4fcd96cedc8d4dc41660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6b01f8b381458da0b96905ac458cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88b074915e64b259335b5066b726f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41336670af8149aab5bfcfbb93f14fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b80a5b8c424406081a894793a004301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66bfeae781ee4bdfabecbe7cbfec9f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7049f14300bf499d9f96a17633a28682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3433b8311def41dd80d2754d9389c272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af729ebf97e548ec8a7c80d4b2f2409e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd20d42cf7344ed922f35f1677cc697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447445abb1964275b39fb20bd21ca1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52c015157144df2809d07d56d4bbc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bbdabb229649cd91cba65f1082f172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75afecd5d7be4a18a598f79ea7c7a44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5fc49014d34002bf5dc7cde5b644ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83a193baae8447eba75cb9a30fdf157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767b2c5993db4ea29f84a01e2533a46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d8b6257b18419d966312949aae91f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba6a7ad408343038657dac92f866b87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d775e479b1a54596acffbf3a6ba5db9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68dc2788b9c844629429d7fe1f4078e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed92406ca4a44ab8780fb2221f42390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20877e9a5084aaba8dd9e604f758d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34120257bcd742ddadc7e1218706e5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a632f0aefddf4626acdfc78d8ca8d777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c134096860814939959e661d40440357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a9330638864482991566736d97edbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69b692fb01948a78de882814cbf7890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01eda13c1d3411bbc2683d950040a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cfcc3a5a6d4bea9c55276aa6cb1890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbe0ae2926544de91f6147b1ca46234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628c8d811da04d5594308dbab21bf74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1b6d30e155408cb3b8401c64f9da62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6785ec54a0384c5d877c8062e84493b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be9656507164bb48a3af2dcfb50ac7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f991693b64741faad814979c8c2078c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe5dada4bb2435281119480c8ef7964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c593fd8283d44917b3a5bfc74927214d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e964c62c90314650afaeac896c9bf5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c8e28c332045bfb6ce903daacd5319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b835e3962b084655a0d2020b7e7a24ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd03e83477a415f919d52723447c10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8e6d858cfc4503a1d05c70520dca82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8b7ebf292942bba34a3973c15ede7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2667408091294d84ba767b02514f6907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84e1e3843b140afad45fd816e91a604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6cec15b4f945b3b02bcbab504e655e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838cbe8a3a5c4fd3bf657722aac98afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f60ca2d98b54c0daf16be320cc04245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9c4fed49b444f8931e6fbc369e9499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f4ac1daf7247328381c7bb5d5fdc1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42790d7eefdb4be3b37ffe4a1fa355dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517d9b3a241246fe8295bfd1ebf54e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761697171a6946aa81925a1f21d18a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb6cddb8ade4979ab4b91cd46f9fb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1574a41a976543ec9a52460f043e1756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc47c050d4d44f89b446f14da774b26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6761e2b47f024782ac91efe38d2546e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bb6d99accf4f7aa58efa91dad9c249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9bd8e9abf94121aba99da45a4ad8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ca53ba55fe41a88ba0e826b05a4934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bcc2c70a8ad40fc998a3db625fb29e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e676cc838346dfb2faebb0d26d005e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f488956d2610454cb31c1b0bd22c5b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8273ae2b655a4b498a307b965868ab9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf59972e34f4c49aa3d87d101224a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316971f8ebfb42e89dfe7b5eda752fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3ae2bfb3f44ba2af5e3f7bcec82730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5d6ee37dec43e2922bffcda60b5104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bcb7a5730041fda99891f0e06e856e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837c6fe393054e5eabccd1ca8823e883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59df5c9f748f4ba393f28ec0adf08600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19faaaf14124904983eba29298c018e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4854e94fb6745b48aa537cbf5462405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d0d3b5e33c40a8820839bad8659894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f724ad0c6892482cbf737ed523c105d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980b6d6074e74a06b4254c583e3c679d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982dc8badeaa409582f48db74f9b826b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc96ded70704ed7b4bb9b9f81092fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223cc6f362234a26a1dfb25aec001858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e574617e6adc4716b4476a6f8cbec07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bbbe68b9d943348dd28bf6e66a53fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8937fcc9428845e09fc33694807faac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f834c21b5624afdbf99fe37749a912e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00d4219951e4e19b8f8e7ba92bdabb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0ab8eb9651404f85d8338edac18706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7985b5b9714044b68047112ccf9fff05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94576f07b60845048e65b8eaf8bb2a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082b1a93aca74e339632be11f4d908f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95cd3a004d1743e18ae6b374f3517f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34bba3e1d324ecba38908e295053b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947fd740e2fd4ff8a77b3aacecd5d836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644a270e3c1d4443ab34ea8ce61b136b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b139b06d51468e89d9e09ff4509e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ea63c837684a8eb2d2a361b4d2503c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e1cf7bad914e61900f13dfbee21649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7dfae5494074489b8fe18ccadb87e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1d05b7ed914000992dac128c9b8976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fb369128e04bc3b8e7e0ab39fc1607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ae2fc4963f495997db1fffd15fbc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513245e3c1bb4bc498e75b72d7433b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0136d9fa28343cc8b40901869b0bcfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9c27d8b8d840bcbc05cc02e59ba273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330f5d894f024ae9ab6b1d094c597974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15118b4245f4b7281acac18e5cd64bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95b553fac6d4199860306cda84c17aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e90f4435414a209ef76b3a74e37a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812fe30ae907498f8eeab9e88cabe7ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691c47ec09f54325bf3f31cf91ad6e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bde9839a5d54e1cb4b3097ada6566a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13bfa2b6cac2422e92eb32f6061f18f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2baf1285014b4b7fa13bb2a0228c151f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba403d35a5074e54b612fbf3df4a9a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054ebbdd5449489a9f995a0853a48880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34409fc8ad6442197e0b22522dd8bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef53897b902484db25325115630c3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b24d4af08b4d51b8640a575d96b513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae9f605b1cc47d8903cccb602e6fe7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57840d0b67d48ea8a66762eaaefbd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3462eb3507e64024bbe23668d896db9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae84a47a46db45cd9e1cd3e056fbf4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f784eaaf524840bd6e2166d318a439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a200cf0b2a2d43879baad68bfb2cfd4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0493ec19519c49cdb46855affc623e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c15bf2cfdfd4d5d9d62560145d53a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f97a613850458eac85edfe839fa4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e890aa77c3cf496bb087c50ec2b008df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5842c8d66cd54120b436f6ae7d6e6d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e2e07351a343e0bdb1b7278f94aea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9c4cf338ae4edfa22289130696e76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cccfa01042124c19965c441e91e210ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e779837abe154137afedbda697a37de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a16bb95fdfd4b4a87c41fcc431faa39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3806fb7166264029a6102eac94f9a4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa8ffa1bd8e426aa3f639403eee0c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf778e210d84f7191ce852422b6e2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41117b0a03154914a54eea7bc6634cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853585ade91d4852923fc999593e8639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76167b2c9dc844a0ad43005f1a881ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Подготовка данных для оценки метода голосование\n",
      "\n",
      "Оценка метода голосование:\n",
      "Несоответствия доменов (истинный vs. бейзлайн): 185 из 258\n",
      "Примеры несоответствий:\n",
      "Dataset 10: Истинный=Medical Science, Предсказанный (бейзлайн)=Computational Biology, Columns=lymphatics, block_of_affere, bl_of_lymph_c, bl_of_lymph_s, by_pass, extravasates, regeneration_of, early_uptake_in, lym_nodes_dimin, lym_nodes_enlar, changes_in_lym, defect_in_node, changes_in_node, changes_in_stru, special_forms, dislocation_of, exclusion_of_no, no_of_nodes_in, class\n",
      "Dataset 11: Истинный=Synthetic Systems, Предсказанный (бейзлайн)=Medical Science, Columns=left-weight, left-distance, right-weight, right-distance, class\n",
      "Dataset 1100: Истинный=Social Sciences, Предсказанный (бейзлайн)=Medical Science, Columns=Gender, Grade, Age, Race, Urban/Rural, School, Goals, Grades, Sports, Looks, Money\n",
      "Dataset 115: Истинный=Computer Science & Artificial Intelligence, Предсказанный (бейзлайн)=Engineering & Technology, Columns=att1, att2, att3, att4, att5, att6, att7, att8, att9, att10, att11, att12, att13, att14, att15, att16, att17, att18, att19, att20, att21, att22, att23, att24, att25, att26, att27, att28, att29, att30, att31, att32, att33, att34, att35, att36, att37, att38, att39, att40, att41, att42, att43, att44, att45, att46, att47, att48, att49, att50\n",
      "Dataset 116: Истинный=Engineering & Technology, Предсказанный (бейзлайн)=Transportation Systems, Columns=IDENTIF, RIVER, LOCATION, ERECTED, PURPOSE, LENGTH, LANES, CLEAR-G, T-OR-D, MATERIAL, SPAN, REL-L, TYPE\n",
      "\n",
      "Метрики бейзлайна:\n",
      "Accuracy: 0.28 ± 0.00\n",
      "F1-Score (Macro): 0.23 ± 0.00\n",
      "F1-Score (Weighted): 0.24 ± 0.00\n",
      "\n",
      "Создание эмбеддингов для классификации...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e55466a9386427c8a51fbdc0289a491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5540d8b9a3df4cce81d59a531cc19d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd0337fe7f5460ab0b8364a45f818ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b81fb7de5f3463fb5278681af117a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80060c17f8a445e2a6b79a89f1eb8c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36ecc138fd548619b04c4c445f68bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf02486a55c647bcacd26ea497b40f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81e25c0a5174de898f65920635276a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29c0a40824047fa953a35b7463f4a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895b81871d5f44f5a173746dec44779b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b40edaea4148d7a7703ab833058aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e38b6f39294a26b9d58a633d57f83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f663167d234c94bd9f2bc1f368b055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7ae46b487b413a92ebeb60ee9bdfa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb512d48b79542b09561910777971026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d875d5724ce40d2b2f95ecc9a183d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44987dc18dc546d88a6e732b7b7c501f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ae4a38cc47418e8bbb23f1c6e923f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4241b76544154d13b072c7131e076a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4170e24b32a94600a28e1cc09d483d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523c93d64b054452814f1ee51cb5250d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0275c9e5f84208a30f257ad238cb36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587c5f51c33240c8bba058ffb7bec647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2989e15f5a9a4b3598abaeb9fa41bfe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03efcea3ac1e4f978225e259c0b0e3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48a1ba7af2a4010bc42c5a7d1acdaaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7748bc7481948dd9d18d5824004155c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd55d50aad5c4b06a76016fee644b99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aef6731a84544efa1644dd9e9ed1984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb81bd6c4621407bb05c829b74a7c1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccdf2a1880b744bf8eb47144c941bd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e498f3fbd24b988a04044cbd5c3330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2e05fa06b74ebfb2859024357ae585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a9e53818764e929922c25fcef9036b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b19c1eb6fb84b428da5669dd3054755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d8b5a6261249f29c381e4b6675dc67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7b011d49f547a2bc61a5dbfcc41d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b07d8a1075e46749ce7d2beca689487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca017baadbdb4afabe2cf08f062b9ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db6ec15be3e494baf4f15c67f03abf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fda9a63496403b87a6b042424ed5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee490d2221cb40399f99a9e1f3cd4d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa32dcf7ea24a2fbb4e2a0e32c02a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aecd421276f4c078893931ad45a9cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4872ca58328a4ddeaf1216f54f93ac22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3360b7db41414d0fb77f995492c4fda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9253ba9d92de4267b388544bde277b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfac9b7ec956499f94947817a2a874d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb22807b4c984adea8d60f2925311bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f9ba0f22ff49efa5048a0f3b9023a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278001c0d3bc4904b7b5b41ccd085a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6205a0479d4ba494de513aedc770d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fce8865b1f476b9c1bca228ff5c9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f773c1dd2749f79dcadc1358704f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d979654ecc8d4a87809ac74090241354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790a1daee1354c7b8ec0f836b2c7ffe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7497ced12c704434aab31aca73925aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5437f0c818be4106b864abcd215f0c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2645e1fe34c245658307a9d4a1a9a6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76b6ce1e1ec4a01b983f780ab7a36e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0768483b3a2b4e65bbc5c8c547678676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba62ec83f644ee5b3907785fca23e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db5fdc9e1f745649af4464a8d6ac035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0389e2e5d0d04119870031a0ef1275e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54aa7a6a5c14723b4ae59b1c54121cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5f5c8af59042a394288a2e9b220726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc98561e3d3c48e190a8c91ede32acb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e599627210ea424eaa1bc04bcaabdf8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fad560812ed4fb18a6a33ee0fa8e49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169bbe83eab24b47b07b0640cc1a0913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0080cf8165484b0e881257544a098f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b03eb421d99449e8a29a6803468c9ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1775662b6daa48f587ae0752ebe96af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86080a3d500c4ea2ba2214fabc81c229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43acd14ec974ff4a50c2df707d6e0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a927b3a0e8d14935a483ce99a71c4f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defe2c2664b14ac49167da6316fa60c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e54794f05145b6a905c4e440fbc0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f98af03bec4b8e9fee280f6070a7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07965f39051a480ea65c970cb3a2a5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cfdeab65704fa8bd7b40f82b0eab05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bf47c2b5014b13942bf0c68a18cab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9567091c4c504304898210edd955c5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980c1ae193fa4329955fbcc9d6e055bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de9d8f21b634ed7806433482636a74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec343f134474ac89f753e2461a82dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673299a7c7cd4626b57ec0f78cd29ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96627b97f26e4ef885714a0878a7a6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d01924e9f784b2a988371f13c3a424e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6530c5e769d74bf3b56849d67c57ba22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35974c4d62c549dd854ad89158049ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b3ab5c7fbe4900b1e6e99ca3baa86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e24f11cdff427e883d932c896bbb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1248936ccc540d4bb7b3cd42e2e82c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dc54d2259d4cb098ebde1ee6494086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6094687f33414d189b49c8e821c6ce63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a90e9c76b89405d8a615975cbe549f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0ce275f48b4b7990098831f935917d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6559d58e438e4243a1680f53c18c56cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439a693b92f44ea99014975cf12e61ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4415b41a3c31441f8c53537cdd30a1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae85ef5aafc4aa4bd8df8b87aabc186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09ba078bc26457bba6dd8f6a9833bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1edfc43554a4facbcd48ce20780cd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df34e2f75f4f49978f4b39b26790c603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc4b1d52e3a4cf9bca9fec92dafc797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54c1c6966d8474780df55ddbff510f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb73bc6d76443589792d89f7e97e679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4758f3a37a4a139527f4e3a0d594f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9693d588f6740ef874354ebfa179ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcaffd78e4844d1ac9f7c70ddd62edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912bd464cebb4b7ea0085526e10d8150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040c91e0d1974cee93c3bad935d25b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bebf228630e4c8d9a53c042360204b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50334532bdeb415d8e3c665f106c7dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026f6ab53c034058a292e8b2e33497c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55669f54fce452baf1fe554a7708361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e474f5ae211842958d77f37a1657d313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe5e13cc76d4460ab316e6ca1807180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69765c919e34f7898f0022440c55df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70bf023c1a14d979dfe00b3bc528649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7cf8b2a5524f04819d29dd7274d91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ebc5167816e4d5ba36dde56be47f58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5f331b9153400aa55fd976098d4f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ceb8b572d54cde863c6be58a298cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21082f6b9e5d4869a9df812fd52ce7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c064a8c982494b3884a8ed3f2090c163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6965ef2b5a954262bda381df62894071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5a8cc1e51b415c85942ba0708e85f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a198a9fcc946799131aa0f8f5d89d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e0a96416ac4c0eb0a2eb9ce75a20b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b13d8addfb4d07b4eff4d07dfdf188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf0673d5cbb4d14854cc4b8e270f4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4230d1ba51b64e5a869325d82eb3a8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca84192d66c40ae8f6fa7dbb1009107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9dc6db13444c389bd77ad864a9c015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d7662f67d445b4bc5c9b3b7141e529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ee1559b70441459bad9e72bea3197a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353304bf8b7a45ce98e6fe2a6f08de39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de33f708ee64a51ba5684e0ff067d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb6267c8ef1425194f6a0432598c0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef476b529eb4e74a269d6d72339363c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc01c12d71241b293200c4898fa5b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0b8bbc7cec4444a668d997e30edf6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4339d8d2fc4c0493c5d6144bb4ef6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ac7be368754ee1a81bebd4c9c64b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc571d59cada4b11aa11a56787310840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9e50d2c40948d3b80fb4f6f5aa3895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e664594bcf345aab865f632f50f5022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98b0d7b1f8341febf3bfc696085f3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0dff2fe1a6d4f33b487d5221471b655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4714761a2b0c4b9ea5b353560a2002fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64f8f7ee03646a09e59de272630621e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a313e406f904d8c8952bd9a67f69f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1ee4a306204b2e8c4cb72d05d1f613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccee687c5d54244b95626d4e13b663d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9df440b19df41b7b5d5ef1f341a13b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb093ac09114ab2895e0961ebdc5757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f2657df1954d2d8d02e93fff77f32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee60979123440e381a85c6c92c24a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4025bc4627a492dbac825d037dc2cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596fcd0f55cb4740a4a1268706aa86c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a3a09b42934f61b4e35f971a61c9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d5324db563482c8d6e2765fd15b105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b76d45ee92b4e8ca7f811104a118628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15bb3587b6b493d8c0b1ffa60ab172b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72990e0aa97c45a4acb74dbf0e9df6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457c2399f7734240826fe6442c11375a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d4e0aa0c9f4e588cb9d8f72f86cf46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534fa63501044e1d92be13fe3e4ed26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6479a007af0742f395d5c0506300c682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b517df346d9c49c79db1ee12c3f226c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70108fd3b25244bea89b70c56423a30b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4f351d9c09425bbf8103f3b0e26ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe87f53cb304bc8a0ea8d7834f4a80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb417f1f9f34329a16d947922f8617d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5400d646c14bb58b0fa431ec8a3f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0947380d0f3a49149e4104141e11bda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f33a39e84746b2b603f4463e79e368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044b2a96c09147b69866813708fbd64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ca5fb62a9449669f42a24af0d4cad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9479bbbec08647388129729506cf086e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71b9ac0cc7e43fd8586c503cb35577b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880cc45d26c94e57a77c3b8569d756d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1928a1590b74241be7b300997a6e9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4cd98f75587473fb1a496ffcdeb90d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5a453b7d4e45e499e7dd6c7ac36867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cebd7903f434862bb8a1d2dbb4f608b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5432c35041414580b417ce887009e7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc20f2b7a39345a7ab78033d3e457472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57e59d4fc324956bbc68812d514c228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d6387b03a84334af36b7107231ae2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c10421693a48f3870f638916f0f243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfd66dad27c45f3947c40fe8c6edce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955cc2c9c74d4b808c099508da1de0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a20fab345b742359b0e8bb3f96be8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3594d1eb40ae481daffc15fb4926a561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a776a42f8a9498282933da815a3bb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609725d565ba411a829837a30630fee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36347f4968274b7aa759fa10f1028d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96eb6d76946a46ebaee9829d8ba49afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0894f740232e40739593e214ad4df8a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286df4a8a8ba47968b04a6d0317cc516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6687dc0f009746d492744de76fa8d3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e36dbb707a4a5bb201025473a30150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7126a617213d4d4a869799d4b036b8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181b6e79548d4c77a48ce3b0f71b8ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a6db2745dc49fc9eb882ed60701e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4622d9272fb347f2b1e85199d87f19ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8534142b51aa4da492aa07aa337fdc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e00bce18f54a18bf9c5742a4a38744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ff3b1a56e7452c83784a9bb965d2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a8e50450b44ade89232b110a60bbe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ecdabba4d74b7abddf1bcb93490a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d4963cd4b2467ca4182317c1b33b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6388ee26904e4a768b404e4ab2b74d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5fcfa769f542d180415e2c1456a4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05afcb25914f4b2a9ea93f312680ca0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46912844694d42d69da9f8c3b6e1a3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1782cbf1243a4c99afd83f44a191ddfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd53201065041bbaabd390e8d91e55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc178f419df4ab3a755f3b623109977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7fd3cfe8344f12a8e63a5cc0f176c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0d281ad7ec479385aa3f38069ade5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ad030d96034f528c661e6365695de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce999b1e8e04832a9e7f763d688af01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ce3ec9402d4b09a0e1d9d578f8ab85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d89485002484c21b8255d807f5f6f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3b9a16202a4967b08462f01ec9c258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ba605e881f4bcf82f7dd12225eff4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e797b4c4f0d440e805633aac3d45c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f39788d7284e0e98db685dfd0ccd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc79495e34c4511b3cefd385bf6f093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73825ceae3ee4efc96787603e4b5e3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37989e38f324806970eaa646d6bdc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b0804c386a4335b6084158428e45fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f080b3561431453db8dbe56a6b8b6c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f32d170f0054957a5010459ebb38770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e643fee1ec458688d168d9a6a9e3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5adfac7d2d64ca48a85a80a970f4dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93292a0c2f124073b67cdb526d8b2a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572203d85cec402c9d20857fba2ad91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8510f9e36ae94e9ebfed70d6afcf4f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f032df2cd141139e680efbc20c6a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f57e0ea3af45e1a33d85c1ddd314ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b17a25a39c4d1ba9709b9b1643073d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20aeaeaf1916491b8f612a4d47879a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37018250c400412a9bd70c580412fe5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5b63393a7a4e9d87217a45d90c0a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bfc6f9a11d457f9917d2e12a33d87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9677af4d466b437ba1572c31aef3a5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f83dc1f3bd4c098d76dbe95ace7cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f764dd93df2d48739906572eddec0318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945fcdb343d94c868f58daced4c9806d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf75639b9df462596811d91784fa49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c971bc4d0a144f6ba9b5d0f51aa0f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4225cdc31dd04705b9e9e2443a1979fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecadbf0047f434a86f3c424a981a9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подготовка данных для классификации\n",
      "\n",
      "Распределение доменов:\n",
      "Computer Science & Artificial Intelligence    41\n",
      "Medical Science                               30\n",
      "Engineering & Technology                      22\n",
      "Sports & Recreation                           19\n",
      "Social Sciences                               18\n",
      "Finance & Economics                           17\n",
      "Synthetic Systems                             16\n",
      "Transportation Systems                        16\n",
      "Environmental Science                         16\n",
      "Computational Biology                         16\n",
      "Biology & Genetics                            15\n",
      "Physics & Mathematics                         14\n",
      "Materials Science                             14\n",
      "Veterinary Science                             4\n",
      "Name: count, dtype: int64\n",
      "Количество уникальных доменов: 14\n",
      "\n",
      "Обучение моделей и кросс-валидация\n",
      "Обучение и кросс-валидация модели: RandomForest\n",
      "Accuracy: 0.61 ± 0.04\n",
      "F1-Score (Macro): 0.54 ± 0.06\n",
      "F1-Score (Weighted): 0.59 ± 0.04\n",
      "Обучение и кросс-валидация модели: LogisticRegression\n",
      "Accuracy: 0.48 ± 0.06\n",
      "F1-Score (Macro): 0.47 ± 0.07\n",
      "F1-Score (Weighted): 0.47 ± 0.07\n",
      "Обучение и кросс-валидация модели: SVM\n",
      "Accuracy: 0.55 ± 0.08\n",
      "F1-Score (Macro): 0.54 ± 0.08\n",
      "F1-Score (Weighted): 0.53 ± 0.11\n",
      "Обучение и кросс-валидация модели: KNN\n",
      "Accuracy: 0.48 ± 0.09\n",
      "F1-Score (Macro): 0.43 ± 0.07\n",
      "F1-Score (Weighted): 0.46 ± 0.09\n",
      "\n",
      "Сравнение бейзлайна и моделей:\n",
      "             Model    Accuracy Precision (Macro) Recall (Macro) F1-Score (Macro) Precision (Weighted) Recall (Weighted) F1-Score (Weighted)\n",
      "            Voting 0.28 ± 0.00       0.31 ± 0.00    0.26 ± 0.00      0.23 ± 0.00          0.39 ± 0.00       0.28 ± 0.00         0.24 ± 0.00\n",
      "      RandomForest 0.61 ± 0.04       0.58 ± 0.07    0.60 ± 0.05      0.54 ± 0.06          0.64 ± 0.04       0.61 ± 0.04         0.59 ± 0.04\n",
      "LogisticRegression 0.48 ± 0.06       0.51 ± 0.09    0.53 ± 0.05      0.47 ± 0.07          0.53 ± 0.10       0.48 ± 0.06         0.47 ± 0.07\n",
      "               SVM 0.55 ± 0.08       0.56 ± 0.12    0.60 ± 0.07      0.54 ± 0.08          0.57 ± 0.15       0.55 ± 0.08         0.53 ± 0.11\n",
      "               KNN 0.48 ± 0.09       0.46 ± 0.08    0.47 ± 0.04      0.43 ± 0.07          0.51 ± 0.07       0.48 ± 0.09         0.46 ± 0.09\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Лучшая модель: RandomForest\n",
      "Лучшая модель сохранена как column_embedding_classifier.pkl\n",
      "\n",
      "Найдено ошибок классификации (RandomForest): 101 из 258\n",
      "\n",
      "Примеры ошибок классификации:\n",
      "\n",
      "Dataset 10:\n",
      "  Истинный домен: Medical Science\n",
      "  Предсказанный домен (модель): Biology & Genetics\n",
      "  Предсказанный домен (бейзлайн): Computational Biology\n",
      "  Columns: lymphatics, block_of_affere, bl_of_lymph_c, bl_of_lymph_s, by_pass, extravasates, regeneration_of, early_uptake_in, lym_nodes_dimin, lym_nodes_enlar, changes_in_lym, defect_in_node, changes_in_node, changes_in_stru, special_forms, dislocation_of, exclusion_of_no, no_of_nodes_in, class\n",
      "\n",
      "Dataset 11:\n",
      "  Истинный домен: Synthetic Systems\n",
      "  Предсказанный домен (модель): Transportation Systems\n",
      "  Предсказанный домен (бейзлайн): Medical Science\n",
      "  Columns: left-weight, left-distance, right-weight, right-distance, class\n",
      "\n",
      "Dataset 1141:\n",
      "  Истинный домен: Biology & Genetics\n",
      "  Предсказанный домен (модель): Computer Science & Artificial Intelligence\n",
      "  Предсказанный домен (бейзлайн): Biology & Genetics\n",
      "  Columns: 1007_s_at, 121_at, 1405_i_at, 1438_at, 1552257_a_at, 1552309_a_at, 1552348_at, 1552349_a_at, 1552365_at, 1552378_s_at, 1552426_a_at, 1552455_at, 1552463_at, 1552594_at, 1552610_a_at, 1552615_at, 1552621_at, 1552622_s_at, 1552628_a_at, 1552701_a_at, 1552703_s_at, 1552715_a_at, 1552735_at, 1552767_a_at, 1552797_s_at, 1553062_at, 1553103_at, 1553105_s_at, 1553132_a_at, 1553179_at, 1553185_at, 1553186_x_at, 1553454_at, 1553530_a_at, 1553538_s_at, 1553551_s_at, 1553567_s_at, 1553569_at, 1553570_x_at, 1553575_at, 1553582_a_at, 1553588_at, 1553589_a_at, 1553602_at, 1553613_s_at, 1553655_at, 1553678_a_at, 1553693_s_at, 1553715_s_at, 1553718_at\n",
      "\n",
      "Dataset 119:\n",
      "  Истинный домен: Medical Science\n",
      "  Предсказанный домен (модель): Social Sciences\n",
      "  Предсказанный домен (бейзлайн): Medical Science\n",
      "  Columns: Wifes_age, Wifes_education, Husbands_education, Number_of_children_ever_born, Wifes_religion, Wifes_now_working%3F, Husbands_occupation, Standard-of-living_index, Media_exposure, Contraceptive_method_used\n",
      "\n",
      "Dataset 1192:\n",
      "  Истинный домен: Engineering & Technology\n",
      "  Предсказанный домен (модель): Transportation Systems\n",
      "  Предсказанный домен (бейзлайн): Transportation Systems\n",
      "  Columns: symboling, normalized-losses, make, fuel-type, aspiration, num-of-doors, body-style, drive-wheels, engine-location, wheel-base, length, width, height, curb-weight, engine-type, num-of-cylinders, engine-size, fuel-system, bore, stroke, compression-ratio, peak-rpm, city-mpg, highway-mpg, price, class\n",
      "\n",
      "Сохранение результатов...\n",
      "Результаты классификации и бейзлайна сохранены в classification_results.csv\n",
      "Метрики сохранены в comparison_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = \"all_datasets\"\n",
    "MODEL_SAVE_PATH = \"column_embedding_classifier.pkl\"\n",
    "OUTPUT_DIR = \"results\"\n",
    "EMBEDDING_METHOD = \"avg\"  \n",
    "N_SPLITS = 5 \n",
    "\n",
    "def load_dataset_info():\n",
    "    \"\"\"Загрузка информации о датасетах и их доменов с отладкой\"\"\"\n",
    "    dataset_info = {}\n",
    "    skipped_datasets = []\n",
    "    domains = []\n",
    "    \n",
    "    for dataset_id in os.listdir(INPUT_DIR):\n",
    "        csv_path = os.path.join(INPUT_DIR, dataset_id, \"random_rows.csv\")\n",
    "        meta_path = os.path.join(INPUT_DIR, dataset_id, \"metadata.json\")\n",
    "        \n",
    "        if not os.path.exists(csv_path):\n",
    "            skipped_datasets.append((dataset_id, \"No file random_rows.csv\"))\n",
    "            continue\n",
    "        if not os.path.exists(meta_path):\n",
    "            skipped_datasets.append((dataset_id, \"No file metadata.json\"))\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            with open(meta_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "                domain = metadata.get('domain', 'Other')\n",
    "                domains.append(domain)\n",
    "        except Exception as e:\n",
    "            skipped_datasets.append((dataset_id, f\"Ошибка чтения metadata.json: {e}\"))\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, nrows=0)\n",
    "            columns = list(df.columns)\n",
    "        except Exception as e:\n",
    "            skipped_datasets.append((dataset_id, f\"Ошибка чтения sample.csv: {e}\"))\n",
    "            continue\n",
    "        \n",
    "        dataset_info[dataset_id] = {\n",
    "            'columns': columns,\n",
    "            'domain': domain\n",
    "        }\n",
    "    \n",
    "    print(f\"Всего папок в {INPUT_DIR}: {len(os.listdir(INPUT_DIR))}\")\n",
    "    print(f\"Успешно загружено датасетов: {len(dataset_info)}\")\n",
    "    if skipped_datasets:\n",
    "        print(\"Пропущенные датасеты:\")\n",
    "        for ds_id, reason in skipped_datasets:\n",
    "            print(f\"  {ds_id}: {reason}\")\n",
    "    print(\"Уникальные домены:\")\n",
    "    domain_counts = pd.Series(domains).value_counts()\n",
    "    print(domain_counts)\n",
    "    print(f\"Количество уникальных доменов: {len(domain_counts)}\")\n",
    "    \n",
    "    return dataset_info, domains\n",
    "\n",
    "def voting_baseline_predict(columns, embedding_model, domain_embeddings, unique_domains, max_columns=50):\n",
    "    \"\"\"\n",
    "    Предсказывает домен датасета методом голосования на основе названий столбцов\n",
    "    \n",
    "    Параметры:\n",
    "        columns (list): Список названий столбцов\n",
    "        embedding_model: Модель для создания эмбеддингов\n",
    "        domain_embeddings (np.array): Матрица эмбеддингов доменов\n",
    "        unique_domains (list): Список уникальных доменов\n",
    "        max_columns (int): Максимальное количество столбцов для обработки\n",
    "        \n",
    "    Возвращает:\n",
    "        tuple: (feature_vector, predicted_domain)\n",
    "    \"\"\"\n",
    "    # Ограничиваем число столбцов\n",
    "    columns = columns[:max_columns]\n",
    "    \n",
    "    # Кодируем названия столбцов\n",
    "    col_embeddings = embedding_model.encode(columns, batch_size=128)\n",
    "    \n",
    "    # Нормализация эмбеддингов для косинусного сходства\n",
    "    col_embeddings = col_embeddings / np.linalg.norm(col_embeddings, axis=1, keepdims=True)\n",
    "    domain_embeddings_norm = domain_embeddings / np.linalg.norm(domain_embeddings, axis=1, keepdims=True)\n",
    "    \n",
    "    # Создание FAISS индекса\n",
    "    index = faiss.IndexFlatIP(domain_embeddings_norm.shape[1])\n",
    "    index.add(domain_embeddings_norm)\n",
    "    \n",
    "    # Поиск ближайших доменов для каждого столбца\n",
    "    _, indices = index.search(col_embeddings, 1)\n",
    "    nearest_domains = [unique_domains[idx[0]] for idx in indices]\n",
    "    \n",
    "    # Подсчет голосов\n",
    "    domain_counts = Counter(nearest_domains)\n",
    "    feature_vector = np.array([domain_counts.get(d, 0) / len(columns) for d in unique_domains])\n",
    "    most_common_domain = domain_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return feature_vector, most_common_domain\n",
    "\n",
    "def generate_embeddings(dataset_info, embedding_model, domains, method=\"avg\", cache_file=\"embeddings_cache.pkl\", max_columns=50):\n",
    "    \"\"\"Создание эмбеддингов датасета: усредненные или на основе долей доменов\"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Загрузка эмбеддингов из кэша: {cache_file}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            return pickle.load(f), list(set(domains))\n",
    "    \n",
    "    unique_domains = list(set(domains))\n",
    "    embeddings = {}\n",
    "    \n",
    "    if method == \"voting\":\n",
    "        # Загрузка/создание эмбеддингов доменов\n",
    "        domain_cache_file = \"domain_embeddings_cache.pkl\"\n",
    "        if os.path.exists(domain_cache_file):\n",
    "            with open(domain_cache_file, 'rb') as f:\n",
    "                domain_embeddings = pickle.load(f)\n",
    "        else:\n",
    "            domain_embeddings = embedding_model.encode(unique_domains, batch_size=128)\n",
    "            with open(domain_cache_file, 'wb') as f:\n",
    "                pickle.dump(domain_embeddings, f)\n",
    "    \n",
    "    for ds_id, info in dataset_info.items():\n",
    "        try:\n",
    "            columns = info['columns']\n",
    "            if method == \"avg\":\n",
    "                # Обработка для усредненных эмбеддингов\n",
    "                col_embeddings = embedding_model.encode(columns[:max_columns], batch_size=128)\n",
    "                feature_vector = np.mean(col_embeddings, axis=0)\n",
    "                embeddings[ds_id] = {\n",
    "                    'embedding': feature_vector,\n",
    "                    'predicted_domain': None,\n",
    "                    'domain': info['domain']\n",
    "                }\n",
    "            elif method == \"voting\":\n",
    "                # Используем новую функцию для voting\n",
    "                feature_vector, most_common_domain = voting_baseline_predict(\n",
    "                    columns=columns,\n",
    "                    embedding_model=embedding_model,\n",
    "                    domain_embeddings=domain_embeddings,\n",
    "                    unique_domains=unique_domains,\n",
    "                    max_columns=max_columns\n",
    "                )\n",
    "                embeddings[ds_id] = {\n",
    "                    'embedding': feature_vector,\n",
    "                    'predicted_domain': most_common_domain,\n",
    "                    'domain': info['domain']\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка создания эмбеддинга для {ds_id}: {e}\")\n",
    "    \n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "    return embeddings, unique_domains\n",
    "\n",
    "def train_models(X, y):\n",
    "    models = {\n",
    "        'RandomForest': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        'LogisticRegression': LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        'SVM': SVC(\n",
    "            kernel='rbf',\n",
    "            probability=True,\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "    }\n",
    "    \n",
    "    cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"Обучение и кросс-валидация модели: {name}\")\n",
    "        \n",
    "        acc_scores = []\n",
    "        prec_macro_scores = []\n",
    "        rec_macro_scores = []\n",
    "        f1_macro_scores = []\n",
    "        prec_weighted_scores = []\n",
    "        rec_weighted_scores = []\n",
    "        f1_weighted_scores = []\n",
    "        \n",
    "        y_pred_all = np.zeros_like(y)\n",
    "        \n",
    "        for train_idx, test_idx in cv.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            y_pred_all[test_idx] = y_pred\n",
    "            \n",
    "            acc_scores.append(accuracy_score(y_test, y_pred))\n",
    "            prec_macro_scores.append(precision_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "            rec_macro_scores.append(recall_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "            f1_macro_scores.append(f1_score(y_test, y_pred, average='macro', zero_division=0))\n",
    "            prec_weighted_scores.append(precision_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "            rec_weighted_scores.append(recall_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "            f1_weighted_scores.append(f1_score(y_test, y_pred, average='weighted', zero_division=0))\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'y_pred': y_pred_all,\n",
    "            'metrics': {\n",
    "                'Accuracy': {'mean': np.mean(acc_scores), 'std': np.std(acc_scores)},\n",
    "                'Precision (Macro)': {'mean': np.mean(prec_macro_scores), 'std': np.std(prec_macro_scores)},\n",
    "                'Recall (Macro)': {'mean': np.mean(rec_macro_scores), 'std': np.std(rec_macro_scores)},\n",
    "                'F1-Score (Macro)': {'mean': np.mean(f1_macro_scores), 'std': np.std(f1_macro_scores)},\n",
    "                'Precision (Weighted)': {'mean': np.mean(prec_weighted_scores), 'std': np.std(prec_weighted_scores)},\n",
    "                'Recall (Weighted)': {'mean': np.mean(rec_weighted_scores), 'std': np.std(rec_weighted_scores)},\n",
    "                'F1-Score (Weighted)': {'mean': np.mean(f1_weighted_scores), 'std': np.std(f1_weighted_scores)}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy: {results[name]['metrics']['Accuracy']['mean']:.2f} ± {results[name]['metrics']['Accuracy']['std']:.2f}\")\n",
    "        print(f\"F1-Score (Macro): {results[name]['metrics']['F1-Score (Macro)']['mean']:.2f} ± {results[name]['metrics']['F1-Score (Macro)']['std']:.2f}\")\n",
    "        print(f\"F1-Score (Weighted): {results[name]['metrics']['F1-Score (Weighted)']['mean']:.2f} ± {results[name]['metrics']['F1-Score (Weighted)']['std']:.2f}\")\n",
    "    \n",
    "    return models, results\n",
    "\n",
    "def clear_cache():\n",
    "    cache_files = [\n",
    "        \"baseline_embeddings_cache.pkl\",\n",
    "        \"classification_embeddings_cache.pkl\",\n",
    "        \"domain_embeddings_cache.pkl\"\n",
    "    ]\n",
    "    for file in cache_files:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "            print(f\"Удален кэш-файл: {file}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Загрузка информации о датасетах...\")\n",
    "    dataset_info, domains = load_dataset_info()\n",
    "    print(f\"Загружено {len(dataset_info)} датасетов\")\n",
    "    \n",
    "    if not dataset_info:\n",
    "        print(\"Не найдено датасетов для обработки\")\n",
    "        return\n",
    "    \n",
    "    print(\"Инициализация модели для создания эмбеддингов...\")\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  \n",
    "    \n",
    "    # Создание эмбеддингов для метода голосование\n",
    "    print(\"Создание эмбеддингов для метода голосование...\")\n",
    "    baseline_embeddings, unique_domains = generate_embeddings(\n",
    "        dataset_info, \n",
    "        embedding_model, \n",
    "        domains, \n",
    "        method=\"voting\", \n",
    "        cache_file=\"baseline_embeddings_cache.pkl\"\n",
    "    )\n",
    "    \n",
    "    if not baseline_embeddings:\n",
    "        print(\"Не удалось создать эмбеддинги для метода голосование\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nПодготовка данных для оценки метода голосование\")\n",
    "    dataset_ids = list(baseline_embeddings.keys())\n",
    "    y_true = [data['domain'] for data in baseline_embeddings.values()]\n",
    "    predicted_domains = [data['predicted_domain'] for data in baseline_embeddings.values()]\n",
    "    \n",
    "    print(\"\\nОценка метода голосование:\")\n",
    "    mismatches = [(ds_id, true, pred) for ds_id, true, pred in zip(dataset_ids, y_true, predicted_domains) if true != pred]\n",
    "    print(f\"Несоответствия доменов (истинный vs. бейзлайн): {len(mismatches)} из {len(dataset_ids)}\")\n",
    "    if mismatches:\n",
    "        print(\"Примеры несоответствий:\")\n",
    "        for ds_id, true, pred in mismatches[:min(5, len(mismatches))]:\n",
    "            print(f\"Dataset {ds_id}: Истинный={true}, Предсказанный (бейзлайн)={pred}, Columns={', '.join(dataset_info[ds_id]['columns'])}\")\n",
    "    \n",
    "    # Расчет метрик для бейзлайна\n",
    "    baseline_acc = accuracy_score(y_true, predicted_domains)\n",
    "    baseline_prec_macro = precision_score(y_true, predicted_domains, average='macro', zero_division=0)\n",
    "    baseline_rec_macro = recall_score(y_true, predicted_domains, average='macro', zero_division=0)\n",
    "    baseline_f1_macro = f1_score(y_true, predicted_domains, average='macro', zero_division=0)\n",
    "    baseline_prec_weighted = precision_score(y_true, predicted_domains, average='weighted', zero_division=0)\n",
    "    baseline_rec_weighted = recall_score(y_true, predicted_domains, average='weighted', zero_division=0)\n",
    "    baseline_f1_weighted = f1_score(y_true, predicted_domains, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(\"\\nМетрики бейзлайна:\")\n",
    "    print(f\"Accuracy: {baseline_acc:.2f} ± 0.00\")\n",
    "    print(f\"F1-Score (Macro): {baseline_f1_macro:.2f} ± 0.00\")\n",
    "    print(f\"F1-Score (Weighted): {baseline_f1_weighted:.2f} ± 0.00\")\n",
    "    \n",
    "    print(\"\\nСоздание эмбеддингов для классификации...\")\n",
    "    classification_embeddings, _ = generate_embeddings(\n",
    "        dataset_info, \n",
    "        embedding_model, \n",
    "        domains, \n",
    "        method=EMBEDDING_METHOD, \n",
    "        cache_file=\"classification_embeddings_cache.pkl\"\n",
    "    )\n",
    "    \n",
    "    if not classification_embeddings:\n",
    "        print(\"Не удалось создать эмбеддинги для классификации\")\n",
    "        return\n",
    "    \n",
    "    print(\"Подготовка данных для классификации\")\n",
    "    X = np.array([data['embedding'] for data in classification_embeddings.values()])\n",
    "    y_true = [data['domain'] for data in classification_embeddings.values()]\n",
    "    dataset_ids = list(classification_embeddings.keys())\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y_true)\n",
    "    \n",
    "    print(\"\\nРаспределение доменов:\")\n",
    "    domain_counts = pd.Series(y_true).value_counts()\n",
    "    print(domain_counts)\n",
    "    print(f\"Количество уникальных доменов: {len(domain_counts)}\")\n",
    "    \n",
    "    print(\"\\nОбучение моделей и кросс-валидация\")\n",
    "    models, cv_results = train_models(X, y_encoded)\n",
    "    \n",
    "    print(\"\\nСравнение бейзлайна и моделей:\")\n",
    "    comparison = []\n",
    "    \n",
    "    # Добавляем бейзлайн\n",
    "    comparison.append({\n",
    "        'Model': 'Voting',\n",
    "        'Accuracy': f\"{baseline_acc:.2f} ± 0.00\",\n",
    "        'Precision (Macro)': f\"{baseline_prec_macro:.2f} ± 0.00\",\n",
    "        'Recall (Macro)': f\"{baseline_rec_macro:.2f} ± 0.00\",\n",
    "        'F1-Score (Macro)': f\"{baseline_f1_macro:.2f} ± 0.00\",\n",
    "        'Precision (Weighted)': f\"{baseline_prec_weighted:.2f} ± 0.00\",\n",
    "        'Recall (Weighted)': f\"{baseline_rec_weighted:.2f} ± 0.00\",\n",
    "        'F1-Score (Weighted)': f\"{baseline_f1_weighted:.2f} ± 0.00\"\n",
    "    })\n",
    "\n",
    "    for model_name, res in cv_results.items():\n",
    "        metrics = res['metrics']\n",
    "        comparison.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': f\"{metrics['Accuracy']['mean']:.2f} ± {metrics['Accuracy']['std']:.2f}\",\n",
    "            'Precision (Macro)': f\"{metrics['Precision (Macro)']['mean']:.2f} ± {metrics['Precision (Macro)']['std']:.2f}\",\n",
    "            'Recall (Macro)': f\"{metrics['Recall (Macro)']['mean']:.2f} ± {metrics['Recall (Macro)']['std']:.2f}\",\n",
    "            'F1-Score (Macro)': f\"{metrics['F1-Score (Macro)']['mean']:.2f} ± {metrics['F1-Score (Macro)']['std']:.2f}\",\n",
    "            'Precision (Weighted)': f\"{metrics['Precision (Weighted)']['mean']:.2f} ± {metrics['Precision (Weighted)']['std']:.2f}\",\n",
    "            'Recall (Weighted)': f\"{metrics['Recall (Weighted)']['mean']:.2f} ± {metrics['Recall (Weighted)']['std']:.2f}\",\n",
    "            'F1-Score (Weighted)': f\"{metrics['F1-Score (Weighted)']['mean']:.2f} ± {metrics['F1-Score (Weighted)']['std']:.2f}\"\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "\n",
    "    plot_data = []\n",
    "    for model in comparison_df['Model']:\n",
    "        row = comparison_df[comparison_df['Model'] == model].iloc[0]\n",
    "        plot_data.append({\n",
    "            'Model': model,\n",
    "            'Metric': 'Accuracy',\n",
    "            'Value': float(row['Accuracy'].split(' ')[0])\n",
    "        })\n",
    "        plot_data.append({\n",
    "            'Model': model,\n",
    "            'Metric': 'F1-Score (Macro)',\n",
    "            'Value': float(row['F1-Score (Macro)'].split(' ')[0])\n",
    "        })\n",
    "        plot_data.append({\n",
    "            'Model': model,\n",
    "            'Metric': 'F1-Score (Weighted)',\n",
    "            'Value': float(row['F1-Score (Weighted)'].split(' ')[0])\n",
    "        })\n",
    "    \n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.barplot(data=plot_df, x='Model', y='Value', hue='Metric')\n",
    "    plt.title('Сравнение бейзлайна и моделей классификации')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1.1)\n",
    "    \n",
    "    for p in plt.gca().patches:\n",
    "        height = p.get_height()\n",
    "        plt.gca().annotate(f'{height:.2f}', \n",
    "                          (p.get_x() + p.get_width() / 2., height),\n",
    "                          ha='center', va='bottom',\n",
    "                          xytext=(0, 5),\n",
    "                          textcoords='offset points',\n",
    "                          fontsize=9)\n",
    "    \n",
    "    plt.legend(title='Metric', loc='lower right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparison_metrics.png')\n",
    "    plt.show()\n",
    "\n",
    "    best_model_name = None\n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for model_name, res in cv_results.items():\n",
    "        if res['metrics']['Accuracy']['mean'] > best_accuracy:\n",
    "            best_accuracy = res['metrics']['Accuracy']['mean']\n",
    "            best_model_name = model_name\n",
    "    \n",
    "    if best_accuracy <= baseline_acc:\n",
    "        best_model_name = 'Voting'\n",
    "    \n",
    "    print(f\"\\nЛучшая модель: {best_model_name}\")\n",
    "    \n",
    "    if best_model_name != 'Voting':\n",
    "        best_model = cv_results[best_model_name]['model']\n",
    "        best_model.fit(X, y_encoded)  \n",
    "        joblib.dump({\n",
    "            'model': best_model,\n",
    "            'label_encoder': label_encoder,\n",
    "            'embedding_model': embedding_model,\n",
    "            'class_names': label_encoder.classes_\n",
    "        }, MODEL_SAVE_PATH)\n",
    "        print(f\"Лучшая модель сохранена как {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    if best_model_name != 'Voting':\n",
    "        y_pred = cv_results[best_model_name]['y_pred']\n",
    "        y_pred_domain = label_encoder.inverse_transform(y_pred)\n",
    "        errors = []\n",
    "        for i, ds_id in enumerate(dataset_ids):\n",
    "            if y_true[i] != y_pred_domain[i]:\n",
    "                errors.append({\n",
    "                    'dataset_id': ds_id,\n",
    "                    'columns': dataset_info[ds_id]['columns'],\n",
    "                    'actual_domain': y_true[i],\n",
    "                    'predicted_domain': y_pred_domain[i],\n",
    "                    'predicted_domain_baseline': predicted_domains[i]\n",
    "                })\n",
    "        \n",
    "        print(f\"\\nНайдено ошибок классификации ({best_model_name}): {len(errors)} из {len(dataset_ids)}\")\n",
    "        if errors:\n",
    "            print(\"\\nПримеры ошибок классификации:\")\n",
    "            for error in errors[:min(5, len(errors))]:\n",
    "                print(f\"\\nDataset {error['dataset_id']}:\")\n",
    "                print(f\"  Истинный домен: {error['actual_domain']}\")\n",
    "                print(f\"  Предсказанный домен (модель): {error['predicted_domain']}\")\n",
    "                print(f\"  Предсказанный домен (бейзлайн): {error['predicted_domain_baseline']}\")\n",
    "                print(f\"  Columns: {', '.join(error['columns'])}\")\n",
    "    \n",
    "    print(\"\\nСохранение результатов...\")\n",
    "    all_results = []\n",
    "    for i, ds_id in enumerate(dataset_ids):\n",
    "        result_row = {\n",
    "            'dataset_id': ds_id,\n",
    "            'actual_domain': y_true[i],\n",
    "            'predicted_domain_baseline': predicted_domains[i],\n",
    "            'correct_baseline': int(y_true[i] == predicted_domains[i]),\n",
    "            'columns': ', '.join(dataset_info[ds_id]['columns'])\n",
    "        }\n",
    "        for model_name in models.keys():\n",
    "            y_pred_domain = label_encoder.inverse_transform(cv_results[model_name]['y_pred'])\n",
    "            result_row[f'predicted_{model_name}'] = y_pred_domain[i]\n",
    "            result_row[f'correct_{model_name}'] = int(y_true[i] == y_pred_domain[i])\n",
    "        all_results.append(result_row)\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_csv('classification_results.csv', index=False)\n",
    "    comparison_df.to_csv('comparison_metrics.csv', index=False)\n",
    "    \n",
    "    print(\"Результаты классификации и бейзлайна сохранены в classification_results.csv\")\n",
    "    print(\"Метрики сохранены в comparison_metrics.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clear_cache()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e385d-805f-4974-989a-d33d59caee7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## D4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f61389b-3703-4e23-a243-e7819359be6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One-shot fold 2:   0%|                                                                  | 0/52 [15:41:30<?, ?dataset/s]\n",
      "Loading datasets: 100%|██████████████████████████████████████████████████████████████| 258/258 [00:08<00:00, 29.51it/s]\n",
      "2025-08-21 12:47:38,451 - INFO - Загружено датасетов: 258\n",
      "2025-08-21 12:47:38,451 - INFO - Уникальных доменов: 14\n",
      "2025-08-21 12:47:38,452 - INFO - Train датасетов: 211\n",
      "2025-08-21 12:47:38,452 - INFO - Test датасетов: 47\n",
      "Generating domain terms:   0%|                                                                  | 0/14 [00:00<?, ?it/s]\n",
      "Processing datasets for domain terms: 100%|████████████████████████████████████████████████████| 24/24 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Computing local domains:   0%|                                                                 | 0/528 [00:00<?, ?it/s]\u001b[A\n",
      "Computing local domains:  11%|██████▏                                                | 59/528 [00:00<00:00, 584.15it/s]\u001b[A\n",
      "Computing local domains:  23%|████████████▋                                         | 124/528 [00:00<00:00, 619.10it/s]\u001b[A\n",
      "Computing local domains:  35%|███████████████████                                   | 186/528 [00:00<00:01, 321.58it/s]\u001b[A\n",
      "Computing local domains:  43%|███████████████████████▍                              | 229/528 [00:00<00:00, 323.62it/s]\u001b[A\n",
      "Computing local domains:  55%|█████████████████████████████▊                        | 292/528 [00:00<00:00, 398.31it/s]\u001b[A\n",
      "Computing local domains:  68%|████████████████████████████████████▊                 | 360/528 [00:00<00:00, 470.95it/s]\u001b[A\n",
      "Computing local domains:  79%|██████████████████████████████████████████▍           | 415/528 [00:00<00:00, 473.73it/s]\u001b[A\n",
      "Computing local domains: 100%|██████████████████████████████████████████████████████| 528/528 [00:01<00:00, 462.26it/s]\u001b[A\n",
      "Generating domain terms:   7%|████▏                                                     | 1/14 [00:01<00:17,  1.37s/it]\n",
      "Processing datasets for domain terms: 100%|████████████████████████████████████████████████████| 15/15 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Computing local domains:   0%|                                                                 | 0/282 [00:00<?, ?it/s]\u001b[A\n",
      "Computing local domains:  26%|██████████████                                         | 72/282 [00:00<00:00, 719.99it/s]\u001b[A\n",
      "Computing local domains:  55%|█████████████████████████████▊                        | 156/282 [00:00<00:00, 776.88it/s]\u001b[A\n",
      "Computing local domains: 100%|██████████████████████████████████████████████████████| 282/282 [00:00<00:00, 794.34it/s]\u001b[A\n",
      "Generating domain terms:  14%|████████▎                                                 | 2/14 [00:01<00:09,  1.24it/s]\n",
      "Processing datasets for domain terms: 100%|████████████████████████████████████████████████████| 13/13 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Computing local domains:   0%|                                                                 | 0/181 [00:00<?, ?it/s]\u001b[A\n",
      "Computing local domains: 100%|█████████████████████████████████████████████████████| 181/181 [00:00<00:00, 1073.27it/s]\u001b[A\n",
      "Generating domain terms:  21%|████████████▍                                             | 3/14 [00:01<00:05,  1.91it/s]\n",
      "Processing datasets for domain terms: 100%|████████████████████████████████████████████████████| 12/12 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Computing local domains:   0%|                                                                 | 0/255 [00:00<?, ?it/s]\u001b[A\n",
      "Computing local domains:  20%|███████████                                            | 51/255 [00:00<00:00, 497.05it/s]\u001b[A\n",
      "Computing local domains:  62%|█████████████████████████████████▍                    | 158/255 [00:00<00:00, 820.95it/s]\u001b[A\n",
      "Computing local domains: 100%|██████████████████████████████████████████████████████| 255/255 [00:00<00:00, 785.57it/s]\u001b[A\n",
      "Generating domain terms:  29%|████████████████▌                                         | 4/14 [00:02<00:04,  2.14it/s]\n",
      "Processing datasets for domain terms: 100%|█████████████████████████████████████████| 33/33 [00:00<00:00, 33057.57it/s]\u001b[A\n",
      "\n",
      "Computing local domains:   0%|                                                                | 0/1071 [00:00<?, ?it/s]\u001b[A\n",
      "Computing local domains:   2%|█▏                                                    | 24/1071 [00:00<00:04, 226.35it/s]\u001b[A\n",
      "Computing local domains:  11%|█████▉                                               | 120/1071 [00:00<00:01, 640.22it/s]\u001b[A\n",
      "Computing local domains:  17%|█████████▏                                           | 185/1071 [00:01<00:06, 131.70it/s]\u001b[A\n",
      "Computing local domains:  21%|███████████                                          | 224/1071 [00:01<00:06, 136.21it/s]\u001b[A\n",
      "Computing local domains:  25%|█████████████▏                                       | 266/1071 [00:01<00:04, 170.15it/s]\u001b[A\n",
      "Computing local domains:  28%|██████████████▊                                      | 300/1071 [00:01<00:05, 141.97it/s]\u001b[A\n",
      "Computing local domains:  30%|████████████████▏                                    | 326/1071 [00:02<00:04, 151.25it/s]\u001b[A\n",
      "Computing local domains:  33%|█████████████████▎                                   | 350/1071 [00:02<00:04, 160.98it/s]\u001b[A\n",
      "Computing local domains:  37%|███████████████████▌                                 | 396/1071 [00:02<00:03, 210.22it/s]\u001b[A\n",
      "Computing local domains:  40%|█████████████████████                                | 425/1071 [00:02<00:03, 172.78it/s]\u001b[A\n",
      "Computing local domains:  42%|██████████████████████▏                              | 449/1071 [00:02<00:03, 175.82it/s]\u001b[A\n",
      "Computing local domains:  44%|███████████████████████▎                             | 471/1071 [00:02<00:03, 156.95it/s]\u001b[A\n",
      "Computing local domains:  46%|████████████████████████▏                            | 490/1071 [00:03<00:04, 137.00it/s]\u001b[A\n",
      "Computing local domains:  47%|█████████████████████████                            | 506/1071 [00:03<00:04, 130.78it/s]\u001b[A\n",
      "Computing local domains:  49%|█████████████████████████▊                           | 521/1071 [00:03<00:04, 129.71it/s]\u001b[A\n",
      "Computing local domains:  52%|███████████████████████████▌                         | 558/1071 [00:03<00:02, 177.43it/s]\u001b[A\n",
      "Computing local domains:  54%|████████████████████████████▌                        | 578/1071 [00:03<00:03, 142.37it/s]\u001b[A\n",
      "Computing local domains:  56%|█████████████████████████████▍                       | 595/1071 [00:03<00:03, 127.28it/s]\u001b[A\n",
      "Computing local domains:  59%|███████████████████████████████▏                     | 629/1071 [00:03<00:02, 169.63it/s]\u001b[A\n",
      "Computing local domains:  62%|████████████████████████████████▊                    | 663/1071 [00:04<00:02, 200.83it/s]\u001b[A\n",
      "Computing local domains:  64%|█████████████████████████████████▉                   | 687/1071 [00:04<00:03, 107.18it/s]\u001b[A\n",
      "Computing local domains:  66%|███████████████████████████████████▌                  | 705/1071 [00:04<00:04, 77.09it/s]\u001b[A\n",
      "Computing local domains:  67%|████████████████████████████████████▎                 | 719/1071 [00:05<00:04, 74.53it/s]\u001b[A\n",
      "Computing local domains:  68%|████████████████████████████████████▊                 | 731/1071 [00:05<00:04, 74.89it/s]\u001b[A\n",
      "Computing local domains:  69%|█████████████████████████████████████▍                | 742/1071 [00:05<00:04, 71.41it/s]\u001b[A\n",
      "Computing local domains:  70%|█████████████████████████████████████▊                | 751/1071 [00:05<00:04, 72.39it/s]\u001b[A\n",
      "Computing local domains:  71%|██████████████████████████████████████▎               | 760/1071 [00:05<00:04, 75.59it/s]\u001b[A\n",
      "Computing local domains:  72%|██████████████████████████████████████▊               | 770/1071 [00:05<00:03, 79.79it/s]\u001b[A\n",
      "Computing local domains:  73%|███████████████████████████████████████▎              | 779/1071 [00:05<00:03, 76.40it/s]\u001b[A\n",
      "Computing local domains:  74%|███████████████████████████████████████▋              | 788/1071 [00:06<00:03, 72.22it/s]\u001b[A\n",
      "Computing local domains:  74%|████████████████████████████████████████▏             | 796/1071 [00:06<00:04, 67.77it/s]\u001b[A\n",
      "Computing local domains:  75%|████████████████████████████████████████▌             | 804/1071 [00:06<00:04, 66.02it/s]\u001b[A\n",
      "Computing local domains:  76%|████████████████████████████████████████▉             | 813/1071 [00:06<00:03, 70.84it/s]\u001b[A\n",
      "Computing local domains:  78%|██████████████████████████████████████████▏           | 837/1071 [00:06<00:02, 92.48it/s]\u001b[A\n",
      "Computing local domains:  79%|██████████████████████████████████████████▋           | 846/1071 [00:06<00:02, 87.55it/s]\u001b[A\n",
      "Computing local domains:  83%|███████████████████████████████████████████▋         | 884/1071 [00:06<00:01, 156.32it/s]\u001b[A\n",
      "Computing local domains:  85%|████████████████████████████████████████████▉        | 909/1071 [00:07<00:00, 172.92it/s]\u001b[A\n",
      "Computing local domains:  87%|█████████████████████████████████████████████▉       | 928/1071 [00:07<00:01, 130.68it/s]\u001b[A\n",
      "Computing local domains:  88%|██████████████████████████████████████████████▋      | 944/1071 [00:07<00:01, 108.88it/s]\u001b[A\n",
      "Computing local domains:  89%|████████████████████████████████████████████████▎     | 957/1071 [00:07<00:01, 99.76it/s]\u001b[A\n",
      "Computing local domains:  90%|████████████████████████████████████████████████▊     | 969/1071 [00:07<00:01, 93.27it/s]\u001b[A\n",
      "Computing local domains:  92%|█████████████████████████████████████████████████▍    | 980/1071 [00:07<00:00, 92.18it/s]\u001b[A\n",
      "Computing local domains:  92%|█████████████████████████████████████████████████▉    | 990/1071 [00:08<00:00, 88.88it/s]\u001b[A\n",
      "Computing local domains: 100%|████████████████████████████████████████████████████| 1071/1071 [00:08<00:00, 129.26it/s]\u001b[A\n",
      "Generating domain terms:  36%|████████████████████▋                                     | 5/14 [00:11<00:31,  3.51s/it]\n",
      "Processing datasets for domain terms: 100%|████████████████████████████████████████████████████| 18/18 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Computing local domains:   0%|                                                                 | 0/593 [00:00<?, ?it/s]\u001b[A\n",
      "Computing local domains:   9%|████▋                                                  | 51/593 [00:00<00:01, 485.72it/s]\u001b[A\n",
      "Computing local domains:  17%|█████████                                             | 100/593 [00:00<00:01, 418.28it/s]\u001b[A\n",
      "Computing local domains:  27%|██████████████▋                                       | 161/593 [00:00<00:00, 497.58it/s]\u001b[A\n",
      "Computing local domains:  36%|███████████████████▎                                  | 212/593 [00:00<00:00, 467.23it/s]\u001b[A\n",
      "Computing local domains:  45%|████████████████████████▎                             | 267/593 [00:00<00:00, 486.54it/s]\u001b[A\n",
      "Computing local domains:  59%|███████████████████████████████▊                      | 349/593 [00:00<00:00, 589.06it/s]\u001b[A\n",
      "Computing local domains:  69%|█████████████████████████████████████▏                | 409/593 [00:00<00:00, 544.24it/s]\u001b[A\n",
      "Computing local domains:  78%|██████████████████████████████████████████▎           | 465/593 [00:00<00:00, 483.05it/s]\u001b[A\n",
      "Computing local domains:  87%|██████████████████████████████████████████████▉       | 516/593 [00:01<00:00, 472.59it/s]\u001b[A\n",
      "Computing local domains: 100%|██████████████████████████████████████████████████████| 593/593 [00:01<00:00, 507.53it/s]\u001b[A\n",
      "Generating domain terms:  43%|████████████████████████▊                                 | 6/14 [00:12<00:22,  2.86s/it]\n",
      "Processing datasets for domain terms: 100%|████████████████████████████████████████████████████| 13/13 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Computing local domains:   0%|                                                                 | 0/252 [00:00<?, ?it/s]\u001b[A\n",
      "Computing local domains:  47%|█████████████████████████                            | 119/252 [00:00<00:00, 1182.87it/s]\u001b[A\n",
      "Computing local domains: 100%|█████████████████████████████████████████████████████| 252/252 [00:00<00:00, 1099.77it/s]\u001b[A\n",
      "Generating domain terms:  50%|█████████████████████████████                             | 7/14 [00:13<00:14,  2.01s/it]\n",
      "Processing datasets for domain terms: 100%|████████████████████████████████████████████████████| 16/16 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Computing local domains:   0%|                                                                 | 0/354 [00:00<?, ?it/s]\u001b[A\n",
      "Computing local domains:   8%|████▏                                                  | 27/354 [00:00<00:01, 234.71it/s]\u001b[A\n",
      "Computing local domains:  19%|██████████▍                                            | 67/354 [00:00<00:00, 322.76it/s]\u001b[A\n",
      "Computing local domains:  28%|███████████████▎                                      | 100/354 [00:00<00:00, 293.70it/s]\u001b[A\n",
      "Computing local domains:  43%|███████████████████████▎                              | 153/354 [00:00<00:00, 377.62it/s]\u001b[A\n",
      "Computing local domains:  55%|█████████████████████████████▉                        | 196/354 [00:00<00:00, 379.96it/s]\u001b[A\n",
      "Computing local domains:  81%|███████████████████████████████████████████▋          | 286/354 [00:00<00:00, 541.84it/s]\u001b[A\n",
      "Computing local domains: 100%|██████████████████████████████████████████████████████| 354/354 [00:00<00:00, 435.66it/s]\u001b[A\n",
      "Generating domain terms:  57%|█████████████████████████████████▏                        | 8/14 [00:14<00:09,  1.66s/it]\n",
      "Processing datasets for domain terms: 100%|██████████████████████████████████████████████████████| 3/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Computing local domains: 100%|███████████████████████████████████████████████████████| 56/56 [00:00<00:00, 1436.03it/s]\u001b[A\n",
      "\n",
      "Processing datasets for domain terms: 100%|████████████████████████████████████████████████████| 12/12 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Computing local domains:   0%|                                                                 | 0/247 [00:00<?, ?it/s]\u001b[A\n",
      "Computing local domains: 100%|█████████████████████████████████████████████████████| 247/247 [00:00<00:00, 2398.00it/s]\u001b[A\n",
      "Generating domain terms:  71%|████████████████████████████████████████▋                | 10/14 [00:14<00:03,  1.10it/s]\n",
      "Processing datasets for domain terms: 100%|████████████████████████████████████████████████████| 14/14 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Computing local domains:   0%|                                                                 | 0/195 [00:00<?, ?it/s]\u001b[A\n",
      "Computing local domains:  39%|█████████████████████▋                                 | 77/195 [00:00<00:00, 762.40it/s]\u001b[A\n",
      "Computing local domains: 100%|██████████████████████████████████████████████████████| 195/195 [00:00<00:00, 812.50it/s]\u001b[A\n",
      "Generating domain terms:  79%|████████████████████████████████████████████▊            | 11/14 [00:14<00:02,  1.35it/s]\n",
      "Processing datasets for domain terms: 100%|█████████████████████████████████████████| 12/12 [00:00<00:00, 12009.46it/s]\u001b[A\n",
      "\n",
      "Computing local domains:   0%|                                                                 | 0/362 [00:00<?, ?it/s]\u001b[A\n",
      "Computing local domains:   6%|███▏                                                   | 21/362 [00:00<00:01, 198.12it/s]\u001b[A\n",
      "Computing local domains:  18%|█████████▉                                             | 65/362 [00:00<00:00, 336.97it/s]\u001b[A\n",
      "Computing local domains:  50%|███████████████████████████▏                          | 182/362 [00:00<00:00, 702.81it/s]\u001b[A\n",
      "Computing local domains:  70%|█████████████████████████████████████▋                | 253/362 [00:00<00:00, 479.31it/s]\u001b[A\n",
      "Computing local domains: 100%|██████████████████████████████████████████████████████| 362/362 [00:00<00:00, 569.96it/s]\u001b[A\n",
      "Generating domain terms:  86%|████████████████████████████████████████████████▊        | 12/14 [00:15<00:01,  1.33it/s]\n",
      "Processing datasets for domain terms: 100%|████████████████████████████████████████████████████| 13/13 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Computing local domains:   0%|                                                                 | 0/270 [00:00<?, ?it/s]\u001b[A\n",
      "Computing local domains:  42%|██████████████████████▍                              | 114/270 [00:00<00:00, 1117.67it/s]\u001b[A\n",
      "Computing local domains: 100%|██████████████████████████████████████████████████████| 270/270 [00:00<00:00, 479.57it/s]\u001b[A\n",
      "Generating domain terms:  93%|████████████████████████████████████████████████████▉    | 13/14 [00:15<00:00,  1.41it/s]\n",
      "Processing datasets for domain terms: 100%|█████████████████████████████████████████| 13/13 [00:00<00:00, 12997.84it/s]\u001b[A\n",
      "\n",
      "Computing local domains: 100%|█████████████████████████████████████████████████████| 608/608 [00:00<00:00, 6713.54it/s]\u001b[A\n",
      "Generating domain terms: 100%|█████████████████████████████████████████████████████████| 14/14 [00:15<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain Terms Dict:\n",
      "Medical Science: [\"'B1of3'\", \"'B2of3'\", \"'B3of3'\", '(-0.001, 0.2]', '(-0.001, 0.4]', '(-0.001, 0.8]', '(-0.001, 4.8]', '(-0.001, 9.6]', '(0.6, 1.0]', '(0.8, 1.0]']...\n",
      "Social Sciences: ['(-0.001, 0.2]', '(-0.001, 0.4]', '(-0.001, 0.6]', '(-0.001, 0.8]', '(-0.001, 1620.0]', '(-0.001, 4.4]', '(-0.001, 50.0]', '(0.6, 1.0]', '(0.8, 1.0]', '(0.8, 1.2]']...\n",
      "Synthetic Systems: ['(-0.001, 0.2]', '(-0.001, 0.4]', '(-0.0855, -0.00226]', '(-0.157, 0.0298]', '(-0.2, 0.0]', '(-0.2, 0.2]', '(-0.216, -0.0555]', '(-0.216, -0.15]', '(-0.392, -0.0602]', '(-0.4, 0.0]']...\n",
      "Biology & Genetics: ['(-0.001, 0.2]', '(-0.001, 0.4]', '(-0.001, 0.8]', '(0.6, 1.0]', '(0.8, 1.0]', '(0.8, 1.2]', '(0.999, 1.2]', '(0.999, 1.4]', '(0.999, 1.8]', '(1.2, 2.0]']...\n",
      "Computer Science & Artificial Intelligence: [\"'B1of3'\", \"'B2of3'\", \"'B3of3'\", '(-0.001, 0.108]', '(-0.001, 0.286]', '(-0.001, 0.2]', '(-0.001, 0.4]', '(-0.001, 0.6]', '(-0.001, 0.8]', '(-0.001, 1.0]']...\n",
      "Engineering & Technology: [\"'B1of3'\", \"'B2of3'\", \"'B3of3'\", '(-0.001, 1.008]', '(-0.001, 100.0]', '(-0.001, 120.0]', '(-0.001, 154.086]', '(-0.001, 18.0]', '(-0.001, 2.015]', '(-0.001, 240.0]']...\n",
      "Transportation Systems: ['(-0.001, 0.2]', '(-0.001, 0.4]', '(-0.2, 0.0]', '(-0.2, 0.2]', '(-1.001, -0.6]', '(0.6, 1.0]', '(0.8, 1.0]', '(0.8, 1.2]', '(0.999, 1.2]', '(0.999, 1.4]']...\n",
      "Sports & Recreation: [\"'B1of3'\", \"'B2of3'\", \"'B3of3'\", '(-0.001, 0.2]', '(-0.001, 0.4]', '(-0.001, 0.8]', '(-0.001, 1.2]', '(-0.164, -0.0864]', '(-0.344, -0.332]', '(-0.419, -0.243]']...\n",
      "Veterinary Science: ['(0.999, 1.2]', '(0.999, 1.4]', '(0.999, 1.8]', '(1.6, 2.0]', '(1.8, 2.0]', '(1.8, 2.2]', '(1.999, 2.4]', '(2.2, 3.0]', '(2.6, 3.0]', '(2.6, 3.2]']...\n",
      "Physics & Mathematics: [\"'B1of3'\", \"'B2of3'\", \"'B3of3'\", '(0.999, 1.2]', '(0.999, 1.4]', '(0.999, 1.8]', '(1.6, 2.0]', '(1.8, 2.0]', '(1.8, 2.2]', '(2.6, 3.0]']...\n",
      "Finance & Economics: [\"'B1of3'\", \"'B2of3'\", \"'B3of3'\", '(0.733, 0.74]', '(0.734, 0.741]', '(0.734, 0.74]', '(0.736, 0.741]', '(0.736, 0.742]', '(0.737, 0.742]', '(0.745, 0.751]']...\n",
      "Materials Science: [\"'B1of3'\", \"'B2of3'\", \"'B3of3'\", '(-0.001, 100.0]', '(-0.001, 154.086]', '(-0.001, 77.043]', '(0.39, 0.446]', '(0.39, 0.496]', '(0.5, 0.553]', '(0.527, 0.578]']...\n",
      "Environmental Science: ['(-0.001, 0.000391]', '(-0.001, 0.000781]', '(-0.001, 0.00117]', '(-0.001, 0.00156]', '(-0.001, 0.00234]', '(-0.001, 0.00391]', '(-0.001, 0.136]', '(-0.001, 0.4]', '(-0.001, 1.2]', '(0.0009499999999999999, 0.00273]']...\n",
      "Computational Biology: ['a', 'c', 'g', 't']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying datasets: 100%|██████████████████████████████████████████████████████████| 47/47 [00:00<00:00, 7834.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Метрики качества ===\n",
      "Accuracy: 0.2979 ± 0.0672\n",
      "Precision (Macro): 0.4390 ± 0.0810\n",
      "Recall (Macro): 0.2530 ± 0.0578\n",
      "F1-Score (Macro): 0.2739 ± 0.0590\n",
      "Precision (Weighted): 0.5257 ± 0.1260\n",
      "Recall (Weighted): 0.2979 ± 0.0672\n",
      "F1-Score (Weighted): 0.3035 ± 0.0726\n",
      "\n",
      "Confusion Matrix:\n",
      " [[0 0 0 1 0 0 0 0 0 0 0 0 0 2]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 2 0]\n",
      " [0 0 2 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 2 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 1 0 0 1 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 1 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 2 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 5 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 3 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 2 0 2 1 1 1]\n",
      " [0 0 0 0 0 0 0 0 3 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 2 0 0 0 0 0]]\n",
      "Unique Labels: ['Transportation Systems', 'Finance & Economics', 'Synthetic Systems', 'Social Sciences', 'Engineering & Technology', 'Veterinary Science', 'Environmental Science', 'Materials Science', 'Medical Science', 'Sports & Recreation', 'Computational Biology', 'Computer Science & Artificial Intelligence', 'Biology & Genetics', 'Physics & Mathematics']\n",
      "\n",
      "Predictions:\n",
      "Dataset_49: True=Medical Science, Pred=Medical Science, Scores={'Medical Science': 0.3902439024390244, 'Social Sciences': 0.07317073170731707, 'Synthetic Systems': 0.07317073170731707, 'Biology & Genetics': 0.13157894736842105, 'Computer Science & Artificial Intelligence': 0.0975609756097561, 'Engineering & Technology': 0.04878048780487805, 'Transportation Systems': 0.07317073170731707, 'Sports & Recreation': 0.14634146341463414, 'Veterinary Science': 0.07142857142857142, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.04878048780487805, 'Materials Science': 0.024390243902439025, 'Environmental Science': 0.07317073170731707, 'Computational Biology': 0.25}\n",
      "Dataset_51: True=Medical Science, Pred=Medical Science, Scores={'Medical Science': 0.4, 'Social Sciences': 0.08571428571428572, 'Synthetic Systems': 0.05714285714285714, 'Biology & Genetics': 0.11428571428571428, 'Computer Science & Artificial Intelligence': 0.14285714285714285, 'Engineering & Technology': 0.05714285714285714, 'Transportation Systems': 0.08571428571428572, 'Sports & Recreation': 0.14285714285714285, 'Veterinary Science': 0.07142857142857142, 'Physics & Mathematics': 0.09090909090909091, 'Finance & Economics': 0.05714285714285714, 'Materials Science': 0.02857142857142857, 'Environmental Science': 0.11428571428571428, 'Computational Biology': 0.0}\n",
      "Dataset_34: True=Medical Science, Pred=Biology & Genetics, Scores={'Medical Science': 0.0, 'Social Sciences': 0.0, 'Synthetic Systems': 0.0, 'Biology & Genetics': 0.1111111111111111, 'Computer Science & Artificial Intelligence': 0.0, 'Engineering & Technology': 0.1111111111111111, 'Transportation Systems': 0.1111111111111111, 'Sports & Recreation': 0.0, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.1111111111111111, 'Environmental Science': 0.0, 'Computational Biology': 0.0}\n",
      "Dataset_13: True=Medical Science, Pred=Medical Science, Scores={'Medical Science': 0.18181818181818182, 'Social Sciences': 0.09090909090909091, 'Synthetic Systems': 0.045454545454545456, 'Biology & Genetics': 0.045454545454545456, 'Computer Science & Artificial Intelligence': 0.13636363636363635, 'Engineering & Technology': 0.09090909090909091, 'Transportation Systems': 0.045454545454545456, 'Sports & Recreation': 0.18181818181818182, 'Veterinary Science': 0.045454545454545456, 'Physics & Mathematics': 0.09090909090909091, 'Finance & Economics': 0.09090909090909091, 'Materials Science': 0.09090909090909091, 'Environmental Science': 0.0, 'Computational Biology': 0.0}\n",
      "Dataset_53: True=Medical Science, Pred=Medical Science, Scores={'Medical Science': 0.2777777777777778, 'Social Sciences': 0.2222222222222222, 'Synthetic Systems': 0.1111111111111111, 'Biology & Genetics': 0.25, 'Computer Science & Artificial Intelligence': 0.2222222222222222, 'Engineering & Technology': 0.1111111111111111, 'Transportation Systems': 0.1111111111111111, 'Sports & Recreation': 0.2777777777777778, 'Veterinary Science': 0.21428571428571427, 'Physics & Mathematics': 0.2727272727272727, 'Finance & Economics': 0.0, 'Materials Science': 0.1388888888888889, 'Environmental Science': 0.1388888888888889, 'Computational Biology': 0.0}\n",
      "Dataset_77: True=Medical Science, Pred=Medical Science, Scores={'Medical Science': 0.18181818181818182, 'Social Sciences': 0.09090909090909091, 'Synthetic Systems': 0.09090909090909091, 'Biology & Genetics': 0.09090909090909091, 'Computer Science & Artificial Intelligence': 0.18181818181818182, 'Engineering & Technology': 0.09090909090909091, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.18181818181818182, 'Veterinary Science': 0.09090909090909091, 'Physics & Mathematics': 0.09090909090909091, 'Finance & Economics': 0.09090909090909091, 'Materials Science': 0.09090909090909091, 'Environmental Science': 0.045454545454545456, 'Computational Biology': 0.0}\n",
      "Dataset_1025: True=Social Sciences, Pred=Social Sciences, Scores={'Medical Science': 0.0, 'Social Sciences': 0.14285714285714285, 'Synthetic Systems': 0.0, 'Biology & Genetics': 0.0, 'Computer Science & Artificial Intelligence': 0.0, 'Engineering & Technology': 0.14285714285714285, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.0, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.14285714285714285, 'Environmental Science': 0.14285714285714285, 'Computational Biology': 0.0}\n",
      "Dataset_899: True=Social Sciences, Pred=Social Sciences, Scores={'Medical Science': 0.0, 'Social Sciences': 0.10526315789473684, 'Synthetic Systems': 0.0, 'Biology & Genetics': 0.0, 'Computer Science & Artificial Intelligence': 0.0, 'Engineering & Technology': 0.10526315789473684, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.0, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.10526315789473684, 'Environmental Science': 0.10526315789473684, 'Computational Biology': 0.0}\n",
      "Dataset_4: True=Social Sciences, Pred=Physics & Mathematics, Scores={'Medical Science': 0.16216216216216217, 'Social Sciences': 0.08108108108108109, 'Synthetic Systems': 0.05405405405405406, 'Biology & Genetics': 0.08108108108108109, 'Computer Science & Artificial Intelligence': 0.1891891891891892, 'Engineering & Technology': 0.05405405405405406, 'Transportation Systems': 0.08108108108108109, 'Sports & Recreation': 0.1891891891891892, 'Veterinary Science': 0.14285714285714285, 'Physics & Mathematics': 0.2727272727272727, 'Finance & Economics': 0.08108108108108109, 'Materials Science': 0.10810810810810811, 'Environmental Science': 0.02702702702702703, 'Computational Biology': 0.0}\n",
      "Dataset_159: True=Synthetic Systems, Pred=Synthetic Systems, Scores={'Medical Science': 0.0, 'Social Sciences': 0.0, 'Synthetic Systems': 0.058823529411764705, 'Biology & Genetics': 0.0, 'Computer Science & Artificial Intelligence': 0.0, 'Engineering & Technology': 0.0, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.0, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.0, 'Environmental Science': 0.0, 'Computational Biology': 0.0}\n",
      "Dataset_1236: True=Synthetic Systems, Pred=Synthetic Systems, Scores={'Medical Science': 0.2, 'Social Sciences': 0.2, 'Synthetic Systems': 0.8, 'Biology & Genetics': 0.2, 'Computer Science & Artificial Intelligence': 0.2, 'Engineering & Technology': 0.0, 'Transportation Systems': 0.2, 'Sports & Recreation': 0.2, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.0, 'Environmental Science': 0.1, 'Computational Biology': 0.0}\n",
      "Dataset_1206: True=Synthetic Systems, Pred=Physics & Mathematics, Scores={'Medical Science': 0.007936507936507936, 'Social Sciences': 0.0, 'Synthetic Systems': 0.008064516129032258, 'Biology & Genetics': 0.02631578947368421, 'Computer Science & Artificial Intelligence': 0.011976047904191617, 'Engineering & Technology': 0.007575757575757576, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.005988023952095809, 'Veterinary Science': 0.03571428571428571, 'Physics & Mathematics': 0.09090909090909091, 'Finance & Economics': 0.005988023952095809, 'Materials Science': 0.0, 'Environmental Science': 0.0, 'Computational Biology': 0.0}\n",
      "Dataset_1141: True=Biology & Genetics, Pred=Medical Science, Scores={'Medical Science': 0.0, 'Social Sciences': 0.0, 'Synthetic Systems': 0.0, 'Biology & Genetics': 0.0, 'Computer Science & Artificial Intelligence': 0.0, 'Engineering & Technology': 0.0, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.0, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.0, 'Environmental Science': 0.0, 'Computational Biology': 0.0}\n",
      "Dataset_249: True=Biology & Genetics, Pred=Medical Science, Scores={'Medical Science': 0.6923076923076923, 'Social Sciences': 0.19230769230769232, 'Synthetic Systems': 0.038461538461538464, 'Biology & Genetics': 0.15384615384615385, 'Computer Science & Artificial Intelligence': 0.19230769230769232, 'Engineering & Technology': 0.11538461538461539, 'Transportation Systems': 0.07692307692307693, 'Sports & Recreation': 0.23076923076923078, 'Veterinary Science': 0.15384615384615385, 'Physics & Mathematics': 0.09090909090909091, 'Finance & Economics': 0.07692307692307693, 'Materials Science': 0.19230769230769232, 'Environmental Science': 0.0, 'Computational Biology': 0.0}\n",
      "Dataset_269: True=Biology & Genetics, Pred=Medical Science, Scores={'Medical Science': 0.058823529411764705, 'Social Sciences': 0.0, 'Synthetic Systems': 0.0, 'Biology & Genetics': 0.0, 'Computer Science & Artificial Intelligence': 0.058823529411764705, 'Engineering & Technology': 0.0, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.058823529411764705, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.058823529411764705, 'Materials Science': 0.0, 'Environmental Science': 0.0, 'Computational Biology': 0.0}\n",
      "Dataset_73: True=Computer Science & Artificial Intelligence, Pred=Medical Science, Scores={'Medical Science': 0.42857142857142855, 'Social Sciences': 0.0, 'Synthetic Systems': 0.0, 'Biology & Genetics': 0.0, 'Computer Science & Artificial Intelligence': 0.35714285714285715, 'Engineering & Technology': 0.21428571428571427, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.42857142857142855, 'Veterinary Science': 0.07142857142857142, 'Physics & Mathematics': 0.2727272727272727, 'Finance & Economics': 0.42857142857142855, 'Materials Science': 0.21428571428571427, 'Environmental Science': 0.07142857142857142, 'Computational Biology': 0.0}\n",
      "Dataset_52: True=Computer Science & Artificial Intelligence, Pred=Physics & Mathematics, Scores={'Medical Science': 0.4117647058823529, 'Social Sciences': 0.38235294117647056, 'Synthetic Systems': 0.23529411764705882, 'Biology & Genetics': 0.4117647058823529, 'Computer Science & Artificial Intelligence': 0.3235294117647059, 'Engineering & Technology': 0.2647058823529412, 'Transportation Systems': 0.23529411764705882, 'Sports & Recreation': 0.38235294117647056, 'Veterinary Science': 0.25, 'Physics & Mathematics': 0.45454545454545453, 'Finance & Economics': 0.029411764705882353, 'Materials Science': 0.29411764705882354, 'Environmental Science': 0.08823529411764706, 'Computational Biology': 0.0}\n",
      "Dataset_59: True=Computer Science & Artificial Intelligence, Pred=Computational Biology, Scores={'Medical Science': 0.0, 'Social Sciences': 0.008264462809917356, 'Synthetic Systems': 0.0, 'Biology & Genetics': 0.0, 'Computer Science & Artificial Intelligence': 0.0136986301369863, 'Engineering & Technology': 0.007575757575757576, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.00684931506849315, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.012658227848101266, 'Environmental Science': 0.0136986301369863, 'Computational Biology': 0.25}\n",
      "Dataset_22: True=Computer Science & Artificial Intelligence, Pred=Biology & Genetics, Scores={'Medical Science': 0.0, 'Social Sciences': 0.0, 'Synthetic Systems': 0.0, 'Biology & Genetics': 0.02631578947368421, 'Computer Science & Artificial Intelligence': 0.018433179723502304, 'Engineering & Technology': 0.0, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.0058823529411764705, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.0, 'Environmental Science': 0.0, 'Computational Biology': 0.0}\n",
      "Dataset_123: True=Computer Science & Artificial Intelligence, Pred=Medical Science, Scores={'Medical Science': 0.75, 'Social Sciences': 0.0, 'Synthetic Systems': 0.0, 'Biology & Genetics': 0.0, 'Computer Science & Artificial Intelligence': 0.75, 'Engineering & Technology': 0.75, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.75, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.75, 'Finance & Economics': 0.75, 'Materials Science': 0.75, 'Environmental Science': 0.0, 'Computational Biology': 0.0}\n",
      "Dataset_139: True=Computer Science & Artificial Intelligence, Pred=Social Sciences, Scores={'Medical Science': 0.41935483870967744, 'Social Sciences': 0.4838709677419355, 'Synthetic Systems': 0.16129032258064516, 'Biology & Genetics': 0.2903225806451613, 'Computer Science & Artificial Intelligence': 0.3225806451612903, 'Engineering & Technology': 0.1935483870967742, 'Transportation Systems': 0.3548387096774194, 'Sports & Recreation': 0.3870967741935484, 'Veterinary Science': 0.2857142857142857, 'Physics & Mathematics': 0.45454545454545453, 'Finance & Economics': 0.0, 'Materials Science': 0.3225806451612903, 'Environmental Science': 0.12903225806451613, 'Computational Biology': 0.0}\n",
      "Dataset_124: True=Computer Science & Artificial Intelligence, Pred=Computational Biology, Scores={'Medical Science': 0.2, 'Social Sciences': 0.05, 'Synthetic Systems': 0.0, 'Biology & Genetics': 0.0, 'Computer Science & Artificial Intelligence': 0.65, 'Engineering & Technology': 0.2, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.25, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.18181818181818182, 'Finance & Economics': 0.1, 'Materials Science': 0.1, 'Environmental Science': 0.5, 'Computational Biology': 1.0}\n",
      "Dataset_62: True=Computer Science & Artificial Intelligence, Pred=Computer Science & Artificial Intelligence, Scores={'Medical Science': 0.6, 'Social Sciences': 0.6, 'Synthetic Systems': 0.5, 'Biology & Genetics': 0.5, 'Computer Science & Artificial Intelligence': 0.7, 'Engineering & Technology': 0.1, 'Transportation Systems': 0.4, 'Sports & Recreation': 0.6, 'Veterinary Science': 0.1, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.1, 'Environmental Science': 0.1, 'Computational Biology': 0.0}\n",
      "Dataset_2: True=Engineering & Technology, Pred=Engineering & Technology, Scores={'Medical Science': 0.0, 'Social Sciences': 0.03225806451612903, 'Synthetic Systems': 0.0, 'Biology & Genetics': 0.0967741935483871, 'Computer Science & Artificial Intelligence': 0.0, 'Engineering & Technology': 0.3870967741935484, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.0967741935483871, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.3225806451612903, 'Environmental Science': 0.0, 'Computational Biology': 0.0}\n",
      "Dataset_248: True=Engineering & Technology, Pred=Transportation Systems, Scores={'Medical Science': 0.02040816326530612, 'Social Sciences': 0.02040816326530612, 'Synthetic Systems': 0.030612244897959183, 'Biology & Genetics': 0.05263157894736842, 'Computer Science & Artificial Intelligence': 0.02040816326530612, 'Engineering & Technology': 0.17346938775510204, 'Transportation Systems': 0.32653061224489793, 'Sports & Recreation': 0.02040816326530612, 'Veterinary Science': 0.03571428571428571, 'Physics & Mathematics': 0.09090909090909091, 'Finance & Economics': 0.0, 'Materials Science': 0.012658227848101266, 'Environmental Science': 0.01020408163265306, 'Computational Biology': 0.0}\n",
      "Dataset_1353: True=Engineering & Technology, Pred=Materials Science, Scores={'Medical Science': 0.12962962962962962, 'Social Sciences': 0.18518518518518517, 'Synthetic Systems': 0.09259259259259259, 'Biology & Genetics': 0.23684210526315788, 'Computer Science & Artificial Intelligence': 0.1111111111111111, 'Engineering & Technology': 0.6481481481481481, 'Transportation Systems': 0.07407407407407407, 'Sports & Recreation': 0.2222222222222222, 'Veterinary Science': 0.25, 'Physics & Mathematics': 0.36363636363636365, 'Finance & Economics': 0.0, 'Materials Science': 0.6666666666666666, 'Environmental Science': 0.05555555555555555, 'Computational Biology': 0.0}\n",
      "Dataset_262: True=Engineering & Technology, Pred=Physics & Mathematics, Scores={'Medical Science': 0.03361344537815126, 'Social Sciences': 0.01680672268907563, 'Synthetic Systems': 0.0, 'Biology & Genetics': 0.05263157894736842, 'Computer Science & Artificial Intelligence': 0.05042016806722689, 'Engineering & Technology': 0.0, 'Transportation Systems': 0.01680672268907563, 'Sports & Recreation': 0.03361344537815126, 'Veterinary Science': 0.07142857142857142, 'Physics & Mathematics': 0.18181818181818182, 'Finance & Economics': 0.01680672268907563, 'Materials Science': 0.02531645569620253, 'Environmental Science': 0.0, 'Computational Biology': 0.0}\n",
      "Dataset_455: True=Transportation Systems, Pred=Physics & Mathematics, Scores={'Medical Science': 0.058823529411764705, 'Social Sciences': 0.058823529411764705, 'Synthetic Systems': 0.029411764705882353, 'Biology & Genetics': 0.058823529411764705, 'Computer Science & Artificial Intelligence': 0.11764705882352941, 'Engineering & Technology': 0.058823529411764705, 'Transportation Systems': 0.058823529411764705, 'Sports & Recreation': 0.08823529411764706, 'Veterinary Science': 0.07142857142857142, 'Physics & Mathematics': 0.18181818181818182, 'Finance & Economics': 0.0, 'Materials Science': 0.058823529411764705, 'Environmental Science': 0.058823529411764705, 'Computational Biology': 0.0}\n",
      "Dataset_1195: True=Transportation Systems, Pred=Physics & Mathematics, Scores={'Medical Science': 0.05333333333333334, 'Social Sciences': 0.05333333333333334, 'Synthetic Systems': 0.02666666666666667, 'Biology & Genetics': 0.10526315789473684, 'Computer Science & Artificial Intelligence': 0.05333333333333334, 'Engineering & Technology': 0.02666666666666667, 'Transportation Systems': 0.04, 'Sports & Recreation': 0.04, 'Veterinary Science': 0.07142857142857142, 'Physics & Mathematics': 0.18181818181818182, 'Finance & Economics': 0.0, 'Materials Science': 0.02666666666666667, 'Environmental Science': 0.05333333333333334, 'Computational Biology': 0.0}\n",
      "Dataset_967: True=Transportation Systems, Pred=Social Sciences, Scores={'Medical Science': 0.0, 'Social Sciences': 0.05128205128205128, 'Synthetic Systems': 0.02564102564102564, 'Biology & Genetics': 0.0, 'Computer Science & Artificial Intelligence': 0.05128205128205128, 'Engineering & Technology': 0.05128205128205128, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.02564102564102564, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.05128205128205128, 'Environmental Science': 0.05128205128205128, 'Computational Biology': 0.0}\n",
      "Dataset_120: True=Sports & Recreation, Pred=Computational Biology, Scores={'Medical Science': 0.09523809523809523, 'Social Sciences': 0.09523809523809523, 'Synthetic Systems': 0.0, 'Biology & Genetics': 0.0, 'Computer Science & Artificial Intelligence': 0.6666666666666666, 'Engineering & Technology': 0.14285714285714285, 'Transportation Systems': 0.047619047619047616, 'Sports & Recreation': 0.3333333333333333, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.0, 'Environmental Science': 0.8095238095238095, 'Computational Biology': 1.0}\n",
      "Dataset_40668: True=Sports & Recreation, Pred=Medical Science, Scores={'Medical Science': 0.9090909090909091, 'Social Sciences': 0.8181818181818182, 'Synthetic Systems': 0.36363636363636365, 'Biology & Genetics': 0.9090909090909091, 'Computer Science & Artificial Intelligence': 0.9090909090909091, 'Engineering & Technology': 0.09090909090909091, 'Transportation Systems': 0.8181818181818182, 'Sports & Recreation': 0.8181818181818182, 'Veterinary Science': 0.2727272727272727, 'Physics & Mathematics': 0.2727272727272727, 'Finance & Economics': 0.0, 'Materials Science': 0.2727272727272727, 'Environmental Science': 0.2727272727272727, 'Computational Biology': 0.0}\n",
      "Dataset_1385: True=Sports & Recreation, Pred=Sports & Recreation, Scores={'Medical Science': 0.015625, 'Social Sciences': 0.015625, 'Synthetic Systems': 0.0, 'Biology & Genetics': 0.05263157894736842, 'Computer Science & Artificial Intelligence': 0.0625, 'Engineering & Technology': 0.0625, 'Transportation Systems': 0.015625, 'Sports & Recreation': 0.15625, 'Veterinary Science': 0.03571428571428571, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.046875, 'Environmental Science': 0.0, 'Computational Biology': 0.0}\n",
      "Dataset_121: True=Veterinary Science, Pred=Physics & Mathematics, Scores={'Medical Science': 0.5294117647058824, 'Social Sciences': 0.35294117647058826, 'Synthetic Systems': 0.14705882352941177, 'Biology & Genetics': 0.35294117647058826, 'Computer Science & Artificial Intelligence': 0.47058823529411764, 'Engineering & Technology': 0.3235294117647059, 'Transportation Systems': 0.20588235294117646, 'Sports & Recreation': 0.5, 'Veterinary Science': 0.5, 'Physics & Mathematics': 1.0, 'Finance & Economics': 0.14705882352941177, 'Materials Science': 0.4411764705882353, 'Environmental Science': 0.14705882352941177, 'Computational Biology': 0.0}\n",
      "Dataset_581: True=Physics & Mathematics, Pred=Medical Science, Scores={'Medical Science': 0.0, 'Social Sciences': 0.0, 'Synthetic Systems': 0.0, 'Biology & Genetics': 0.0, 'Computer Science & Artificial Intelligence': 0.0, 'Engineering & Technology': 0.0, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.0, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.0, 'Environmental Science': 0.0, 'Computational Biology': 0.0}\n",
      "Dataset_335: True=Physics & Mathematics, Pred=Medical Science, Scores={'Medical Science': 1.0, 'Social Sciences': 1.0, 'Synthetic Systems': 0.36363636363636365, 'Biology & Genetics': 0.8181818181818182, 'Computer Science & Artificial Intelligence': 0.9090909090909091, 'Engineering & Technology': 0.5454545454545454, 'Transportation Systems': 0.7272727272727273, 'Sports & Recreation': 1.0, 'Veterinary Science': 0.7272727272727273, 'Physics & Mathematics': 0.6363636363636364, 'Finance & Economics': 0.0, 'Materials Science': 0.8181818181818182, 'Environmental Science': 0.36363636363636365, 'Computational Biology': 0.0}\n",
      "Dataset_31: True=Finance & Economics, Pred=Finance & Economics, Scores={'Medical Science': 0.15217391304347827, 'Social Sciences': 0.10869565217391304, 'Synthetic Systems': 0.021739130434782608, 'Biology & Genetics': 0.10526315789473684, 'Computer Science & Artificial Intelligence': 0.08695652173913043, 'Engineering & Technology': 0.08695652173913043, 'Transportation Systems': 0.06521739130434782, 'Sports & Recreation': 0.15217391304347827, 'Veterinary Science': 0.17857142857142858, 'Physics & Mathematics': 0.2727272727272727, 'Finance & Economics': 0.3695652173913043, 'Materials Science': 0.10869565217391304, 'Environmental Science': 0.06521739130434782, 'Computational Biology': 0.0}\n",
      "Dataset_41713: True=Finance & Economics, Pred=Biology & Genetics, Scores={'Medical Science': 0.0392156862745098, 'Social Sciences': 0.0392156862745098, 'Synthetic Systems': 0.0392156862745098, 'Biology & Genetics': 0.05263157894736842, 'Computer Science & Artificial Intelligence': 0.0392156862745098, 'Engineering & Technology': 0.0, 'Transportation Systems': 0.0392156862745098, 'Sports & Recreation': 0.0392156862745098, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.0, 'Environmental Science': 0.0, 'Computational Biology': 0.0}\n",
      "Dataset_41730: True=Finance & Economics, Pred=Biology & Genetics, Scores={'Medical Science': 0.047619047619047616, 'Social Sciences': 0.047619047619047616, 'Synthetic Systems': 0.047619047619047616, 'Biology & Genetics': 0.05263157894736842, 'Computer Science & Artificial Intelligence': 0.047619047619047616, 'Engineering & Technology': 0.0, 'Transportation Systems': 0.047619047619047616, 'Sports & Recreation': 0.047619047619047616, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.0, 'Environmental Science': 0.0, 'Computational Biology': 0.0}\n",
      "Dataset_40520: True=Materials Science, Pred=Engineering & Technology, Scores={'Medical Science': 0.05263157894736842, 'Social Sciences': 0.15789473684210525, 'Synthetic Systems': 0.05263157894736842, 'Biology & Genetics': 0.15789473684210525, 'Computer Science & Artificial Intelligence': 0.05263157894736842, 'Engineering & Technology': 0.631578947368421, 'Transportation Systems': 0.02631578947368421, 'Sports & Recreation': 0.13157894736842105, 'Veterinary Science': 0.07142857142857142, 'Physics & Mathematics': 0.09090909090909091, 'Finance & Economics': 0.0, 'Materials Science': 0.5, 'Environmental Science': 0.05263157894736842, 'Computational Biology': 0.0}\n",
      "Dataset_1368: True=Materials Science, Pred=Engineering & Technology, Scores={'Medical Science': 0.06976744186046512, 'Social Sciences': 0.13953488372093023, 'Synthetic Systems': 0.046511627906976744, 'Biology & Genetics': 0.18421052631578946, 'Computer Science & Artificial Intelligence': 0.06976744186046512, 'Engineering & Technology': 0.9302325581395349, 'Transportation Systems': 0.023255813953488372, 'Sports & Recreation': 0.09302325581395349, 'Veterinary Science': 0.07142857142857142, 'Physics & Mathematics': 0.09090909090909091, 'Finance & Economics': 0.0, 'Materials Science': 0.7674418604651163, 'Environmental Science': 0.06976744186046512, 'Computational Biology': 0.0}\n",
      "Dataset_339: True=Environmental Science, Pred=Veterinary Science, Scores={'Medical Science': 0.0297029702970297, 'Social Sciences': 0.0297029702970297, 'Synthetic Systems': 0.019801980198019802, 'Biology & Genetics': 0.02631578947368421, 'Computer Science & Artificial Intelligence': 0.019801980198019802, 'Engineering & Technology': 0.009900990099009901, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.039603960396039604, 'Veterinary Science': 0.10714285714285714, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.02531645569620253, 'Environmental Science': 0.009900990099009901, 'Computational Biology': 0.0}\n",
      "Dataset_1482: True=Environmental Science, Pred=Biology & Genetics, Scores={'Medical Science': 0.0273972602739726, 'Social Sciences': 0.0136986301369863, 'Synthetic Systems': 0.0136986301369863, 'Biology & Genetics': 0.05263157894736842, 'Computer Science & Artificial Intelligence': 0.0273972602739726, 'Engineering & Technology': 0.0136986301369863, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.0273972602739726, 'Veterinary Science': 0.03571428571428571, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.0273972602739726, 'Environmental Science': 0.0, 'Computational Biology': 0.0}\n",
      "Dataset_1493: True=Environmental Science, Pred=Environmental Science, Scores={'Medical Science': 0.0, 'Social Sciences': 0.0, 'Synthetic Systems': 0.0, 'Biology & Genetics': 0.0, 'Computer Science & Artificial Intelligence': 0.0, 'Engineering & Technology': 0.0, 'Transportation Systems': 0.0, 'Sports & Recreation': 0.0, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.0, 'Environmental Science': 0.08571428571428572, 'Computational Biology': 0.0}\n",
      "Dataset_3212: True=Computational Biology, Pred=Medical Science, Scores={'Medical Science': 0.6666666666666666, 'Social Sciences': 0.6666666666666666, 'Synthetic Systems': 0.6666666666666666, 'Biology & Genetics': 0.6666666666666666, 'Computer Science & Artificial Intelligence': 0.6666666666666666, 'Engineering & Technology': 0.0, 'Transportation Systems': 0.6666666666666666, 'Sports & Recreation': 0.6666666666666666, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.0, 'Environmental Science': 0.16666666666666666, 'Computational Biology': 0.0}\n",
      "Dataset_3221: True=Computational Biology, Pred=Medical Science, Scores={'Medical Science': 0.6666666666666666, 'Social Sciences': 0.6666666666666666, 'Synthetic Systems': 0.6666666666666666, 'Biology & Genetics': 0.6666666666666666, 'Computer Science & Artificial Intelligence': 0.6666666666666666, 'Engineering & Technology': 0.0, 'Transportation Systems': 0.6666666666666666, 'Sports & Recreation': 0.6666666666666666, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.0, 'Environmental Science': 0.16666666666666666, 'Computational Biology': 0.0}\n",
      "Dataset_3819: True=Computational Biology, Pred=Medical Science, Scores={'Medical Science': 0.6666666666666666, 'Social Sciences': 0.6666666666666666, 'Synthetic Systems': 0.6666666666666666, 'Biology & Genetics': 0.6666666666666666, 'Computer Science & Artificial Intelligence': 0.6666666666666666, 'Engineering & Technology': 0.0, 'Transportation Systems': 0.6666666666666666, 'Sports & Recreation': 0.6666666666666666, 'Veterinary Science': 0.0, 'Physics & Mathematics': 0.0, 'Finance & Economics': 0.0, 'Materials Science': 0.0, 'Environmental Science': 0.16666666666666666, 'Computational Biology': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 12:48:07,400 - INFO - Матрица ошибок сохранена в confusion_matrix.png\n",
      "2025-08-21 12:48:07,406 - INFO - Результаты сохранены в classification_results.csv\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = \"all_datasets\"\n",
    "TAG_FILE = \"unique_tags.json\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('dataset_processing.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Основные функции D4 \n",
    "def compute_eqs(term_to_cols):\n",
    "    eq_dict = defaultdict(list)\n",
    "    for term, cols in term_to_cols.items():\n",
    "        key = tuple(sorted(cols))\n",
    "        eq_dict[key].append(term)\n",
    "    eqs = list(eq_dict.values())\n",
    "    term_to_eq = {term: i for i, eq in enumerate(eqs) for term in eq}\n",
    "    return eqs, term_to_eq\n",
    "\n",
    "def columns_to_eq(columns, eqs, term_to_eq):\n",
    "    eq_columns = []\n",
    "    for col in columns:\n",
    "        eq_col = set(term_to_eq.get(term, -1) for term in col if term in term_to_eq)\n",
    "        if -1 not in eq_col:\n",
    "            eq_columns.append(eq_col)\n",
    "    return eq_columns\n",
    "\n",
    "def eq_to_terms(eq_set, eqs):\n",
    "    terms = set()\n",
    "    for e in eq_set:\n",
    "        terms.update(eqs[e])\n",
    "    return terms\n",
    "\n",
    "def jaccard_eq(e1, e2, eq_to_cols):\n",
    "    c1 = eq_to_cols.get(e1, set())\n",
    "    c2 = eq_to_cols.get(e2, set())\n",
    "    inter = len(c1 & c2)\n",
    "    union = len(c1 | c2)\n",
    "    return inter / union if union > 0 else 0.0\n",
    "\n",
    "def context_signature_eq(e, all_eqs, eq_to_cols):\n",
    "    sig = [(e2, jaccard_eq(e, e2, eq_to_cols)) for e2 in all_eqs if e2 != e and jaccard_eq(e, e2, eq_to_cols) > 0]\n",
    "    sig.sort(key=lambda x: -x[1])\n",
    "    return sig\n",
    "\n",
    "def drop_index(sim_vec, start):\n",
    "    if len(sim_vec) <= start + 1:\n",
    "        return len(sim_vec)\n",
    "    max_diff = -1\n",
    "    drop_idx = start\n",
    "    for i in range(start, len(sim_vec) - 1):\n",
    "        diff = sim_vec[i] - sim_vec[i + 1]\n",
    "        if diff > max_diff:\n",
    "            max_diff = diff\n",
    "            drop_idx = i + 1\n",
    "    return drop_idx\n",
    "\n",
    "def signature_blocks_eq(e, all_eqs, eq_to_cols):\n",
    "    sig = context_signature_eq(e, all_eqs, eq_to_cols)\n",
    "    if not sig:\n",
    "        return []\n",
    "    terms, sims = zip(*sig) if sig else ([], [])\n",
    "    blocks = []\n",
    "    start = 0\n",
    "    while start < len(sims):\n",
    "        drop = drop_index(sims, start)\n",
    "        block = set(terms[start:drop])\n",
    "        blocks.append(block)\n",
    "        start = drop\n",
    "    return blocks\n",
    "\n",
    "def liberal_blocks(blocks):\n",
    "    if not blocks:\n",
    "        return []\n",
    "    sizes = [len(b) for b in blocks]\n",
    "    max_idx = np.argmax(sizes)\n",
    "    return blocks[:max_idx]\n",
    "\n",
    "def prune_centrist_eq(e, C_eq, blocks):\n",
    "    x = [(set(), 0.0)]\n",
    "    lib_blocks = liberal_blocks(blocks)\n",
    "    for B in lib_blocks:\n",
    "        inter = len(B & C_eq)\n",
    "        rel = inter / len(B) if len(B) > 0 else 0.0\n",
    "        x.append((B, rel))\n",
    "    x.sort(key=lambda y: -y[1])\n",
    "    rel_vec = [rel for _, rel in x]\n",
    "    drop = drop_index(rel_vec, 1)\n",
    "    robust = set()\n",
    "    for i in range(1, drop):\n",
    "        robust.update(x[i][0])\n",
    "    return robust\n",
    "\n",
    "def prune_conservative_eq(blocks):\n",
    "    return blocks[0] if blocks else set()\n",
    "\n",
    "def column_expand(C_eq, tau_sup, delta_dec, all_eqs, eq_to_cols, eqs, pruning_strategy='conservative'):\n",
    "    C_plus = C_eq.copy()\n",
    "    tau_col = tau_sup\n",
    "    while tau_col > 0:\n",
    "        C_prime = set()\n",
    "        if pruning_strategy == 'conservative':\n",
    "            robsigs = {t: prune_conservative_eq(signature_blocks_eq(t, all_eqs, eq_to_cols)) for t in C_plus}\n",
    "        else:\n",
    "            robsigs = {t: prune_centrist_eq(t, C_plus, signature_blocks_eq(t, all_eqs, eq_to_cols)) for t in C_plus}\n",
    "        candidates = set()\n",
    "        for robsig in robsigs.values():\n",
    "            candidates.update(robsig)\n",
    "        candidates -= C_plus\n",
    "        for t_prime in candidates:\n",
    "            S = set(t for t in C_plus if t_prime in robsigs[t])\n",
    "            if len(S) / len(C_plus) > tau_sup and len(S & C_eq) / len(C_eq) > tau_col:\n",
    "                C_prime.add(t_prime)\n",
    "        if C_prime:\n",
    "            C_plus.update(C_prime)\n",
    "            tau_col -= delta_dec\n",
    "        else:\n",
    "            break\n",
    "    return C_plus\n",
    "\n",
    "def local_domains(C_plus, all_eqs, eq_to_cols, eqs, pruning_strategy='conservative'):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(C_plus)\n",
    "    for t1, t2 in combinations(C_plus, 2):\n",
    "        if pruning_strategy == 'conservative':\n",
    "            robsig1 = prune_conservative_eq(signature_blocks_eq(t1, all_eqs, eq_to_cols))\n",
    "            robsig2 = prune_conservative_eq(signature_blocks_eq(t2, all_eqs, eq_to_cols))\n",
    "        else:\n",
    "            robsig1 = prune_centrist_eq(t1, C_plus, signature_blocks_eq(t1, all_eqs, eq_to_cols))\n",
    "            robsig2 = prune_centrist_eq(t2, C_plus, signature_blocks_eq(t2, all_eqs, eq_to_cols))\n",
    "        if t2 in robsig1 or t1 in robsig2:\n",
    "            G.add_edge(t1, t2)\n",
    "    components = list(nx.connected_components(G))\n",
    "    local = [set(comp) for comp in components if len(comp) > 1]\n",
    "    return local\n",
    "\n",
    "def jaccard_sets(s1, s2):\n",
    "    inter = len(s1 & s2)\n",
    "    union = len(s1 | s2)\n",
    "    return inter / union if union > 0 else 0.0\n",
    "\n",
    "def domain_support(L, tau, all_local_domains):\n",
    "    support = 0\n",
    "    for other_locals in all_local_domains:\n",
    "        for other_L in other_locals:\n",
    "            if jaccard_sets(L, other_L) > tau:\n",
    "                support += 1\n",
    "                break\n",
    "    return support\n",
    "\n",
    "def strong_domains(all_local_domains, tau_dsim, tau_est, tau_strong):\n",
    "    strong = []\n",
    "    for locals in all_local_domains:\n",
    "        for L in locals:\n",
    "            sup_dsim = domain_support(L, tau_dsim, all_local_domains)\n",
    "            sup_est = domain_support(L, tau_est, all_local_domains)\n",
    "            if sup_est > 0 and sup_dsim / sup_est > tau_strong:\n",
    "                strong.append(L)\n",
    "    return strong\n",
    "\n",
    "def generate_domain_terms(domain_representative_datasets, pruning_strategy='conservative', use_column_expansion=True, tau_dsim=0.3, tau_est=0.05, tau_strong=0.1):\n",
    "    all_columns = []\n",
    "    for dataset in tqdm(domain_representative_datasets, desc=\"Processing datasets for domain terms\"):\n",
    "        all_columns.extend(dataset)\n",
    "    \n",
    "    if not all_columns:\n",
    "        return set()\n",
    "    \n",
    "    term_to_cols = defaultdict(set)\n",
    "    for i, col in enumerate(all_columns):\n",
    "        for term in col:\n",
    "            term_to_cols[term].add(i)\n",
    "    \n",
    "    eqs, term_to_eq = compute_eqs(term_to_cols)\n",
    "    if not eqs:\n",
    "        all_terms = set()\n",
    "        for col in all_columns:\n",
    "            all_terms.update(col)\n",
    "        return all_terms\n",
    "    \n",
    "    eq_columns = columns_to_eq(all_columns, eqs, term_to_eq)\n",
    "    eq_to_cols = defaultdict(set)\n",
    "    for term, cols in term_to_cols.items():\n",
    "        eq_idx = term_to_eq[term]\n",
    "        eq_to_cols[eq_idx].update(cols)\n",
    "    \n",
    "    all_eqs = list(range(len(eqs)))\n",
    "    all_local_domains = []\n",
    "    for C_eq in tqdm(eq_columns, desc=\"Computing local domains\"):\n",
    "        if use_column_expansion:\n",
    "            C_plus = column_expand(C_eq, tau_sup=0.25, delta_dec=0.05, all_eqs=all_eqs, eq_to_cols=eq_to_cols, eqs=eqs, pruning_strategy=pruning_strategy)\n",
    "        else:\n",
    "            C_plus = C_eq\n",
    "        local = local_domains(C_plus, all_eqs, eq_to_cols, eqs, pruning_strategy=pruning_strategy)\n",
    "        all_local_domains.append(local)\n",
    "    \n",
    "    strong = strong_domains(all_local_domains, tau_dsim, tau_est, tau_strong)\n",
    "    \n",
    "    if not strong:\n",
    "        all_terms = set()\n",
    "        for col in all_columns:\n",
    "            all_terms.update(col)\n",
    "        return all_terms\n",
    "    \n",
    "    strong_terms = [eq_to_terms(L, eqs) for L in strong]\n",
    "    \n",
    "    domain_terms = set()\n",
    "    for dom in strong_terms:\n",
    "        domain_terms.update(dom)\n",
    "    \n",
    "    return domain_terms\n",
    "\n",
    "def filter_common_terms(domain_terms_dict, min_domain_freq=0.8):\n",
    "    \"\"\"Фильтрация терминов, которые встречаются в большинстве доменов\"\"\"\n",
    "    term_counts = defaultdict(int)\n",
    "    total_domains = len(domain_terms_dict)\n",
    "    \n",
    "    for domain, terms in domain_terms_dict.items():\n",
    "        for term in terms:\n",
    "            term_counts[term] += 1\n",
    "    \n",
    "    common_terms = {term for term, count in term_counts.items() if count / total_domains >= min_domain_freq}\n",
    "    \n",
    "    filtered_dict = {}\n",
    "    for domain, terms in domain_terms_dict.items():\n",
    "        filtered_dict[domain] = terms - common_terms\n",
    "    \n",
    "    return filtered_dict\n",
    "\n",
    "def generate_multi_domain_terms(domains_representative):\n",
    "    domain_terms_dict = {}\n",
    "    for domain, reps in tqdm(domains_representative.items(), desc=\"Generating domain terms\"):\n",
    "        domain_terms = generate_domain_terms(reps)\n",
    "        domain_terms_dict[domain] = domain_terms\n",
    "    return filter_common_terms(domain_terms_dict)\n",
    "\n",
    "def compute_similarity(dataset_columns, domain_terms):\n",
    "    extracted_terms = set()\n",
    "    for col in dataset_columns:\n",
    "        extracted_terms.update(col)\n",
    "    \n",
    "    matched_terms = extracted_terms.intersection(domain_terms)\n",
    "    sim_score = len(matched_terms) / min(len(extracted_terms), len(domain_terms)) if min(len(extracted_terms), len(domain_terms)) > 0 else 0.0\n",
    "    return sim_score\n",
    "\n",
    "def classify_dataset(dataset_columns, domain_terms_dict):\n",
    "    scores = {}\n",
    "    max_score = -1\n",
    "    predicted_domain = None\n",
    "    for domain, terms in domain_terms_dict.items():\n",
    "        score = compute_similarity(dataset_columns, terms)\n",
    "        scores[domain] = score\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            predicted_domain = domain\n",
    "    return predicted_domain, scores\n",
    "\n",
    "def bootstrap_metrics(true_labels, pred_labels, n_iterations=1000):\n",
    "    n_samples = len(true_labels)\n",
    "    if n_samples == 0:\n",
    "        return {key: 0.0 for key in ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro', 'precision_weighted', 'recall_weighted', 'f1_weighted']}\n",
    "    \n",
    "    accuracies = []\n",
    "    precisions_macro = []\n",
    "    recalls_macro = []\n",
    "    f1s_macro = []\n",
    "    precisions_weighted = []\n",
    "    recalls_weighted = []\n",
    "    f1s_weighted = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        boot_true = [true_labels[i] for i in indices]\n",
    "        boot_pred = [pred_labels[i] for i in indices]\n",
    "        \n",
    "        accuracies.append(accuracy_score(boot_true, boot_pred))\n",
    "        precisions_macro.append(precision_score(boot_true, boot_pred, average='macro', zero_division=0))\n",
    "        recalls_macro.append(recall_score(boot_true, boot_pred, average='macro', zero_division=0))\n",
    "        f1s_macro.append(f1_score(boot_true, boot_pred, average='macro', zero_division=0))\n",
    "        precisions_weighted.append(precision_score(boot_true, boot_pred, average='weighted', zero_division=0))\n",
    "        recalls_weighted.append(recall_score(boot_true, boot_pred, average='weighted', zero_division=0))\n",
    "        f1s_weighted.append(f1_score(boot_true, boot_pred, average='weighted', zero_division=0))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': np.std(accuracies),\n",
    "        'precision_macro': np.std(precisions_macro),\n",
    "        'recall_macro': np.std(recalls_macro),\n",
    "        'f1_macro': np.std(f1s_macro),\n",
    "        'precision_weighted': np.std(precisions_weighted),\n",
    "        'recall_weighted': np.std(recalls_weighted),\n",
    "        'f1_weighted': np.std(f1s_weighted)\n",
    "    }\n",
    "\n",
    "def compute_classification_metrics(true_labels, pred_labels, unique_labels):\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision_macro = precision_score(true_labels, pred_labels, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(true_labels, pred_labels, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(true_labels, pred_labels, average='macro', zero_division=0)\n",
    "    precision_weighted = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    recall_weighted = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    f1_weighted = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'f1_weighted': f1_weighted\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(cm, unique_labels, output_path=\"confusion_matrix.png\"):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    logger.info(f\"Матрица ошибок сохранена в {output_path}\")\n",
    "\n",
    "def save_results(metrics_data, predictions, output_path=\"classification_results.csv\"):\n",
    "    results_df = pd.DataFrame(metrics_data)\n",
    "    predictions_df = pd.DataFrame(predictions, columns=[\"Dataset\", \"True Domain\", \"Predicted Domain\", \"Scores\"])\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(\"=== Метрики качества ===\\n\")\n",
    "        results_df.to_csv(f, index=False)\n",
    "        f.write(\"\\n=== Предсказания ===\\n\")\n",
    "        predictions_df.to_csv(f, index=False)\n",
    "    logger.info(f\"Результаты сохранены в {output_path}\")\n",
    "\n",
    "def discretize_numeric_column(column, bins=10):\n",
    "    \"\"\"Дискретизация числового столбца в строковые категории.\"\"\"\n",
    "    try:\n",
    "        valid_data = pd.to_numeric(column.dropna(), errors='coerce')\n",
    "        if valid_data.empty:\n",
    "            return set()\n",
    "        \n",
    "        min_val, max_val = valid_data.min(), valid_data.max()\n",
    "        if min_val == max_val:\n",
    "            return {str(min_val)}\n",
    "\n",
    "        bin_edges = pd.qcut(valid_data, q=bins, duplicates='drop').cat.categories\n",
    "        if len(bin_edges) < 2:\n",
    "            return {str(min_val)}\n",
    "        \n",
    "        labels = [f\"{bin_edges[i].left:.2f}-{bin_edges[i].right:.2f}\" for i in range(len(bin_edges))]\n",
    "        discretized = pd.cut(valid_data, bins=bin_edges, labels=labels, include_lowest=True, ordered=False)\n",
    "        return set(discretized.dropna().astype(str))\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Ошибка дискретизации столбца: {str(e)}\")\n",
    "        return set()\n",
    "\n",
    "\n",
    "def load_datasets_as_columns():\n",
    "    dataset_columns = []\n",
    "    unique_domains = set()\n",
    "    \n",
    "    subdirs = [d for d in os.listdir(OUTPUT_DIR) if os.path.isdir(os.path.join(OUTPUT_DIR, d))]\n",
    "    for subdir in tqdm(subdirs, desc=\"Loading datasets\"):\n",
    "        try:\n",
    "            dataset_id = int(subdir)\n",
    "            metadata_path = os.path.join(OUTPUT_DIR, subdir, \"metadata.json\")\n",
    "            csv_path = os.path.join(OUTPUT_DIR, subdir, \"random_rows.csv\")\n",
    "            \n",
    "            if not os.path.exists(metadata_path) or not os.path.exists(csv_path):\n",
    "                logger.warning(f\"Пропущен {subdir}: отсутствует metadata.json или random_rows.csv\")\n",
    "                continue\n",
    "            \n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            domain = metadata.get('domain', None)\n",
    "            if not domain:\n",
    "                logger.warning(f\"Домен отсутствует в {subdir}\")\n",
    "                continue\n",
    "            \n",
    "            data = pd.read_csv(csv_path)\n",
    "            columns_sets = []\n",
    "            for col in data.columns:\n",
    "                if data[col].dtype == 'object':\n",
    "                    unique_vals = set(data[col].dropna().astype(str))\n",
    "                    if unique_vals:\n",
    "                        columns_sets.append(unique_vals)\n",
    "                else:\n",
    "                    unique_vals = discretize_numeric_column(data[col])\n",
    "                    if unique_vals:\n",
    "                        columns_sets.append(unique_vals)\n",
    "            \n",
    "            if columns_sets:\n",
    "                dataset_columns.append((dataset_id, columns_sets, domain))\n",
    "                unique_domains.add(domain)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка загрузки {subdir}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return dataset_columns, unique_domains\n",
    "\n",
    "def stratified_train_test_split(dataset_columns, test_size=0.2):\n",
    "    domain_groups = defaultdict(list)\n",
    "    for ds_id, cols, dom in dataset_columns:\n",
    "        domain_groups[dom].append((ds_id, cols, dom))\n",
    "    \n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for dom, group in domain_groups.items():\n",
    "        random.shuffle(group)\n",
    "        n = len(group)\n",
    "        n_test = max(1, int(n * test_size)) if n > 1 else 0\n",
    "        n_train = n - n_test\n",
    "        \n",
    "        if n_train == 0 and n > 0:\n",
    "            n_train = 1\n",
    "            n_test = 0\n",
    "        \n",
    "        train_data.extend(group[:n_train])\n",
    "        test_data.extend(group[n_train:])\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def evaluate_classification(test_datasets, domain_terms_dict):\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    predictions = []\n",
    "    for name, columns, true_domain in tqdm(test_datasets, desc=\"Classifying datasets\"):\n",
    "        pred_domain, scores = classify_dataset(columns, domain_terms_dict)\n",
    "        true_labels.append(true_domain)\n",
    "        pred_labels.append(pred_domain)\n",
    "        predictions.append((name, true_domain, pred_domain, scores))\n",
    "    \n",
    "    unique_labels = list(set(true_labels + pred_labels))\n",
    "    results = compute_classification_metrics(true_labels, pred_labels, unique_labels)\n",
    "    cm = confusion_matrix(true_labels, pred_labels, labels=unique_labels)\n",
    "    std_metrics = bootstrap_metrics(true_labels, pred_labels)\n",
    "    \n",
    "    return results, cm, unique_labels, predictions, std_metrics\n",
    "\n",
    "def main():\n",
    "    # Загрузка датасетов\n",
    "    dataset_columns, unique_domains = load_datasets_as_columns()\n",
    "    \n",
    "    if not dataset_columns:\n",
    "        logger.error(\"Нет подходящих датасетов для обработки!\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Загружено датасетов: {len(dataset_columns)}\")\n",
    "    logger.info(f\"Уникальных доменов: {len(unique_domains)}\")\n",
    "    \n",
    "    # Стратифицированное разбиение\n",
    "    train_data, test_data = stratified_train_test_split(dataset_columns)\n",
    "    \n",
    "    logger.info(f\"Train датасетов: {len(train_data)}\")\n",
    "    logger.info(f\"Test датасетов: {len(test_data)}\")\n",
    "    \n",
    "    domains_representative_train = defaultdict(list)\n",
    "    for _, cols, dom in train_data:\n",
    "        domains_representative_train[dom].append(cols)\n",
    "    \n",
    "    # Генерация domain terms\n",
    "    domain_terms_dict = generate_multi_domain_terms(domains_representative_train)\n",
    "    \n",
    "    print(\"Domain Terms Dict:\")\n",
    "    for domain, terms in domain_terms_dict.items():\n",
    "        print(f\"{domain}: {sorted(terms)[:10]}...\")  \n",
    "    \n",
    "    test_datasets = [(f\"Dataset_{ds_id}\", cols, dom) for ds_id, cols, dom in test_data]\n",
    "    \n",
    "    results, cm, unique_labels, predictions, std_metrics = evaluate_classification(test_datasets, domain_terms_dict)\n",
    "    \n",
    "    print(\"\\n=== Метрики качества ===\")\n",
    "    metrics_data = [\n",
    "        {'Metric': 'Accuracy', 'Value': f\"{results['accuracy']:.4f} ± {std_metrics['accuracy']:.4f}\"},\n",
    "        {'Metric': 'Precision (Macro)', 'Value': f\"{results['precision_macro']:.4f} ± {std_metrics['precision_macro']:.4f}\"},\n",
    "        {'Metric': 'Recall (Macro)', 'Value': f\"{results['recall_macro']:.4f} ± {std_metrics['recall_macro']:.4f}\"},\n",
    "        {'Metric': 'F1-Score (Macro)', 'Value': f\"{results['f1_macro']:.4f} ± {std_metrics['f1_macro']:.4f}\"},\n",
    "        {'Metric': 'Precision (Weighted)', 'Value': f\"{results['precision_weighted']:.4f} ± {std_metrics['precision_weighted']:.4f}\"},\n",
    "        {'Metric': 'Recall (Weighted)', 'Value': f\"{results['recall_weighted']:.4f} ± {std_metrics['recall_weighted']:.4f}\"},\n",
    "        {'Metric': 'F1-Score (Weighted)', 'Value': f\"{results['f1_weighted']:.4f} ± {std_metrics['f1_weighted']:.4f}\"}\n",
    "    ]\n",
    "    for metric in metrics_data:\n",
    "        print(f\"{metric['Metric']}: {metric['Value']}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "    print(\"Unique Labels:\", unique_labels)\n",
    "    print(\"\\nPredictions:\")\n",
    "    for name, true, pred, scores in predictions:\n",
    "        print(f\"{name}: True={true}, Pred={pred}, Scores={scores}\")\n",
    "\n",
    "    plot_confusion_matrix(cm, unique_labels)\n",
    "    save_results(metrics_data, predictions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb1864-f24f-4ff4-8af6-1d3b2a48206e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Classification based on metafeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb0c21c6-e628-4d81-9236-3314754bf30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Извлечение мета-признаков: 100%|█████████████████████████████████████████████████████| 258/258 [02:49<00:00,  1.52it/s]\n",
      "2025-08-21 14:00:43,037 - INFO - Мета-признаки сохранены в meta_features_with_domains.csv\n",
      "2025-08-21 14:00:43,037 - INFO - Успешно обработано: 258 датасетов\n",
      "2025-08-21 14:00:43,038 - INFO - Ошибки обработки: 0 датасетов\n",
      "2025-08-21 14:00:43,039 - INFO - \n",
      "Распределение доменов:\n",
      "2025-08-21 14:00:43,040 - INFO - Computer Science & Artificial Intelligence    41\n",
      "Medical Science                               30\n",
      "Engineering & Technology                      22\n",
      "Sports & Recreation                           19\n",
      "Social Sciences                               18\n",
      "Finance & Economics                           17\n",
      "Synthetic Systems                             16\n",
      "Transportation Systems                        16\n",
      "Environmental Science                         16\n",
      "Computational Biology                         16\n",
      "Biology & Genetics                            15\n",
      "Physics & Mathematics                         14\n",
      "Materials Science                             14\n",
      "Veterinary Science                             4\n",
      "2025-08-21 14:00:43,044 - INFO - После удаления столбцов с пропусками (>50%): 47 признаков\n",
      "2025-08-21 14:00:43,052 - INFO - После удаления констант: 40 признаков\n",
      "2025-08-21 14:00:43,075 - INFO - После удаления дубликатов: 39 признаков\n",
      "2025-08-21 14:00:43,132 - INFO - После VIF-фильтрации (VIF < 10): 6 признаков\n",
      "2025-08-21 14:00:43,140 - INFO - Отобрано 6 мета-признаков из 6 с помощью ANOVA F-test\n",
      "2025-08-21 14:00:43,141 - INFO - Используем отобранные мета-признаки: ['sparsity.mean', 'nr_cor_attr', 'sparsity.sd', 'inst_to_attr', 'nr_bin', 'nr_outliers']\n",
      "2025-08-21 14:00:43,142 - INFO - \n",
      "Начало кросс-валидации (StratifiedKFold)\n",
      "2025-08-21 14:00:43,144 - INFO - Оценка модели: RandomForest\n",
      "2025-08-21 14:00:44,844 - INFO -   Точность: 0.4302 ± 0.0330\n",
      "2025-08-21 14:00:44,844 - INFO -   Precision: 0.4230 ± 0.0225\n",
      "2025-08-21 14:00:44,845 - INFO -   Recall: 0.4253 ± 0.0214\n",
      "2025-08-21 14:00:44,846 - INFO -   F1 (macro): 0.4074 ± 0.0096\n",
      "2025-08-21 14:00:44,846 - INFO -   F1 (weighted): 0.4128 ± 0.0230\n",
      "2025-08-21 14:00:44,847 - INFO - Оценка модели: LogisticRegression\n",
      "2025-08-21 14:00:45,016 - INFO -   Точность: 0.2557 ± 0.0372\n",
      "2025-08-21 14:00:45,017 - INFO -   Precision: 0.2231 ± 0.0608\n",
      "2025-08-21 14:00:45,017 - INFO -   Recall: 0.2810 ± 0.0368\n",
      "2025-08-21 14:00:45,018 - INFO -   F1 (macro): 0.2225 ± 0.0355\n",
      "2025-08-21 14:00:45,018 - INFO -   F1 (weighted): 0.2108 ± 0.0428\n",
      "2025-08-21 14:00:45,019 - INFO - Оценка модели: SVM\n",
      "2025-08-21 14:00:45,381 - INFO -   Точность: 0.2907 ± 0.0366\n",
      "2025-08-21 14:00:45,382 - INFO -   Precision: 0.2766 ± 0.0267\n",
      "2025-08-21 14:00:45,382 - INFO -   Recall: 0.3176 ± 0.0427\n",
      "2025-08-21 14:00:45,383 - INFO -   F1 (macro): 0.2769 ± 0.0305\n",
      "2025-08-21 14:00:45,383 - INFO -   F1 (weighted): 0.2594 ± 0.0365\n",
      "2025-08-21 14:00:45,384 - INFO - Оценка модели: KNN\n",
      "2025-08-21 14:00:45,536 - INFO -   Точность: 0.2830 ± 0.0296\n",
      "2025-08-21 14:00:45,537 - INFO -   Precision: 0.2528 ± 0.0626\n",
      "2025-08-21 14:00:45,538 - INFO -   Recall: 0.2818 ± 0.0504\n",
      "2025-08-21 14:00:45,538 - INFO -   F1 (macro): 0.2526 ± 0.0513\n",
      "2025-08-21 14:00:45,539 - INFO -   F1 (weighted): 0.2516 ± 0.0366\n",
      "2025-08-21 14:00:45,539 - INFO - \n",
      "Лучшая модель: RandomForest с точностью 0.4302 ± 0.0330\n",
      "2025-08-21 14:00:45,754 - INFO - Лучшая модель сохранена в domain_classifier.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Финальный отчет классификации:\n",
      "================================================================================\n",
      "                                            precision    recall  f1-score   support\n",
      "\n",
      "                        Biology & Genetics       0.11      0.13      0.12        15\n",
      "                     Computational Biology       0.74      0.88      0.80        16\n",
      "Computer Science & Artificial Intelligence       0.30      0.32      0.31        41\n",
      "                  Engineering & Technology       0.53      0.41      0.46        22\n",
      "                     Environmental Science       0.12      0.06      0.08        16\n",
      "                       Finance & Economics       0.88      0.88      0.88        17\n",
      "                         Materials Science       0.50      0.50      0.50        14\n",
      "                           Medical Science       0.41      0.50      0.45        30\n",
      "                     Physics & Mathematics       0.59      0.71      0.65        14\n",
      "                           Social Sciences       0.12      0.11      0.11        18\n",
      "                       Sports & Recreation       0.40      0.32      0.35        19\n",
      "                         Synthetic Systems       0.54      0.44      0.48        16\n",
      "                    Transportation Systems       0.48      0.62      0.54        16\n",
      "                        Veterinary Science       0.00      0.00      0.00         4\n",
      "\n",
      "                                  accuracy                           0.43       258\n",
      "                                 macro avg       0.41      0.42      0.41       258\n",
      "                              weighted avg       0.42      0.43      0.42       258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = \"all_datasets\"\n",
    "RESULTS_DIR = \"meta_features_results\"\n",
    "MODEL_SAVE_PATH = \"domain_classifier.pkl\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('domain_classification.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def anova_feature_selection(X, y, k=10):\n",
    "    \"\"\"Отбор мета-характеристик с помощью ANOVA F-test\"\"\"\n",
    "    if len(np.unique(y)) < 2:\n",
    "        logger.warning(\"ANOVA F-test требует как минимум 2 класса, возвращаем все признаки\")\n",
    "        return X.columns.tolist(), np.ones(X.shape[1]), None\n",
    "    \n",
    "    f_scores, p_values = f_classif(X, y)\n",
    "    \n",
    "    sorted_indices = np.argsort(f_scores)[::-1]\n",
    "\n",
    "    selected_indices = sorted_indices[:min(k, len(sorted_indices))]\n",
    "    selected_columns = X.columns[selected_indices].tolist()\n",
    "    \n",
    "    logger.info(f\"Отобрано {len(selected_columns)} мета-признаков из {X.shape[1]} с помощью ANOVA F-test\")\n",
    "    return selected_columns, f_scores, None\n",
    "\n",
    "def preprocess_features(X):\n",
    "    threshold = 0.5\n",
    "    X = X.loc[:, X.isna().mean() < threshold]\n",
    "    logger.info(f\"После удаления столбцов с пропусками (>50%): {X.shape[1]} признаков\")\n",
    "\n",
    "    X = X.loc[:, X.nunique() > 1]\n",
    "    logger.info(f\"После удаления констант: {X.shape[1]} признаков\")\n",
    "\n",
    "    X = X.loc[:, ~X.T.duplicated()]\n",
    "    logger.info(f\"После удаления дубликатов: {X.shape[1]} признаков\")\n",
    "\n",
    "    # VIF-фильтрация\n",
    "    try:\n",
    "        X_imputed = SimpleImputer(strategy='median').fit_transform(X)\n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data[\"feature\"] = X.columns\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(X_imputed, i) for i in range(X_imputed.shape[1])]\n",
    "        selected_features = vif_data[vif_data[\"VIF\"] < 10][\"feature\"]\n",
    "        X = X[selected_features]\n",
    "        logger.info(f\"После VIF-фильтрации (VIF < 10): {X.shape[1]} признаков\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Ошибка VIF-фильтрации: {str(e)}. Пропускаем VIF.\")\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def extract_meta_features():\n",
    "    \"\"\"Извлекает мета-характеристики для всех датасетов\"\"\"\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    dataset_ids = [d for d in os.listdir(OUTPUT_DIR) \n",
    "                   if os.path.isdir(os.path.join(OUTPUT_DIR, d))]\n",
    "    \n",
    "    all_features = []\n",
    "    failed_datasets = []\n",
    "    domains = []\n",
    "\n",
    "    for dataset_id in tqdm(dataset_ids, desc=\"Извлечение мета-признаков\"):\n",
    "        try:\n",
    "            meta_path = os.path.join(OUTPUT_DIR, dataset_id, \"metadata.json\")\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            domain = metadata.get(\"domain\")\n",
    "            if not domain:\n",
    "                logger.warning(f\"Домен отсутствует для датасета {dataset_id}\")\n",
    "                continue\n",
    "                \n",
    "            data_path = os.path.join(OUTPUT_DIR, dataset_id, \"random_rows.csv\")\n",
    "            if not os.path.exists(data_path):\n",
    "                logger.warning(f\"Файл random_rows.csv отсутствует для {dataset_id}\")\n",
    "                continue\n",
    "                \n",
    "            df = pd.read_csv(data_path)\n",
    "            \n",
    "            if df.empty:\n",
    "                logger.warning(f\"Пустой датасет: {dataset_id}\")\n",
    "                continue\n",
    "            \n",
    "            # Обработка пропущенных значений\n",
    "            for col in df.columns:\n",
    "                if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    median_val = df[col].median()\n",
    "                    if pd.isna(median_val):\n",
    "                        median_val = 0\n",
    "                    df[col].fillna(median_val, inplace=True)\n",
    "                else:\n",
    "                    mode_val = df[col].mode()\n",
    "                    if not mode_val.empty:\n",
    "                        df[col].fillna(mode_val[0], inplace=True)\n",
    "                    else:\n",
    "                        df[col].fillna('missing', inplace=True)\n",
    "            \n",
    "            numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "            categorical_cols = df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "            \n",
    "            groups = [\"general\", \"statistical\", \"info-theory\"]\n",
    "            if numeric_cols:\n",
    "                groups.append(\"model-based\")\n",
    "            \n",
    "            mfe = MFE(\n",
    "                groups=groups,\n",
    "                summary=[\"mean\", \"sd\"]\n",
    "            )\n",
    "            \n",
    "            cat_features = {}\n",
    "            if categorical_cols:\n",
    "                try:\n",
    "                    cat_df = df[categorical_cols].copy()\n",
    "                    \n",
    "                    for col in cat_df.columns:\n",
    "                        le = LabelEncoder()\n",
    "                        cat_df[col].fillna('missing', inplace=True)\n",
    "                        try:\n",
    "                            cat_df[col] = le.fit_transform(cat_df[col].astype(str))\n",
    "                        except:\n",
    "                            unique_vals = cat_df[col].unique()\n",
    "                            mapping = {val: i for i, val in enumerate(unique_vals)}\n",
    "                            cat_df[col] = cat_df[col].map(mapping)\n",
    "                    \n",
    "                    cat_mfe = MFE(groups=[\"general\", \"info-theory\"])\n",
    "                    cat_mfe.fit(cat_df.values)\n",
    "                    cat_features_list = cat_mfe.extract()\n",
    "                    cat_features = dict(zip(cat_features_list[0], cat_features_list[1]))\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Ошибка при обработке категориальных признаков для {dataset_id}: {str(e)}\")\n",
    "            \n",
    "            num_features = {}\n",
    "            if numeric_cols:\n",
    "                try:\n",
    "                    mfe.fit(df[numeric_cols].values)\n",
    "                    num_features_list = mfe.extract()\n",
    "                    num_features = dict(zip(num_features_list[0], num_features_list[1]))\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Ошибка при обработке числовых признаков для {dataset_id}: {str(e)}\")\n",
    "            \n",
    "            features_dict = {**num_features, **cat_features}\n",
    "            features_dict[\"dataset_id\"] = dataset_id\n",
    "            features_dict[\"domain\"] = domain\n",
    "            \n",
    "            features_dict[\"num_numeric_cols\"] = len(numeric_cols)\n",
    "            features_dict[\"num_categorical_cols\"] = len(categorical_cols)\n",
    "            \n",
    "            all_features.append(features_dict)\n",
    "            domains.append(domain)\n",
    "            \n",
    "            del df\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка для {dataset_id}: {str(e)}\")\n",
    "            failed_datasets.append(dataset_id)\n",
    "    \n",
    "    if all_features:\n",
    "        features_df = pd.DataFrame(all_features)\n",
    "        features_df.to_csv(os.path.join(RESULTS_DIR, \"meta_features_with_domains.csv\"), index=False)\n",
    "        logger.info(f\"Мета-признаки сохранены в meta_features_with_domains.csv\")\n",
    "        \n",
    "    logger.info(f\"Успешно обработано: {len(all_features)} датасетов\")\n",
    "    logger.info(f\"Ошибки обработки: {len(failed_datasets)} датасетов\")\n",
    "    \n",
    "    domain_counts = pd.Series(domains).value_counts()\n",
    "    logger.info(\"\\nРаспределение доменов:\")\n",
    "    logger.info(domain_counts.to_string())\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "def train_domain_classifier(features_df):\n",
    "    if features_df is None or features_df.empty:\n",
    "        raise ValueError(\"Нет данных для обучения\")\n",
    "    \n",
    "    X = features_df.drop(columns=[\"dataset_id\", \"domain\"], errors=\"ignore\")\n",
    "    y = features_df[\"domain\"]\n",
    "    \n",
    "    X = preprocess_features(X)\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    selected_columns, scores, _ = anova_feature_selection(X_imputed, y, k=15)  \n",
    "    \n",
    "    if not selected_columns:\n",
    "        logger.warning(\"Нет отобранных мета-признаков! Используем все.\")\n",
    "        selected_columns = X_imputed.columns.tolist()\n",
    "    else:\n",
    "        X = X[selected_columns]\n",
    "        logger.info(f\"Используем отобранные мета-признаки: {selected_columns}\")\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    class_names = le.classes_\n",
    "    \n",
    "    models = {\n",
    "        'RandomForest': make_pipeline(\n",
    "            SimpleImputer(strategy='median'),\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=100, \n",
    "                random_state=42,\n",
    "                class_weight='balanced',\n",
    "                n_jobs=1\n",
    "            )\n",
    "        ),\n",
    "        'LogisticRegression': make_pipeline(\n",
    "            SimpleImputer(strategy='median'),\n",
    "            StandardScaler(),\n",
    "            LogisticRegression(\n",
    "                max_iter=1000,\n",
    "                random_state=42,\n",
    "                class_weight='balanced',\n",
    "                n_jobs=1\n",
    "            )\n",
    "        ),\n",
    "        'SVM': make_pipeline(\n",
    "            SimpleImputer(strategy='median'),\n",
    "            StandardScaler(),\n",
    "            SVC(\n",
    "                kernel='rbf',\n",
    "                probability=True,\n",
    "                random_state=42,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "        ),\n",
    "        'KNN': make_pipeline(\n",
    "            SimpleImputer(strategy='median'),\n",
    "            StandardScaler(),\n",
    "            KNeighborsClassifier(n_neighbors=5)\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    results = {}\n",
    "    \n",
    "    logger.info(\"\\nНачало кросс-валидации (StratifiedKFold)\")\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            logger.info(f\"Оценка модели: {name}\")\n",
    "            \n",
    "            cv_results = cross_validate(\n",
    "                model, X, y_encoded, cv=skf, n_jobs=1,\n",
    "                scoring=['accuracy', 'f1_macro', 'f1_weighted', 'precision_macro', 'recall_macro'],\n",
    "                return_train_score=False\n",
    "            )\n",
    "            \n",
    "            y_pred = cross_val_predict(model, X, y_encoded, cv=skf, n_jobs=1)\n",
    "            \n",
    "            report = classification_report(\n",
    "                y_encoded, y_pred, \n",
    "                target_names=class_names, \n",
    "                output_dict=True,\n",
    "                zero_division=0\n",
    "            )\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'accuracy': cv_results['test_accuracy'].mean(),\n",
    "                'accuracy_std': cv_results['test_accuracy'].std(),\n",
    "                'precision_macro': cv_results['test_precision_macro'].mean(),\n",
    "                'precision_macro_std': cv_results['test_precision_macro'].std(),\n",
    "                'recall_macro': cv_results['test_recall_macro'].mean(),\n",
    "                'recall_macro_std': cv_results['test_recall_macro'].std(),\n",
    "                'f1_macro': cv_results['test_f1_macro'].mean(),\n",
    "                'f1_macro_std': cv_results['test_f1_macro'].std(),\n",
    "                'f1_weighted': cv_results['test_f1_weighted'].mean(),\n",
    "                'f1_weighted_std': cv_results['test_f1_weighted'].std(),\n",
    "                'report': report,\n",
    "                'y_true': y_encoded,\n",
    "                'y_pred': y_pred,\n",
    "                'class_names': class_names,\n",
    "                'cv_results': cv_results\n",
    "            }\n",
    "\n",
    "            logger.info(f\"  Точность: {results[name]['accuracy']:.4f} ± {results[name]['accuracy_std']:.4f}\")\n",
    "            logger.info(f\"  Precision: {results[name]['precision_macro']:.4f} ± {results[name]['precision_macro_std']:.4f}\")\n",
    "            logger.info(f\"  Recall: {results[name]['recall_macro']:.4f} ± {results[name]['recall_macro_std']:.4f}\")\n",
    "            logger.info(f\"  F1 (macro): {results[name]['f1_macro']:.4f} ± {results[name]['f1_macro_std']:.4f}\")\n",
    "            logger.info(f\"  F1 (weighted): {results[name]['f1_weighted']:.4f} ± {results[name]['f1_weighted_std']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка при оценке модели {name}: {str(e)}\")\n",
    "            results[name] = {\n",
    "                'model': None,\n",
    "                'accuracy': 0,\n",
    "                'accuracy_std': 0,\n",
    "                'f1_macro': 0,\n",
    "                'f1_macro_std': 0,\n",
    "                'f1_weighted': 0,\n",
    "                'f1_weighted_std': 0,\n",
    "                'report': {},\n",
    "                'y_true': [],\n",
    "                'y_pred': [],\n",
    "                'class_names': []\n",
    "            }\n",
    "    \n",
    "    successful_models = {k: v for k, v in results.items() if v['model'] is not None}\n",
    "    \n",
    "    if not successful_models:\n",
    "        logger.error(\"Ни одна модель не была успешно обучена!\")\n",
    "        return results, None\n",
    "    \n",
    "    best_model_name = max(successful_models, key=lambda k: successful_models[k]['accuracy'])\n",
    "    best_model = successful_models[best_model_name]\n",
    "    \n",
    "    logger.info(f\"\\nЛучшая модель: {best_model_name} с точностью {best_model['accuracy']:.4f} ± {best_model['accuracy_std']:.4f}\")\n",
    "    \n",
    "    try:\n",
    "        best_model_instance = best_model['model']\n",
    "        best_model_instance.fit(X, y_encoded)\n",
    "        \n",
    "        joblib.dump({\n",
    "            'model': best_model_instance,\n",
    "            'label_encoder': le,\n",
    "            'feature_names': X.columns.tolist(),\n",
    "            'selected_features': selected_columns,\n",
    "            'feature_scores': scores  \n",
    "        }, os.path.join(RESULTS_DIR, MODEL_SAVE_PATH))\n",
    "        \n",
    "        logger.info(f\"Лучшая модель сохранена в {MODEL_SAVE_PATH}\")\n",
    "        \n",
    "        best_model['trained_model'] = best_model_instance\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка при обучении лучшей модели на всех данных: {str(e)}\")\n",
    "        best_model['trained_model'] = None\n",
    "    \n",
    "    save_results(results, features_df, RESULTS_DIR)\n",
    "    \n",
    "    return results, best_model\n",
    "\n",
    "def save_results(results, features_df, output_dir):\n",
    "    metrics = []\n",
    "    for name, res in results.items():\n",
    "        if res['model'] is None:\n",
    "            continue\n",
    "            \n",
    "        metrics.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': res['accuracy'],\n",
    "            'Accuracy Std': res['accuracy_std'],\n",
    "            'Precision (Macro)': res['precision_macro'],\n",
    "            'Precision (Macro) Std': res['precision_macro_std'],\n",
    "            'Recall (Macro)': res['recall_macro'],\n",
    "            'Recall (Macro) Std': res['recall_macro_std'],\n",
    "            'F1 (Macro)': res['f1_macro'],\n",
    "            'F1 (Macro) Std': res['f1_macro_std'],\n",
    "            'F1 (Weighted)': res['f1_weighted'],\n",
    "            'F1 (Weighted) Std': res['f1_weighted_std'],\n",
    "        })\n",
    "    \n",
    "    if not metrics:\n",
    "        logger.warning(\"Нет метрик для сохранения\")\n",
    "        return None\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    metrics_df.to_csv(os.path.join(output_dir, \"model_metrics.csv\"), index=False)\n",
    "    \n",
    "    formatted_metrics = []\n",
    "    for _, row in metrics_df.iterrows():\n",
    "        formatted_metrics.append({\n",
    "            'Model': row['Model'],\n",
    "            'Accuracy': f\"{row['Accuracy']:.2f} ± {row['Accuracy Std']:.2f}\",\n",
    "            'Precision': f\"{row['Precision (Macro)']:.2f} ± {row['Precision (Macro) Std']:.2f}\",\n",
    "            'Recall': f\"{row['Recall (Macro)']:.2f} ± {row['Recall (Macro) Std']:.2f}\",\n",
    "            'F1 (Macro)': f\"{row['F1 (Macro)']:.2f} ± {row['F1 (Macro) Std']:.2f}\",\n",
    "            'F1 (Weighted)': f\"{row['F1 (Weighted)']:.2f} ± {row['F1 (Weighted) Std']:.2f}\"\n",
    "        })\n",
    "    \n",
    "    formatted_df = pd.DataFrame(formatted_metrics)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Сравнение моделей классификации доменов\")\n",
    "    print(\"=\"*80)\n",
    "    print(formatted_df.to_string(index=False))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    x = np.arange(len(metrics_df))\n",
    "    width = 0.15\n",
    "    \n",
    "    metrics_to_plot = [\n",
    "        ('Accuracy', 'skyblue'),\n",
    "        ('Precision (Macro)', 'lightcoral'),\n",
    "        ('Recall (Macro)', 'gold'),\n",
    "        ('F1 (Macro)', 'lightgreen'),\n",
    "        ('F1 (Weighted)', 'salmon')\n",
    "    ]\n",
    "    \n",
    "    for i, (metric, color) in enumerate(metrics_to_plot):\n",
    "        offset = width * (i - len(metrics_to_plot) / 2)\n",
    "        plt.bar(\n",
    "            x + offset, \n",
    "            metrics_df[metric], \n",
    "            width, \n",
    "            yerr=metrics_df[metric + ' Std'],\n",
    "            label=metric, \n",
    "            alpha=0.8, \n",
    "            capsize=5, \n",
    "            color=color\n",
    "        )\n",
    "    \n",
    "    plt.title('Сравнение моделей классификации доменов', fontsize=16)\n",
    "    plt.ylabel('Score', fontsize=14)\n",
    "    plt.xticks(x, metrics_df['Model'], rotation=45, ha='right', fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.legend(fontsize=12, loc='lower right')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(os.path.join(output_dir, \"model_comparison.png\"), dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    best_model_name = None\n",
    "    best_accuracy = 0\n",
    "    for name, res in results.items():\n",
    "        if res['model'] is not None and res['accuracy'] > best_accuracy:\n",
    "            best_model_name = name\n",
    "            best_accuracy = res['accuracy']\n",
    "    \n",
    "    if best_model_name is None:\n",
    "        logger.warning(\"Нет успешных моделей для отчета\")\n",
    "        return metrics_df\n",
    "    \n",
    "    best_result = results[best_model_name]\n",
    "    y_true = best_result['y_true']\n",
    "    y_pred = best_result['y_pred']\n",
    "    class_names = best_result['class_names']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Отчет классификации для лучшей модели ({best_model_name}):\")\n",
    "    print(\"=\"*80)\n",
    "    print(classification_report(\n",
    "        y_true, y_pred, \n",
    "        target_names=class_names,\n",
    "        zero_division=0\n",
    "    ))\n",
    "    \n",
    "    predictions = []\n",
    "    for i, row in features_df.iterrows():\n",
    "        predictions.append({\n",
    "            'dataset_id': row['dataset_id'],\n",
    "            'domain': row['domain'],\n",
    "            'predicted_domain': class_names[results[best_model_name]['y_pred'][i]]\n",
    "        })\n",
    "    \n",
    "    predictions_df = pd.DataFrame(predictions)\n",
    "    predictions_df.to_csv(os.path.join(output_dir, \"domain_predictions.csv\"), index=False)\n",
    "    \n",
    "    errors = predictions_df[predictions_df['domain'] != predictions_df['predicted_domain']]\n",
    "    if not errors.empty:\n",
    "        errors.to_csv(os.path.join(output_dir, \"classification_errors.csv\"), index=False)\n",
    "        logger.info(f\"Найдено {len(errors)} ошибок классификации. Сохранены в classification_errors.csv\")\n",
    "    \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Топ-5 ошибок классификации:\")\n",
    "        print(\"=\"*80)\n",
    "        print(errors.head(5).to_string(index=False))\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "def main():\n",
    "    features_df = extract_meta_features()\n",
    "    \n",
    "    if features_df is not None and not features_df.empty:\n",
    "        results, best_model = train_domain_classifier(features_df)\n",
    "        \n",
    "        if best_model is None or best_model.get('trained_model') is None:\n",
    "            logger.error(\"Не удалось обучить ни одну модель!\")\n",
    "            return\n",
    "        \n",
    "        best_model_name = max(results, key=lambda k: results[k].get('accuracy', 0))\n",
    "        if results[best_model_name].get('y_true') is not None and results[best_model_name].get('y_pred') is not None:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"Финальный отчет классификации:\")\n",
    "            print(\"=\"*80)\n",
    "            print(classification_report(\n",
    "                results[best_model_name]['y_true'],\n",
    "                results[best_model_name]['y_pred'],\n",
    "                target_names=results[best_model_name].get('class_names', [])\n",
    "            ))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979bbef-dcb0-475b-888c-386c881efc43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Experiment 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519ecff-eddf-4fd7-bdc8-79544313cc53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## gpt-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d3a73-dd4f-411e-bf1d-b3b0ffaa452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"all_datasets\"\n",
    "RESULTS_DIR = \"classification_results\"\n",
    "DOMAIN_MAPPING_FILE = \"clustering_domain_mapping.json\"\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")  \n",
    "TIMEOUT = 180\n",
    "N_RUNS = 5\n",
    "K_FOLDS = 5\n",
    "FEW_SHOT_SIZE = 5\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('llm_classification.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ClassificationResult(BaseModel):\n",
    "    primary_domain: str = Field(..., description=\"The most appropriate domain for the dataset\")\n",
    "    alternative_domains: List[str] = Field(\n",
    "        default_factory=list, \n",
    "        description=\"List of alternative relevant domains (up to 2)\",\n",
    "        max_items=2\n",
    "    )\n",
    "\n",
    "def detect_encoding(file_path: str) -> str:\n",
    "    \"\"\"Detect file encoding\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            rawdata = f.read(50000)\n",
    "        result = chardet.detect(rawdata)\n",
    "        return result['encoding'] if result['confidence'] > 0.7 else 'utf-8'\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Encoding detection error: {e}, using utf-8\")\n",
    "        return 'utf-8'\n",
    "\n",
    "def read_csv_with_memory_limit(file_path: str, dataset_id: str = None) -> pd.DataFrame:\n",
    "    \"\"\"Read CSV random_rows.csv and automatic type detection\"\"\"\n",
    "    try:\n",
    "        if dataset_id:\n",
    "            random_path = os.path.join(OUTPUT_DIR, dataset_id, \"random_rows.csv\")\n",
    "            if os.path.exists(random_path):\n",
    "                encoding = detect_encoding(random_path)\n",
    "                df = pd.read_csv(random_path, encoding=encoding)\n",
    "                logger.debug(f\"Read random rows for {dataset_id} ({len(df)} rows, {len(df.columns)} columns)\")\n",
    "                return df\n",
    "\n",
    "        encoding = detect_encoding(file_path)\n",
    "        logger.debug(f\"Detected encoding: {encoding} for {os.path.basename(file_path)}\")\n",
    "        \n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            preview = pd.read_csv(f, nrows=1000)\n",
    "        \n",
    "        dtype_dict = preview.dtypes.to_dict()\n",
    "        \n",
    "        chunks = []\n",
    "        chunk_size = 10000\n",
    "        \n",
    "        for chunk in pd.read_csv(\n",
    "            file_path, \n",
    "            encoding=encoding,\n",
    "            chunksize=chunk_size,\n",
    "            dtype=dtype_dict\n",
    "        ):\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        if chunks:\n",
    "            df = pd.concat(chunks, ignore_index=True)\n",
    "            logger.debug(f\"Read dataset with {len(df)} rows and {len(df.columns)} columns\")\n",
    "            return df\n",
    "        else:\n",
    "            logger.warning(\"No data read, returning empty DataFrame\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading CSV: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def prepare_for_llm(df: pd.DataFrame, max_rows: int = 5) -> str:\n",
    "    \"\"\"Prepare DataFrame-like representation for LLM\"\"\"\n",
    "    df_subset = df.head(max_rows).copy()\n",
    "    df_subset = df_subset.fillna('missing')\n",
    "    \n",
    "    columns = df_subset.columns.tolist()\n",
    "    data = []\n",
    "    \n",
    "    for _, row in df_subset.iterrows():\n",
    "        formatted_row = []\n",
    "        for value in row:\n",
    "            if isinstance(value, str):\n",
    "                val = value.replace('\"', '\\\\\"')\n",
    "                formatted_value = f'\"{val}\"'\n",
    "            else:\n",
    "                formatted_value = str(value)\n",
    "            formatted_row.append(formatted_value)\n",
    "        data.append(f\"[{', '.join(formatted_row)}]\")\n",
    "    \n",
    "    columns_str = \"[\" + \", \".join([f'\"{col}\"' for col in columns]) + \"]\"\n",
    "    data_str = \",\\n\".join(data)\n",
    "    \n",
    "    return f\"columns = {columns_str}\\ndata = [\\n{data_str}\\n]\"\n",
    "\n",
    "class GPT4oClassifier:\n",
    "    def __init__(self):\n",
    "        self.domains = self.load_domains()\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=OPENROUTER_API_KEY,\n",
    "            default_headers={\n",
    "                \"HTTP-Referer\": \"https://your-site.com\",  \n",
    "                \"X-Title\": \"Dataset Classifier\"  \n",
    "            }\n",
    "        )\n",
    "        self.parser = PydanticOutputParser(pydantic_object=ClassificationResult)\n",
    "        logger.info(f\"Initialized with {len(self.domains)} domains\")\n",
    "\n",
    "    def load_domains(self) -> List[str]:\n",
    "        if not os.path.exists(DOMAIN_MAPPING_FILE):\n",
    "            logger.error(f\"Domain file not found: {os.path.abspath(DOMAIN_MAPPING_FILE)}\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            with open(DOMAIN_MAPPING_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                domain_data = json.load(f)\n",
    "                \n",
    "                if isinstance(domain_data, dict):\n",
    "                    domains = list(domain_data.keys())\n",
    "                    logger.info(f\"Loaded {len(domains)} domains\")\n",
    "                    return domains\n",
    "                else:\n",
    "                    logger.error(f\"Unknown domain file format: {type(domain_data)}\")\n",
    "                    return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading domains: {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def create_examples_text(self, examples: List[Tuple[str, dict]]) -> str:\n",
    "        \"\"\"Create text with examples for few-shot classification\"\"\"\n",
    "        examples_text = \"\"\n",
    "        for i, (data_sample, classification) in enumerate(examples, 1):\n",
    "            examples_text += (\n",
    "                f\"\\nExample {i}:\\nDataset sample:\\n{data_sample}\\n\"\n",
    "                f\"Classification: {json.dumps(classification)}\\n\"\n",
    "            )\n",
    "        return examples_text\n",
    "\n",
    "    @retry(stop=stop_after_attempt(5), wait=wait_fixed(20))\n",
    "    def classify(self, df: pd.DataFrame, mode: str = \"zero-shot\", examples: List[Tuple[str, dict]] = None) -> ClassificationResult:\n",
    "        if df.empty or not self.domains:\n",
    "            logger.warning(\"Empty DataFrame or no domains for classification\")\n",
    "            return ClassificationResult(primary_domain=\"unknown\", alternative_domains=[])\n",
    "        \n",
    "        try:\n",
    "            data_sample = prepare_for_llm(df)\n",
    "            format_instructions = self.parser.get_format_instructions()\n",
    "            \n",
    "            if mode == \"zero-shot\":\n",
    "                system_content = (\n",
    "                    \"You are a dataset domain classification expert. \"\n",
    "                    \"Analyze the dataset sample provided as a DataFrame-like structure \"\n",
    "                    \"and determine its primary domain with optional alternatives. \"\n",
    "                    \"Choose ONLY from the following domains: \" + \", \".join(self.domains) + \"\\n\\n\" +\n",
    "                    format_instructions + \"\\n\\n\" +\n",
    "                    \"Rules:\\n\"\n",
    "                    \"1. primary_domain: SINGLE most appropriate domain (MUST be from list)\\n\"\n",
    "                    \"2. alternative_domains: 0-2 other relevant domains (only if applicable)\\n\"\n",
    "                    \"3. Use EXACT domain names\\n\"\n",
    "                    \"4. No additional text/comments!\"\n",
    "                )\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_content},\n",
    "                    {\"role\": \"user\", \"content\": f\"Dataset sample:\\n{data_sample}\"}\n",
    "                ]\n",
    "            \n",
    "            elif mode in [\"one-shot\", \"few-shot\"] and examples:\n",
    "                examples_text = self.create_examples_text(examples)\n",
    "                system_content = (\n",
    "                    \"You are a dataset domain classification expert. \"\n",
    "                    \"Analyze dataset samples and determine their primary domain with optional alternatives. \"\n",
    "                    \"Choose ONLY from these domains: \" + \", \".join(self.domains) + \"\\n\\n\" +\n",
    "                    format_instructions + \"\\n\\n\" +\n",
    "                    \"Rules:\\n\"\n",
    "                    \"1. primary_domain: SINGLE most appropriate domain (MUST be from list)\\n\"\n",
    "                    \"2. alternative_domains: 0-2 other relevant domains (only if applicable)\\n\"\n",
    "                    \"3. Use EXACT domain names\\n\"\n",
    "                    \"4. No additional text/comments!\\n\\n\" +\n",
    "                    \"Examples:\" + examples_text\n",
    "                )\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_content},\n",
    "                    {\"role\": \"user\", \"content\": f\"Classify this new dataset:\\n{data_sample}\"}\n",
    "                ]\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mode or missing examples: {mode}\")\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"openai/gpt-4o\",\n",
    "                messages=messages,\n",
    "                temperature=0.3,\n",
    "                max_tokens=500,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            content = response.choices[0].message.content\n",
    "            try:\n",
    "                result = self.parser.parse(content)\n",
    "                logger.debug(f\"Classification result ({mode}): {result}\")\n",
    "                return result\n",
    "            except ValidationError as e:\n",
    "                logger.error(f\"Validation error: {e}\\nResponse content: {content}\")\n",
    "                return ClassificationResult(primary_domain=\"unknown\", alternative_domains=[])\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification error ({mode}): {e}\", exc_info=True)\n",
    "            return ClassificationResult(primary_domain=\"unknown\", alternative_domains=[])\n",
    "\n",
    "def load_all_datasets() -> Dict[str, Dict]:\n",
    "    datasets = {}\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        logger.error(f\"Dataset directory not found: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "        return datasets\n",
    "\n",
    "    dataset_ids = [d for d in os.listdir(OUTPUT_DIR) \n",
    "                   if os.path.isdir(os.path.join(OUTPUT_DIR, d))]\n",
    "    \n",
    "    valid_datasets = [\n",
    "        d for d in dataset_ids\n",
    "        if os.path.exists(os.path.join(OUTPUT_DIR, d, \"metadata.json\"))\n",
    "        and os.path.exists(os.path.join(OUTPUT_DIR, d, \"random_rows.csv\"))\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Loading {len(valid_datasets)} valid datasets...\")\n",
    "    \n",
    "    for dataset_id in tqdm(valid_datasets, desc=\"Loading datasets\"):\n",
    "        try:\n",
    "            meta_path = os.path.join(OUTPUT_DIR, dataset_id, \"metadata.json\")\n",
    "            data_path = os.path.join(OUTPUT_DIR, dataset_id, \"random_rows.csv\")\n",
    "\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            if \"domain\" not in metadata:\n",
    "                logger.warning(f\"Skipping {dataset_id}: 'domain' field missing in metadata\")\n",
    "                continue\n",
    "\n",
    "            df = read_csv_with_memory_limit(data_path, dataset_id=dataset_id)\n",
    "            if df.empty:\n",
    "                logger.warning(f\"Empty DataFrame for {dataset_id}, skipping\")\n",
    "                continue\n",
    "\n",
    "            datasets[dataset_id] = {\n",
    "                'metadata': metadata,\n",
    "                'dataframe': df,\n",
    "                'domain': metadata[\"domain\"].lower(),\n",
    "                'data_sample': prepare_for_llm(df)\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {dataset_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Successfully loaded {len(datasets)} datasets\")\n",
    "    return datasets\n",
    "\n",
    "def run_zero_shot_classification(datasets: Dict[str, Dict], run_id: int) -> Tuple[Optional[float], List[str], List[str], Optional[Dict], Optional[float]]:\n",
    "    \"\"\"Run zero-shot classification with random sampling\"\"\"\n",
    "    logger.info(f\"=== Starting zero-shot classification run {run_id} ===\")\n",
    "    classifier = GPT4oClassifier()\n",
    "\n",
    "    if not classifier.domains or not datasets:\n",
    "        logger.error(\"Classification domains or datasets not loaded\")\n",
    "        return None, [], [], {}, None\n",
    "\n",
    "    test_size = len(datasets) // K_FOLDS\n",
    "    dataset_items = list(datasets.items())\n",
    "    random.shuffle(dataset_items)\n",
    "    test_datasets = dict(dataset_items[:test_size])\n",
    "    \n",
    "    matches = 0\n",
    "    top3_matches = 0\n",
    "    total = 0\n",
    "    actual_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    pbar = tqdm(test_datasets.items(), desc=f\"Zero-shot classification (Run {run_id})\", unit=\"dataset\")\n",
    "\n",
    "    for dataset_id, data_info in pbar:\n",
    "        try:\n",
    "            result = classifier.classify(data_info['dataframe'], mode=\"zero-shot\")\n",
    "            actual = data_info['domain']\n",
    "            predicted_primary = result.primary_domain.lower()\n",
    "            \n",
    "            all_predictions = [predicted_primary] + [alt.lower() for alt in result.alternative_domains]\n",
    "            top3 = all_predictions[:3]\n",
    "\n",
    "            if predicted_primary == actual:\n",
    "                matches += 1\n",
    "            if actual in top3:\n",
    "                top3_matches += 1\n",
    "                \n",
    "            total += 1\n",
    "            actual_labels.append(actual)\n",
    "            predicted_labels.append(predicted_primary)\n",
    "                \n",
    "            pbar.set_postfix({\n",
    "                \"Accuracy\": f\"{matches/total:.2%}\" if total > 0 else \"N/A\",\n",
    "                \"Top-3 Recall\": f\"{top3_matches/total:.2%}\" if total > 0 else \"N/A\"\n",
    "            })\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if total > 0:\n",
    "        accuracy = matches / total\n",
    "        top3_recall = top3_matches / total\n",
    "        logger.info(f\"Zero-shot accuracy: {accuracy:.2%}\")\n",
    "        logger.info(f\"Zero-shot top-3 recall: {top3_recall:.2%}\")\n",
    "        \n",
    "        report = classification_report(actual_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "        return accuracy, actual_labels, predicted_labels, report, top3_recall\n",
    "    else:\n",
    "        logger.warning(\"No datasets processed for accuracy calculation\")\n",
    "        return None, [], [], {}, None\n",
    "\n",
    "def run_kfold_classification(datasets: Dict[str, Dict], mode: str, run_id: int) -> Tuple[List[float], List[float], List[Dict]]:\n",
    "    \"\"\"Run K-fold validation for one-shot or few-shot classification\"\"\"\n",
    "    logger.info(f\"=== Starting {mode} classification with K-fold validation (Run {run_id}) ===\")\n",
    "    classifier = GPT4oClassifier()\n",
    "\n",
    "    if not classifier.domains or not datasets:\n",
    "        logger.error(\"Classification domains or datasets not loaded\")\n",
    "        return [], [], []\n",
    "\n",
    "    dataset_items = list(datasets.items())\n",
    "    kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42 + run_id)\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    fold_top3_recalls = []\n",
    "    fold_reports = []\n",
    "\n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(kf.split(dataset_items)):\n",
    "        logger.info(f\"Processing fold {fold_idx + 1}/{K_FOLDS}\")\n",
    "        \n",
    "        train_datasets = [dataset_items[i] for i in train_indices]\n",
    "        test_datasets = [dataset_items[i] for i in test_indices]\n",
    "        \n",
    "        examples = []\n",
    "        if mode == \"one-shot\":\n",
    "            domain_examples = {}\n",
    "            for dataset_id, data_info in train_datasets:\n",
    "                domain = data_info['domain']\n",
    "                if domain not in domain_examples:\n",
    "                    classification = {\n",
    "                        \"primary_domain\": domain,\n",
    "                        \"alternative_domains\": []\n",
    "                    }\n",
    "                    domain_examples[domain] = (data_info['data_sample'], classification)\n",
    "            examples = list(domain_examples.values())\n",
    "        \n",
    "        elif mode == \"few-shot\":\n",
    "            domain_examples = {}\n",
    "            for dataset_id, data_info in train_datasets:\n",
    "                domain = data_info['domain']\n",
    "                if domain not in domain_examples:\n",
    "                    domain_examples[domain] = []\n",
    "                \n",
    "                if len(domain_examples[domain]) < FEW_SHOT_SIZE:\n",
    "                    classification = {\n",
    "                        \"primary_domain\": domain,\n",
    "                        \"alternative_domains\": []\n",
    "                    }\n",
    "                    domain_examples[domain].append((data_info['data_sample'], classification))\n",
    "            \n",
    "            for domain_list in domain_examples.values():\n",
    "                examples.extend(domain_list)\n",
    "        \n",
    "        matches = 0\n",
    "        top3_matches = 0\n",
    "        total = 0\n",
    "        actual_labels = []\n",
    "        predicted_labels = []\n",
    "        \n",
    "        pbar = tqdm(test_datasets, desc=f\"{mode.capitalize()} fold {fold_idx + 1}\", unit=\"dataset\")\n",
    "        \n",
    "        for dataset_id, data_info in pbar:\n",
    "            try:\n",
    "                result = classifier.classify(\n",
    "                    data_info['dataframe'], \n",
    "                    mode=mode, \n",
    "                    examples=examples\n",
    "                )\n",
    "                actual = data_info['domain']\n",
    "                predicted_primary = result.primary_domain.lower()\n",
    "                \n",
    "                all_predictions = [predicted_primary] + [alt.lower() for alt in result.alternative_domains]\n",
    "                top3 = all_predictions[:3]\n",
    "\n",
    "                if predicted_primary == actual:\n",
    "                    matches += 1\n",
    "                if actual in top3:\n",
    "                    top3_matches += 1\n",
    "                    \n",
    "                total += 1\n",
    "                actual_labels.append(actual)\n",
    "                predicted_labels.append(predicted_primary)\n",
    "                    \n",
    "                pbar.set_postfix({\n",
    "                    \"Accuracy\": f\"{matches/total:.2%}\" if total > 0 else \"N/A\",\n",
    "                    \"Top-3 Recall\": f\"{top3_matches/total:.2%}\" if total > 0 else \"N/A\"\n",
    "                })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if total > 0:\n",
    "            fold_accuracy = matches / total\n",
    "            fold_top3_recall = top3_matches / total\n",
    "            fold_report = classification_report(actual_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "            \n",
    "            fold_accuracies.append(fold_accuracy)\n",
    "            fold_top3_recalls.append(fold_top3_recall)\n",
    "            fold_reports.append(fold_report)\n",
    "            \n",
    "            logger.info(f\"Fold {fold_idx + 1} - Accuracy: {fold_accuracy:.2%}, Top-3 Recall: {fold_top3_recall:.2%}\")\n",
    "\n",
    "    return fold_accuracies, fold_top3_recalls, fold_reports\n",
    "\n",
    "def calculate_aggregated_metrics(fold_reports: List[Dict]) -> Dict[str, float]:\n",
    "    if not fold_reports:\n",
    "        return {}\n",
    "    \n",
    "    macro_precisions = [report['macro avg']['precision'] for report in fold_reports]\n",
    "    macro_recalls = [report['macro avg']['recall'] for report in fold_reports]\n",
    "    macro_f1s = [report['macro avg']['f1-score'] for report in fold_reports]\n",
    "    weighted_f1s = [report['weighted avg']['f1-score'] for report in fold_reports]\n",
    "    \n",
    "    return {\n",
    "        'macro_precision_mean': np.mean(macro_precisions),\n",
    "        'macro_precision_std': np.std(macro_precisions),\n",
    "        'macro_recall_mean': np.mean(macro_recalls),\n",
    "        'macro_recall_std': np.std(macro_recalls),\n",
    "        'macro_f1_mean': np.mean(macro_f1s),\n",
    "        'macro_f1_std': np.std(macro_f1s),\n",
    "        'weighted_f1_mean': np.mean(weighted_f1s),\n",
    "        'weighted_f1_std': np.std(weighted_f1s)\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    logger.info(\"Loading all datasets...\")\n",
    "    datasets = load_all_datasets()\n",
    "    \n",
    "    if not datasets:\n",
    "        logger.error(\"No datasets loaded, exiting\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Loaded {len(datasets)} datasets\")\n",
    "    \n",
    "    results = {\n",
    "        'zero-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        },\n",
    "        'one-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        },\n",
    "        'few-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"STARTING CLASSIFICATION EXPERIMENT\")\n",
    "    print(f\"Modes: Zero-shot, One-shot, Few-shot\")\n",
    "    print(f\"Runs per mode: {N_RUNS}\")\n",
    "    print(f\"K-Folds for shot learning: {K_FOLDS}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ZERO-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        accuracy, actual_labels, predicted_labels, report, top3_recall = run_zero_shot_classification(datasets, run_id)\n",
    "        \n",
    "        if accuracy is not None:\n",
    "            results['zero-shot']['accuracies'].append(accuracy)\n",
    "            results['zero-shot']['top3_recalls'].append(top3_recall)\n",
    "            results['zero-shot']['macro_precisions'].append(report['macro avg']['precision'])\n",
    "            results['zero-shot']['macro_recalls'].append(report['macro avg']['recall'])\n",
    "            results['zero-shot']['macro_f1s'].append(report['macro avg']['f1-score'])\n",
    "            results['zero-shot']['weighted_f1s'].append(report['weighted avg']['f1-score'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={accuracy:.4f}, Top-3 Recall={top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ONE-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"one-shot\", run_id)\n",
    "        \n",
    "        if fold_accuracies:\n",
    "            run_accuracy = np.mean(fold_accuracies)\n",
    "            run_top3_recall = np.mean(fold_top3_recalls)\n",
    "            run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "            results['one-shot']['accuracies'].append(run_accuracy)\n",
    "            results['one-shot']['top3_recalls'].append(run_top3_recall)\n",
    "            results['one-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "            results['one-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "            results['one-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "            results['one-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"FEW-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"few-shot\", run_id)\n",
    "        \n",
    "        if fold_accuracies:\n",
    "            run_accuracy = np.mean(fold_accuracies)\n",
    "            run_top3_recall = np.mean(fold_top3_recalls)\n",
    "            run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "            results['few-shot']['accuracies'].append(run_accuracy)\n",
    "            results['few-shot']['top3_recalls'].append(run_top3_recall)\n",
    "            results['few-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "            results['few-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "            results['few-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "            results['few-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"FINAL AGGREGATED RESULTS\".center(80))\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    final_results = {}\n",
    "    \n",
    "    for mode in ['zero-shot', 'one-shot', 'few-shot']:\n",
    "        mode_results = results[mode]\n",
    "        \n",
    "        if mode_results['accuracies']:\n",
    "            final_results[mode] = {\n",
    "                'accuracy': {\n",
    "                    'mean': np.mean(mode_results['accuracies']),\n",
    "                    'std': np.std(mode_results['accuracies'])\n",
    "                },\n",
    "                'top3_recall': {\n",
    "                    'mean': np.mean(mode_results['top3_recalls']),\n",
    "                    'std': np.std(mode_results['top3_recalls'])\n",
    "                },\n",
    "                'macro_precision': {\n",
    "                    'mean': np.mean(mode_results['macro_precisions']),\n",
    "                    'std': np.std(mode_results['macro_precisions'])\n",
    "                },\n",
    "                'macro_recall': {\n",
    "                    'mean': np.mean(mode_results['macro_recalls']),\n",
    "                    'std': np.std(mode_results['macro_recalls'])\n",
    "                },\n",
    "                'macro_f1': {\n",
    "                    'mean': np.mean(mode_results['macro_f1s']),\n",
    "                    'std': np.std(mode_results['macro_f1s'])\n",
    "                },\n",
    "                'weighted_f1': {\n",
    "                    'mean': np.mean(mode_results['weighted_f1s']),\n",
    "                    'std': np.std(mode_results['weighted_f1s'])\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{mode.upper().replace('-', ' ')} RESULTS:\")\n",
    "            print(f\"Accuracy: {final_results[mode]['accuracy']['mean']:.4f} ± {final_results[mode]['accuracy']['std']:.4f}\")\n",
    "            print(f\"Top-3 Recall: {final_results[mode]['top3_recall']['mean']:.4f} ± {final_results[mode]['top3_recall']['std']:.4f}\")\n",
    "            print(f\"Macro Precision: {final_results[mode]['macro_precision']['mean']:.4f} ± {final_results[mode]['macro_precision']['std']:.4f}\")\n",
    "            print(f\"Macro Recall: {final_results[mode]['macro_recall']['mean']:.4f} ± {final_results[mode]['macro_recall']['std']:.4f}\")\n",
    "            print(f\"Macro F1: {final_results[mode]['macro_f1']['mean']:.4f} ± {final_results[mode]['macro_f1']['std']:.4f}\")\n",
    "            print(f\"Weighted F1: {final_results[mode]['weighted_f1']['mean']:.4f} ± {final_results[mode]['weighted_f1']['std']:.4f}\")\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, \"final_all_modes_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Results saved to final_all_modes_results.json\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2594050-ac37-40be-9a96-0b4c3e3aae13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gemma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c61ad4-79c7-4745-b417-b6dd4afe5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"all_datasets\"\n",
    "RESULTS_DIR = \"classification_results\"\n",
    "DOMAIN_MAPPING_FILE = \"clustering_domain_mapping.json\"\n",
    "LLM_API_URL = \"api_url\"\n",
    "TIMEOUT = 600\n",
    "N_RUNS = 5  \n",
    "K_FOLDS = 5  \n",
    "FEW_SHOT_SIZE = 5 \n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('llm_classification.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def detect_encoding(file_path: str) -> str:\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            rawdata = f.read(50000)\n",
    "        result = chardet.detect(rawdata)\n",
    "        return result['encoding'] if result['confidence'] > 0.7 else 'utf-8'\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Encoding detection error: {e}, using utf-8\")\n",
    "        return 'utf-8'\n",
    "\n",
    "def read_csv_with_memory_limit(file_path: str, dataset_id: str = None) -> pd.DataFrame:\n",
    "    try:\n",
    "        if dataset_id:\n",
    "            random_path = os.path.join(OUTPUT_DIR, dataset_id, \"random_rows.csv\")\n",
    "            if os.path.exists(random_path):\n",
    "                encoding = detect_encoding(random_path)\n",
    "                df = pd.read_csv(random_path, encoding=encoding)  # Убрали dtype='object'\n",
    "                logger.debug(f\"Read random rows for {dataset_id} ({len(df)} rows, {len(df.columns)} columns)\")\n",
    "                return df\n",
    "\n",
    "        encoding = detect_encoding(file_path)\n",
    "        logger.debug(f\"Detected encoding: {encoding} for {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Определение типов данных по первой порции\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            preview = pd.read_csv(f, nrows=1000)\n",
    "        \n",
    "        dtype_dict = preview.dtypes.to_dict()\n",
    "        \n",
    "        chunks = []\n",
    "        chunk_size = 10000\n",
    "        \n",
    "        for chunk in pd.read_csv(\n",
    "            file_path, \n",
    "            encoding=encoding,\n",
    "            chunksize=chunk_size,\n",
    "            dtype=dtype_dict  # Используем определенные типы\n",
    "        ):\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        if chunks:\n",
    "            df = pd.concat(chunks, ignore_index=True)\n",
    "            logger.debug(f\"Read dataset with {len(df)} rows and {len(df.columns)} columns\")\n",
    "            return df\n",
    "        else:\n",
    "            logger.warning(\"No data read, returning empty DataFrame\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading CSV: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def prepare_for_llm(df: pd.DataFrame, max_rows: int = 5) -> str:\n",
    "    try:\n",
    "        df_subset = df.head(max_rows).copy()\n",
    "        df_subset = df_subset.fillna('missing')\n",
    "        \n",
    "        columns = df_subset.columns.tolist()\n",
    "        data = []\n",
    "        \n",
    "        for _, row in df_subset.iterrows():\n",
    "            formatted_row = []\n",
    "            for value in row:\n",
    "                if isinstance(value, str):\n",
    "                    val = value.replace('\"', '\\\\\"')\n",
    "                    formatted_value = f'\"{val}\"'\n",
    "                else:\n",
    "                    formatted_value = str(value)\n",
    "                formatted_row.append(formatted_value)\n",
    "            data.append(f\"[{', '.join(formatted_row)}]\")\n",
    "        \n",
    "        columns_str = \"[\" + \", \".join([f'\"{col}\"' for col in columns]) + \"]\"\n",
    "        data_str = \",\\n\".join(data)\n",
    "        \n",
    "        return f\"columns = {columns_str}\\ndata = [\\n{data_str}\\n]\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка при подготовке DataFrame: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "class ClassificationResult(BaseModel):\n",
    "    primary_domain: str = Field(..., description=\"The most appropriate domain for the dataset\")\n",
    "    alternative_domains: List[str] = Field(\n",
    "        default_factory=list, \n",
    "        description=\"List of alternative relevant domains (up to 2)\",\n",
    "        max_items=2\n",
    "    )\n",
    "\n",
    "class FastAPIChatModel(BaseChatModel):\n",
    "    endpoint_url: str\n",
    "    timeout: int = TIMEOUT\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom-fastapi-chat-model\"\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: List[str] | None = None,\n",
    "        run_manager: CallbackManagerForLLMRun | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        api_messages = [\n",
    "            {\"role\": \"system\" if isinstance(m, SystemMessage) else \"user\", \"content\": m.content}\n",
    "            for m in messages\n",
    "        ]\n",
    "        payload = {\n",
    "            \"job_id\": \"domain_classification\",\n",
    "            \"priority\": None,\n",
    "            \"source\": \"local\",\n",
    "            \"meta\": {\"temperature\": 0.3, \"tokens_limit\": 2048, \"stop_words\": [], \"model\": None},\n",
    "            \"messages\": api_messages\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.endpoint_url, \n",
    "                headers=headers, \n",
    "                json=payload, \n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            content = response.json().get(\"content\", \"{}\")\n",
    "            if not isinstance(content, str):\n",
    "                logger.warning(f\"Содержимое ответа не является строкой: {type(content)}\")\n",
    "                content = str(content)\n",
    "            \n",
    "            if \"```json\" in content and \"```\" in content:\n",
    "                try:\n",
    "                    content = content.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "                except IndexError:\n",
    "                    logger.warning(\"Не удалось разобрать JSON-блок из содержимого\")\n",
    "                    content = \"{}\"\n",
    "            elif \"{\" in content and \"}\" in content:\n",
    "                try:\n",
    "                    content = content[content.index(\"{\"):content.rindex(\"}\") + 1]\n",
    "                except ValueError:\n",
    "                    logger.warning(\"Не удалось извлечь JSON-объект из содержимого\")\n",
    "                    content = \"{}\"\n",
    "            else:\n",
    "                logger.warning(\"В содержимом не найден валидный JSON\")\n",
    "                content = \"{}\"\n",
    "            \n",
    "            return ChatResult(generations=[ChatGeneration(message=HumanMessage(content=content))])\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка API: {str(e)}\")\n",
    "            return ChatResult(generations=[ChatGeneration(message=HumanMessage(content=\"{}\"))])\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        return {\"endpoint_url\": self.endpoint_url, \"timeout\": self.timeout}\n",
    "\n",
    "class LLMClassifier:\n",
    "    def __init__(self):\n",
    "        self.domains = self.load_domains()\n",
    "        self.llm = FastAPIChatModel(endpoint_url=LLM_API_URL, timeout=TIMEOUT)\n",
    "        self.parser = PydanticOutputParser(pydantic_object=ClassificationResult)\n",
    "        logger.info(f\"Initialized with {len(self.domains)} domains\")\n",
    "\n",
    "    def load_domains(self) -> List[str]:\n",
    "        if not os.path.exists(DOMAIN_MAPPING_FILE):\n",
    "            logger.error(f\"Domain file not found: {os.path.abspath(DOMAIN_MAPPING_FILE)}\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            with open(DOMAIN_MAPPING_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                domain_data = json.load(f)\n",
    "                \n",
    "            if isinstance(domain_data, dict):\n",
    "                domains = list(domain_data.keys())\n",
    "                logger.info(f\"Loaded {len(domains)} domains\")\n",
    "                return domains\n",
    "            else:\n",
    "                logger.error(f\"Unknown domain file format: {type(domain_data)}\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading domains: {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def create_examples_text(self, examples: List[Tuple[str, str]]) -> str:\n",
    "        examples_text = \"\"\n",
    "        for i, (data_sample, domain) in enumerate(examples, 1):\n",
    "            examples_text += f\"\\nExample {i}:\\nDataset sample:\\n{data_sample}\\nCorrect domain: {domain}\\n\"\n",
    "        return examples_text\n",
    "\n",
    "    @retry(stop=stop_after_attempt(5), wait=wait_fixed(20))\n",
    "    def classify(self, df: pd.DataFrame, mode: str = \"zero-shot\", examples: List[Tuple[str, str]] = None) -> List[str]:\n",
    "        if df.empty or not self.domains:\n",
    "            logger.warning(\"Empty DataFrame or no domains for classification\")\n",
    "            return [\"unknown\"] * 3\n",
    "\n",
    "        try:\n",
    "            data_sample = prepare_for_llm(df)\n",
    "\n",
    "            # print(f\"\\n{'=' * 80}\")\n",
    "            # print(f\"DATA REPRESENTATION FOR LLM (mode: {mode})\")\n",
    "            # print(f\"{'=' * 80}\")\n",
    "            # print(data_sample)\n",
    "            # print(f\"{'=' * 80}\")\n",
    "            \n",
    "            if mode == \"zero-shot\":\n",
    "                system_message = (\n",
    "                    \"You are a dataset domain classification expert. \"\n",
    "                    \"Analyze the dataset sample provided as a DataFrame-like structure \"\n",
    "                    \"(columns and data rows) and determine its primary domain. \"\n",
    "                    \"Choose ONLY from the following domains: {allowed_labels}\\n\\n\"\n",
    "                    \"Respond in strict JSON format: {{\\\"primary_domain\\\": \\\"domain\\\", \\\"alternative_domains\\\": [\\\"domain1\\\", \\\"domain2\\\"]}}\\n\"\n",
    "                    \"Where:\\n\"\n",
    "                    \"- primary_domain: The single most appropriate domain (MUST be from the list)\\n\"\n",
    "                    \"- alternative_domains: Up to 2 other relevant domains (only if applicable)\\n\"\n",
    "                    \"Use EXACT domain names. No additional comments!\"\n",
    "                )\n",
    "                human_message = (\n",
    "                    \"Here's a sample of a dataset represented as DataFrame:\"\n",
    "                    \"{dataframe}\"\n",
    "                    \"What are the top-3 most appropriate domains for this dataset?\\n\"\n",
    "                    \"{format_instructions}\"\n",
    "                )\n",
    "            \n",
    "            elif mode in [\"one-shot\", \"few-shot\"]:\n",
    "                examples_text = self.create_examples_text(examples) if examples else \"\"\n",
    "                system_message = (\n",
    "                    \"You are a dataset domain classification expert. \"\n",
    "                    \"Analyze dataset samples and determine their primary domain with optional alternatives. \"\n",
    "                    \"Choose ONLY from these domains: {allowed_labels}\\n\\n\"\n",
    "                    \"Response format (strict JSON):\\n\"\n",
    "                    \"{{\\\"primary_domain\\\": \\\"exact_domain_name\\\", \\\"alternative_domains\\\": [\\\"domain1\\\", \\\"domain2\\\"]}}\\n\\n\"\n",
    "                    \"Rules:\\n\"\n",
    "                    \"1. primary_domain: SINGLE most appropriate domain (MUST be from list)\\n\"\n",
    "                    \"2. alternative_domains: 0-2 other relevant domains (only if applicable)\\n\"\n",
    "                    \"3. Use EXACT domain names from list\\n\"\n",
    "                    \"4. No additional text/comments!\\n\\n\"\n",
    "                    \"Example:\\n\"\n",
    "                    \"{examples}\"\n",
    "                )\n",
    "                \n",
    "                human_message = (\n",
    "                    \"Here's a sample of a dataset represented as DataFrame:\"\n",
    "                    \"{dataframe}\"\n",
    "                    \"Classify this dataset. Use format: {format_instructions}\"\n",
    "                )\n",
    "            \n",
    "            prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", system_message),\n",
    "                (\"human\", human_message)\n",
    "            ])\n",
    "            \n",
    "            chain = prompt | self.llm | self.parser\n",
    "            \n",
    "            invoke_params = {\n",
    "                \"dataframe\": data_sample,\n",
    "                \"allowed_labels\": \", \".join(self.domains),\n",
    "                \"format_instructions\": self.parser.get_format_instructions()\n",
    "            }\n",
    "\n",
    "            \n",
    "            if mode in [\"one-shot\", \"few-shot\"] and examples:\n",
    "                invoke_params[\"examples\"] = self.create_examples_text(examples)\n",
    "            else:\n",
    "                invoke_params[\"examples\"] = \"\"\n",
    "            \n",
    "            result = chain.invoke(invoke_params)\n",
    "            \n",
    "            predicted_domains = [result.primary_domain.lower()]\n",
    "            predicted_domains.extend([domain.lower() for domain in result.alternative_domains])\n",
    "            while len(predicted_domains) < 3:\n",
    "                predicted_domains.append(\"unknown\")\n",
    "            \n",
    "            logger.debug(f\"Classification result ({mode}): {predicted_domains}\")\n",
    "            return predicted_domains[:3]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification error ({mode}): {e}\", exc_info=True)\n",
    "            return [\"unknown\"] * 3\n",
    "\n",
    "def load_all_datasets() -> Dict[str, Dict]:\n",
    "    datasets = {}\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        logger.error(f\"Dataset directory not found: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "        return datasets\n",
    "\n",
    "    dataset_ids = [d for d in os.listdir(OUTPUT_DIR) \n",
    "                   if os.path.isdir(os.path.join(OUTPUT_DIR, d))]\n",
    "    \n",
    "    valid_datasets = [\n",
    "        d for d in dataset_ids\n",
    "        if os.path.exists(os.path.join(OUTPUT_DIR, d, \"metadata.json\"))\n",
    "        and os.path.exists(os.path.join(OUTPUT_DIR, d, \"random_rows.csv\"))\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Loading {len(valid_datasets)} valid datasets...\")\n",
    "    \n",
    "    for dataset_id in tqdm(valid_datasets, desc=\"Loading datasets\"):\n",
    "        try:\n",
    "            meta_path = os.path.join(OUTPUT_DIR, dataset_id, \"metadata.json\")\n",
    "            data_path = os.path.join(OUTPUT_DIR, dataset_id, \"random_rows.csv\")\n",
    "\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            if \"domain\" not in metadata:\n",
    "                logger.warning(f\"Skipping {dataset_id}: 'domain' field missing in metadata\")\n",
    "                continue\n",
    "\n",
    "            df = read_csv_with_memory_limit(data_path, dataset_id=dataset_id)\n",
    "            if df.empty:\n",
    "                logger.warning(f\"Empty DataFrame for {dataset_id}, skipping\")\n",
    "                continue\n",
    "\n",
    "            datasets[dataset_id] = {\n",
    "                'metadata': metadata,\n",
    "                'dataframe': df,\n",
    "                'domain': metadata[\"domain\"].lower(),\n",
    "                'data_sample': prepare_for_llm(df)\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {dataset_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Successfully loaded {len(datasets)} datasets\")\n",
    "    return datasets\n",
    "\n",
    "def run_zero_shot_classification(datasets: Dict[str, Dict], run_id: int) -> Tuple[Optional[float], List[str], List[str], Optional[Dict], Optional[float]]:\n",
    "    logger.info(f\"=== Starting zero-shot classification run {run_id} ===\")\n",
    "    classifier = LLMClassifier()\n",
    "\n",
    "    if not classifier.domains or not datasets:\n",
    "        logger.error(\"Classification domains or datasets not loaded\")\n",
    "        return None, [], [], {}, None\n",
    "\n",
    "    test_size = len(datasets) // K_FOLDS\n",
    "    dataset_items = list(datasets.items())\n",
    "    \n",
    "    # Случайная выборка датасетов для тестирования\n",
    "    random.shuffle(dataset_items)\n",
    "    test_datasets = dict(dataset_items[:test_size])\n",
    "    \n",
    "    matches = 0\n",
    "    top3_matches = 0\n",
    "    total = 0\n",
    "    actual_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    pbar = tqdm(test_datasets.items(), desc=f\"Zero-shot classification (Run {run_id})\", unit=\"dataset\")\n",
    "\n",
    "    for dataset_id, data_info in pbar:\n",
    "        try:\n",
    "            predicted_domains = classifier.classify(data_info['dataframe'], mode=\"zero-shot\")\n",
    "            actual = data_info['domain']\n",
    "\n",
    "            if predicted_domains[0] == actual:\n",
    "                matches += 1\n",
    "            if actual in predicted_domains:\n",
    "                top3_matches += 1\n",
    "            total += 1\n",
    "\n",
    "            actual_labels.append(actual)\n",
    "            predicted_labels.append(predicted_domains[0])\n",
    "                \n",
    "            pbar.set_postfix({\"Accuracy\": f\"{matches/total:.2%}\" if total > 0 else \"N/A\",\n",
    "                            \"Top-3 Recall\": f\"{top3_matches/total:.2%}\" if total > 0 else \"N/A\"})\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if total > 0:\n",
    "        accuracy = matches / total\n",
    "        top3_recall = top3_matches / total\n",
    "        logger.info(f\"Zero-shot accuracy: {accuracy:.2%}\")\n",
    "        logger.info(f\"Zero-shot top-3 recall: {top3_recall:.2%}\")\n",
    "        \n",
    "        report = classification_report(actual_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "        \n",
    "        return accuracy, actual_labels, predicted_labels, report, top3_recall\n",
    "    else:\n",
    "        logger.warning(\"No datasets processed for accuracy calculation\")\n",
    "        return None, [], [], {}, None\n",
    "\n",
    "def run_kfold_classification(datasets: Dict[str, Dict], mode: str, run_id: int) -> Tuple[List[float], List[float], List[Dict]]:\n",
    "    logger.info(f\"=== Starting {mode} classification with K-fold validation (Run {run_id}) ===\")\n",
    "    classifier = LLMClassifier()\n",
    "\n",
    "    if not classifier.domains or not datasets:\n",
    "        logger.error(\"Classification domains or datasets not loaded\")\n",
    "        return [], [], []\n",
    "\n",
    "    dataset_items = list(datasets.items())\n",
    "    kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42 + run_id)\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    fold_top3_recalls = []\n",
    "    fold_reports = []\n",
    "\n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(kf.split(dataset_items)):\n",
    "        logger.info(f\"Processing fold {fold_idx + 1}/{K_FOLDS}\")\n",
    "        \n",
    "        train_datasets = [dataset_items[i] for i in train_indices]\n",
    "        test_datasets = [dataset_items[i] for i in test_indices]\n",
    "        \n",
    "        examples = []\n",
    "        if mode == \"one-shot\":\n",
    "            domain_examples = {}\n",
    "            for dataset_id, data_info in train_datasets:\n",
    "                domain = data_info['domain']\n",
    "                if domain not in domain_examples:\n",
    "                    domain_examples[domain] = (data_info['data_sample'], domain)\n",
    "            examples = list(domain_examples.values())\n",
    "        \n",
    "        elif mode == \"few-shot\":\n",
    "            domain_examples = {}\n",
    "            for dataset_id, data_info in train_datasets:\n",
    "                domain = data_info['domain']\n",
    "                if domain not in domain_examples:\n",
    "                    domain_examples[domain] = []\n",
    "                if len(domain_examples[domain]) < FEW_SHOT_SIZE:\n",
    "                    domain_examples[domain].append((data_info['data_sample'], domain))\n",
    "            \n",
    "            for domain_list in domain_examples.values():\n",
    "                examples.extend(domain_list)\n",
    "        \n",
    "        matches = 0\n",
    "        top3_matches = 0\n",
    "        total = 0\n",
    "        actual_labels = []\n",
    "        predicted_labels = []\n",
    "        \n",
    "        pbar = tqdm(test_datasets, desc=f\"{mode.capitalize()} fold {fold_idx + 1}\", unit=\"dataset\")\n",
    "        \n",
    "        for dataset_id, data_info in pbar:\n",
    "            try:\n",
    "                predicted_domains = classifier.classify(\n",
    "                    data_info['dataframe'], \n",
    "                    mode=mode, \n",
    "                    examples=examples\n",
    "                )\n",
    "                actual = data_info['domain']\n",
    "\n",
    "                if predicted_domains[0] == actual:\n",
    "                    matches += 1\n",
    "                if actual in predicted_domains:\n",
    "                    top3_matches += 1\n",
    "                total += 1\n",
    "\n",
    "                actual_labels.append(actual)\n",
    "                predicted_labels.append(predicted_domains[0])\n",
    "                    \n",
    "                pbar.set_postfix({\"Accuracy\": f\"{matches/total:.2%}\" if total > 0 else \"N/A\",\n",
    "                                \"Top-3 Recall\": f\"{top3_matches/total:.2%}\" if total > 0 else \"N/A\"})\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if total > 0:\n",
    "            fold_accuracy = matches / total\n",
    "            fold_top3_recall = top3_matches / total\n",
    "            fold_report = classification_report(actual_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "            \n",
    "            fold_accuracies.append(fold_accuracy)\n",
    "            fold_top3_recalls.append(fold_top3_recall)\n",
    "            fold_reports.append(fold_report)\n",
    "            \n",
    "            logger.info(f\"Fold {fold_idx + 1} - Accuracy: {fold_accuracy:.2%}, Top-3 Recall: {fold_top3_recall:.2%}\")\n",
    "\n",
    "    return fold_accuracies, fold_top3_recalls, fold_reports\n",
    "\n",
    "def calculate_aggregated_metrics(fold_reports: List[Dict]) -> Dict[str, float]:\n",
    "    if not fold_reports:\n",
    "        return {}\n",
    "    \n",
    "    # Собираем метрики по всем фолдам\n",
    "    macro_precisions = [report['macro avg']['precision'] for report in fold_reports]\n",
    "    macro_recalls = [report['macro avg']['recall'] for report in fold_reports]\n",
    "    macro_f1s = [report['macro avg']['f1-score'] for report in fold_reports]\n",
    "    weighted_f1s = [report['weighted avg']['f1-score'] for report in fold_reports]\n",
    "    \n",
    "    return {\n",
    "        'macro_precision_mean': np.mean(macro_precisions),\n",
    "        'macro_precision_std': np.std(macro_precisions),\n",
    "        'macro_recall_mean': np.mean(macro_recalls),\n",
    "        'macro_recall_std': np.std(macro_recalls),\n",
    "        'macro_f1_mean': np.mean(macro_f1s),\n",
    "        'macro_f1_std': np.std(macro_f1s),\n",
    "        'weighted_f1_mean': np.mean(weighted_f1s),\n",
    "        'weighted_f1_std': np.std(weighted_f1s)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ed982-f8a7-4f6e-a627-7b09f5732268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logger.info(\"Loading all datasets...\")\n",
    "    datasets = load_all_datasets()\n",
    "    \n",
    "    if not datasets:\n",
    "        logger.error(\"No datasets loaded, exiting\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Loaded {len(datasets)} datasets\")\n",
    "    \n",
    "    results = {\n",
    "        'zero-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        # },\n",
    "        # 'one-shot': {\n",
    "        #     'accuracies': [],\n",
    "        #     'top3_recalls': [],\n",
    "        #     'macro_precisions': [],\n",
    "        #     'macro_recalls': [],\n",
    "        #     'macro_f1s': [],\n",
    "        #     'weighted_f1s': []\n",
    "        # },\n",
    "        # 'few-shot': {\n",
    "        #     'accuracies': [],\n",
    "        #     'top3_recalls': [],\n",
    "        #     'macro_precisions': [],\n",
    "        #     'macro_recalls': [],\n",
    "        #     'macro_f1s': [],\n",
    "        #     'weighted_f1s': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"STARTING CLASSIFICATION EXPERIMENT\")\n",
    "    print(f\"Modes: Zero-shot, One-shot, Few-shot\")\n",
    "    print(f\"Runs per mode: {N_RUNS}\")\n",
    "    print(f\"K-Folds for shot learning: {K_FOLDS}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    # Zero-shot классификация\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ZERO-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        accuracy, actual_labels, predicted_labels, report, top3_recall = run_zero_shot_classification(datasets, run_id)\n",
    "        \n",
    "        if accuracy is not None:\n",
    "            results['zero-shot']['accuracies'].append(accuracy)\n",
    "            results['zero-shot']['top3_recalls'].append(top3_recall)\n",
    "            results['zero-shot']['macro_precisions'].append(report['macro avg']['precision'])\n",
    "            results['zero-shot']['macro_recalls'].append(report['macro avg']['recall'])\n",
    "            results['zero-shot']['macro_f1s'].append(report['macro avg']['f1-score'])\n",
    "            results['zero-shot']['weighted_f1s'].append(report['weighted avg']['f1-score'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={accuracy:.4f}, Top-3 Recall={top3_recall:.4f}\")\n",
    "    \n",
    "    # # One-shot классификация\n",
    "    # print(f\"\\n{'=' * 60}\")\n",
    "    # print(\"ONE-SHOT CLASSIFICATION\".center(60))\n",
    "    # print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # for run_id in range(1, N_RUNS + 1):\n",
    "    #     fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"one-shot\", run_id)\n",
    "        \n",
    "    #     if fold_accuracies:\n",
    "    #         run_accuracy = np.mean(fold_accuracies)\n",
    "    #         run_top3_recall = np.mean(fold_top3_recalls)\n",
    "    #         run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "    #         results['one-shot']['accuracies'].append(run_accuracy)\n",
    "    #         results['one-shot']['top3_recalls'].append(run_top3_recall)\n",
    "    #         results['one-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "    #         results['one-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "    #         results['one-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "    #         results['one-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "    #         print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    # # Few-shot классификация\n",
    "    # print(f\"\\n{'=' * 60}\")\n",
    "    # print(\"FEW-SHOT CLASSIFICATION\".center(60))\n",
    "    # print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # for run_id in range(1, N_RUNS + 1):\n",
    "    #     fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"few-shot\", run_id)\n",
    "        \n",
    "    #     if fold_accuracies:\n",
    "    #         run_accuracy = np.mean(fold_accuracies)\n",
    "    #         run_top3_recall = np.mean(fold_top3_recalls)\n",
    "    #         run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "    #         results['few-shot']['accuracies'].append(run_accuracy)\n",
    "    #         results['few-shot']['top3_recalls'].append(run_top3_recall)\n",
    "    #         results['few-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "    #         results['few-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "    #         results['few-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "    #         results['few-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "    #         print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"FINAL AGGREGATED RESULTS\".center(80))\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    final_results = {}\n",
    "    \n",
    "    # for mode in ['zero-shot', 'one-shot', 'few-shot']:\n",
    "    for mode in ['zero-shot']:\n",
    "        mode_results = results[mode]\n",
    "        \n",
    "        if mode_results['accuracies']:\n",
    "            final_results[mode] = {\n",
    "                'accuracy': {\n",
    "                    'mean': np.mean(mode_results['accuracies']),\n",
    "                    'std': np.std(mode_results['accuracies'])\n",
    "                },\n",
    "                'top3_recall': {\n",
    "                    'mean': np.mean(mode_results['top3_recalls']),\n",
    "                    'std': np.std(mode_results['top3_recalls'])\n",
    "                },\n",
    "                'macro_precision': {\n",
    "                    'mean': np.mean(mode_results['macro_precisions']),\n",
    "                    'std': np.std(mode_results['macro_precisions'])\n",
    "                },\n",
    "                'macro_recall': {\n",
    "                    'mean': np.mean(mode_results['macro_recalls']),\n",
    "                    'std': np.std(mode_results['macro_recalls'])\n",
    "                },\n",
    "                'macro_f1': {\n",
    "                    'mean': np.mean(mode_results['macro_f1s']),\n",
    "                    'std': np.std(mode_results['macro_f1s'])\n",
    "                },\n",
    "                'weighted_f1': {\n",
    "                    'mean': np.mean(mode_results['weighted_f1s']),\n",
    "                    'std': np.std(mode_results['weighted_f1s'])\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{mode.upper().replace('-', ' ')} RESULTS:\")\n",
    "            print(f\"Accuracy: {final_results[mode]['accuracy']['mean']:.4f} ± {final_results[mode]['accuracy']['std']:.4f}\")\n",
    "            print(f\"Top-3 Recall: {final_results[mode]['top3_recall']['mean']:.4f} ± {final_results[mode]['top3_recall']['std']:.4f}\")\n",
    "            print(f\"Macro Precision: {final_results[mode]['macro_precision']['mean']:.4f} ± {final_results[mode]['macro_precision']['std']:.4f}\")\n",
    "            print(f\"Macro Recall: {final_results[mode]['macro_recall']['mean']:.4f} ± {final_results[mode]['macro_recall']['std']:.4f}\")\n",
    "            print(f\"Macro F1: {final_results[mode]['macro_f1']['mean']:.4f} ± {final_results[mode]['macro_f1']['std']:.4f}\")\n",
    "            print(f\"Weighted F1: {final_results[mode]['weighted_f1']['mean']:.4f} ± {final_results[mode]['weighted_f1']['std']:.4f}\")\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, \"final_all_modes_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Results saved to final_all_modes_results.json\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927fc9f6-761b-4e53-8a3d-d8625499d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logger.info(\"Loading all datasets...\")\n",
    "    datasets = load_all_datasets()\n",
    "    \n",
    "    if not datasets:\n",
    "        logger.error(\"No datasets loaded, exiting\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Loaded {len(datasets)} datasets\")\n",
    "    \n",
    "    results = {\n",
    "        # 'zero-shot': {\n",
    "        #     'accuracies': [],\n",
    "        #     'top3_recalls': [],\n",
    "        #     'macro_precisions': [],\n",
    "        #     'macro_recalls': [],\n",
    "        #     'macro_f1s': [],\n",
    "        #     'weighted_f1s': []\n",
    "        # },\n",
    "        'one-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        # },\n",
    "        # 'few-shot': {\n",
    "        #     'accuracies': [],\n",
    "        #     'top3_recalls': [],\n",
    "        #     'macro_precisions': [],\n",
    "        #     'macro_recalls': [],\n",
    "        #     'macro_f1s': [],\n",
    "        #     'weighted_f1s': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"STARTING CLASSIFICATION EXPERIMENT\")\n",
    "    print(f\"Modes: Zero-shot, One-shot, Few-shot\")\n",
    "    print(f\"Runs per mode: {N_RUNS}\")\n",
    "    print(f\"K-Folds for shot learning: {K_FOLDS}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    # # Zero-shot классификация\n",
    "    # print(f\"\\n{'=' * 60}\")\n",
    "    # print(\"ZERO-SHOT CLASSIFICATION\".center(60))\n",
    "    # print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # for run_id in range(1, N_RUNS + 1):\n",
    "    #     accuracy, actual_labels, predicted_labels, report, top3_recall = run_zero_shot_classification(datasets, run_id)\n",
    "        \n",
    "    #     if accuracy is not None:\n",
    "    #         results['zero-shot']['accuracies'].append(accuracy)\n",
    "    #         results['zero-shot']['top3_recalls'].append(top3_recall)\n",
    "    #         results['zero-shot']['macro_precisions'].append(report['macro avg']['precision'])\n",
    "    #         results['zero-shot']['macro_recalls'].append(report['macro avg']['recall'])\n",
    "    #         results['zero-shot']['macro_f1s'].append(report['macro avg']['f1-score'])\n",
    "    #         results['zero-shot']['weighted_f1s'].append(report['weighted avg']['f1-score'])\n",
    "            \n",
    "    #         print(f\"Run {run_id}: Accuracy={accuracy:.4f}, Top-3 Recall={top3_recall:.4f}\")\n",
    "    \n",
    "    # One-shot классификация\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ONE-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"one-shot\", run_id)\n",
    "        \n",
    "        if fold_accuracies:\n",
    "            run_accuracy = np.mean(fold_accuracies)\n",
    "            run_top3_recall = np.mean(fold_top3_recalls)\n",
    "            run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "            results['one-shot']['accuracies'].append(run_accuracy)\n",
    "            results['one-shot']['top3_recalls'].append(run_top3_recall)\n",
    "            results['one-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "            results['one-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "            results['one-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "            results['one-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    # # Few-shot классификация\n",
    "    # print(f\"\\n{'=' * 60}\")\n",
    "    # print(\"FEW-SHOT CLASSIFICATION\".center(60))\n",
    "    # print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # for run_id in range(1, N_RUNS + 1):\n",
    "    #     fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"few-shot\", run_id)\n",
    "        \n",
    "    #     if fold_accuracies:\n",
    "    #         run_accuracy = np.mean(fold_accuracies)\n",
    "    #         run_top3_recall = np.mean(fold_top3_recalls)\n",
    "    #         run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "    #         results['few-shot']['accuracies'].append(run_accuracy)\n",
    "    #         results['few-shot']['top3_recalls'].append(run_top3_recall)\n",
    "    #         results['few-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "    #         results['few-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "    #         results['few-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "    #         results['few-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "    #         print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"FINAL AGGREGATED RESULTS\".center(80))\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    final_results = {}\n",
    "    \n",
    "    # for mode in ['zero-shot', 'one-shot', 'few-shot']:\n",
    "    for mode in ['one-shot']:\n",
    "        mode_results = results[mode]\n",
    "        \n",
    "        if mode_results['accuracies']:\n",
    "            final_results[mode] = {\n",
    "                'accuracy': {\n",
    "                    'mean': np.mean(mode_results['accuracies']),\n",
    "                    'std': np.std(mode_results['accuracies'])\n",
    "                },\n",
    "                'top3_recall': {\n",
    "                    'mean': np.mean(mode_results['top3_recalls']),\n",
    "                    'std': np.std(mode_results['top3_recalls'])\n",
    "                },\n",
    "                'macro_precision': {\n",
    "                    'mean': np.mean(mode_results['macro_precisions']),\n",
    "                    'std': np.std(mode_results['macro_precisions'])\n",
    "                },\n",
    "                'macro_recall': {\n",
    "                    'mean': np.mean(mode_results['macro_recalls']),\n",
    "                    'std': np.std(mode_results['macro_recalls'])\n",
    "                },\n",
    "                'macro_f1': {\n",
    "                    'mean': np.mean(mode_results['macro_f1s']),\n",
    "                    'std': np.std(mode_results['macro_f1s'])\n",
    "                },\n",
    "                'weighted_f1': {\n",
    "                    'mean': np.mean(mode_results['weighted_f1s']),\n",
    "                    'std': np.std(mode_results['weighted_f1s'])\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{mode.upper().replace('-', ' ')} RESULTS:\")\n",
    "            print(f\"Accuracy: {final_results[mode]['accuracy']['mean']:.4f} ± {final_results[mode]['accuracy']['std']:.4f}\")\n",
    "            print(f\"Top-3 Recall: {final_results[mode]['top3_recall']['mean']:.4f} ± {final_results[mode]['top3_recall']['std']:.4f}\")\n",
    "            print(f\"Macro Precision: {final_results[mode]['macro_precision']['mean']:.4f} ± {final_results[mode]['macro_precision']['std']:.4f}\")\n",
    "            print(f\"Macro Recall: {final_results[mode]['macro_recall']['mean']:.4f} ± {final_results[mode]['macro_recall']['std']:.4f}\")\n",
    "            print(f\"Macro F1: {final_results[mode]['macro_f1']['mean']:.4f} ± {final_results[mode]['macro_f1']['std']:.4f}\")\n",
    "            print(f\"Weighted F1: {final_results[mode]['weighted_f1']['mean']:.4f} ± {final_results[mode]['weighted_f1']['std']:.4f}\")\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, \"final_all_modes_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Results saved to final_all_modes_results.json\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6505e-f6d5-4e2d-a440-5b4139de0463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logger.info(\"Loading all datasets...\")\n",
    "    datasets = load_all_datasets()\n",
    "    \n",
    "    if not datasets:\n",
    "        logger.error(\"No datasets loaded, exiting\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Loaded {len(datasets)} datasets\")\n",
    "    \n",
    "    results = {\n",
    "        # 'zero-shot': {\n",
    "        #     'accuracies': [],\n",
    "        #     'top3_recalls': [],\n",
    "        #     'macro_precisions': [],\n",
    "        #     'macro_recalls': [],\n",
    "        #     'macro_f1s': [],\n",
    "        #     'weighted_f1s': []\n",
    "        # },\n",
    "        # 'one-shot': {\n",
    "        #     'accuracies': [],\n",
    "        #     'top3_recalls': [],\n",
    "        #     'macro_precisions': [],\n",
    "        #     'macro_recalls': [],\n",
    "        #     'macro_f1s': [],\n",
    "        #     'weighted_f1s': []\n",
    "        # },\n",
    "        'few-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"STARTING CLASSIFICATION EXPERIMENT\")\n",
    "    print(f\"Modes: Zero-shot, One-shot, Few-shot\")\n",
    "    print(f\"Runs per mode: {N_RUNS}\")\n",
    "    print(f\"K-Folds for shot learning: {K_FOLDS}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    # # Zero-shot классификация\n",
    "    # print(f\"\\n{'=' * 60}\")\n",
    "    # print(\"ZERO-SHOT CLASSIFICATION\".center(60))\n",
    "    # print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # for run_id in range(1, N_RUNS + 1):\n",
    "    #     accuracy, actual_labels, predicted_labels, report, top3_recall = run_zero_shot_classification(datasets, run_id)\n",
    "        \n",
    "    #     if accuracy is not None:\n",
    "    #         results['zero-shot']['accuracies'].append(accuracy)\n",
    "    #         results['zero-shot']['top3_recalls'].append(top3_recall)\n",
    "    #         results['zero-shot']['macro_precisions'].append(report['macro avg']['precision'])\n",
    "    #         results['zero-shot']['macro_recalls'].append(report['macro avg']['recall'])\n",
    "    #         results['zero-shot']['macro_f1s'].append(report['macro avg']['f1-score'])\n",
    "    #         results['zero-shot']['weighted_f1s'].append(report['weighted avg']['f1-score'])\n",
    "            \n",
    "    #         print(f\"Run {run_id}: Accuracy={accuracy:.4f}, Top-3 Recall={top3_recall:.4f}\")\n",
    "    \n",
    "    # # One-shot классификация\n",
    "    # print(f\"\\n{'=' * 60}\")\n",
    "    # print(\"ONE-SHOT CLASSIFICATION\".center(60))\n",
    "    # print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # for run_id in range(1, N_RUNS + 1):\n",
    "    #     fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"one-shot\", run_id)\n",
    "        \n",
    "    #     if fold_accuracies:\n",
    "    #         run_accuracy = np.mean(fold_accuracies)\n",
    "    #         run_top3_recall = np.mean(fold_top3_recalls)\n",
    "    #         run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "    #         results['one-shot']['accuracies'].append(run_accuracy)\n",
    "    #         results['one-shot']['top3_recalls'].append(run_top3_recall)\n",
    "    #         results['one-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "    #         results['one-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "    #         results['one-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "    #         results['one-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "    #         print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    # Few-shot классификация\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"FEW-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"few-shot\", run_id)\n",
    "        \n",
    "        if fold_accuracies:\n",
    "            run_accuracy = np.mean(fold_accuracies)\n",
    "            run_top3_recall = np.mean(fold_top3_recalls)\n",
    "            run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "            results['few-shot']['accuracies'].append(run_accuracy)\n",
    "            results['few-shot']['top3_recalls'].append(run_top3_recall)\n",
    "            results['few-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "            results['few-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "            results['few-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "            results['few-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"FINAL AGGREGATED RESULTS\".center(80))\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    final_results = {}\n",
    "    \n",
    "    # for mode in ['zero-shot', 'one-shot', 'few-shot']:\n",
    "    for mode in ['few-shot']:\n",
    "        mode_results = results[mode]\n",
    "        \n",
    "        if mode_results['accuracies']:\n",
    "            final_results[mode] = {\n",
    "                'accuracy': {\n",
    "                    'mean': np.mean(mode_results['accuracies']),\n",
    "                    'std': np.std(mode_results['accuracies'])\n",
    "                },\n",
    "                'top3_recall': {\n",
    "                    'mean': np.mean(mode_results['top3_recalls']),\n",
    "                    'std': np.std(mode_results['top3_recalls'])\n",
    "                },\n",
    "                'macro_precision': {\n",
    "                    'mean': np.mean(mode_results['macro_precisions']),\n",
    "                    'std': np.std(mode_results['macro_precisions'])\n",
    "                },\n",
    "                'macro_recall': {\n",
    "                    'mean': np.mean(mode_results['macro_recalls']),\n",
    "                    'std': np.std(mode_results['macro_recalls'])\n",
    "                },\n",
    "                'macro_f1': {\n",
    "                    'mean': np.mean(mode_results['macro_f1s']),\n",
    "                    'std': np.std(mode_results['macro_f1s'])\n",
    "                },\n",
    "                'weighted_f1': {\n",
    "                    'mean': np.mean(mode_results['weighted_f1s']),\n",
    "                    'std': np.std(mode_results['weighted_f1s'])\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{mode.upper().replace('-', ' ')} RESULTS:\")\n",
    "            print(f\"Accuracy: {final_results[mode]['accuracy']['mean']:.4f} ± {final_results[mode]['accuracy']['std']:.4f}\")\n",
    "            print(f\"Top-3 Recall: {final_results[mode]['top3_recall']['mean']:.4f} ± {final_results[mode]['top3_recall']['std']:.4f}\")\n",
    "            print(f\"Macro Precision: {final_results[mode]['macro_precision']['mean']:.4f} ± {final_results[mode]['macro_precision']['std']:.4f}\")\n",
    "            print(f\"Macro Recall: {final_results[mode]['macro_recall']['mean']:.4f} ± {final_results[mode]['macro_recall']['std']:.4f}\")\n",
    "            print(f\"Macro F1: {final_results[mode]['macro_f1']['mean']:.4f} ± {final_results[mode]['macro_f1']['std']:.4f}\")\n",
    "            print(f\"Weighted F1: {final_results[mode]['weighted_f1']['mean']:.4f} ± {final_results[mode]['weighted_f1']['std']:.4f}\")\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, \"final_all_modes_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Results saved to final_all_modes_results.json\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06512a6b-a404-4a2c-ae3b-1044bd597927",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2288bbd2-6c27-4932-850e-88775cc167c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"all_datasets\"\n",
    "RESULTS_DIR = \"classification_results\"\n",
    "DOMAIN_MAPPING_FILE = \"clustering_domain_mapping.json\"\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")  \n",
    "TIMEOUT = 60  \n",
    "N_RUNS = 5  \n",
    "K_FOLDS = 5  \n",
    "FEW_SHOT_SIZE = 5  \n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "if not DEEPSEEK_API_KEY:\n",
    "    raise ValueError(\"DEEPSEEK_API_KEY environment variable not set!\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('llm_classification.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def detect_encoding(file_path: str) -> str:\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            rawdata = f.read(50000)\n",
    "        result = chardet.detect(rawdata)\n",
    "        return result['encoding'] if result['confidence'] > 0.7 else 'utf-8'\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Encoding detection error: {e}, using utf-8\")\n",
    "        return 'utf-8'\n",
    "\n",
    "def read_csv_with_memory_limit(file_path: str, dataset_id: str = None) -> pd.DataFrame:\n",
    "    try:\n",
    "        if dataset_id:\n",
    "            random_path = os.path.join(OUTPUT_DIR, dataset_id, \"random_rows.csv\")\n",
    "            if os.path.exists(random_path):\n",
    "                encoding = detect_encoding(random_path)\n",
    "                df = pd.read_csv(random_path, encoding=encoding)\n",
    "                logger.debug(f\"Read random rows for {dataset_id} ({len(df)} rows, {len(df.columns)} columns)\")\n",
    "                return df\n",
    "\n",
    "        encoding = detect_encoding(file_path)\n",
    "        logger.debug(f\"Detected encoding: {encoding} for {os.path.basename(file_path)}\")\n",
    "        \n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            preview = pd.read_csv(f, nrows=1000)\n",
    "        \n",
    "        dtype_dict = preview.dtypes.to_dict()\n",
    "        \n",
    "        chunks = []\n",
    "        chunk_size = 10000\n",
    "        \n",
    "        for chunk in pd.read_csv(\n",
    "            file_path, \n",
    "            encoding=encoding,\n",
    "            chunksize=chunk_size,\n",
    "            dtype=dtype_dict\n",
    "        ):\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        if chunks:\n",
    "            df = pd.concat(chunks, ignore_index=True)\n",
    "            logger.debug(f\"Read dataset with {len(df)} rows and {len(df.columns)} columns\")\n",
    "            return df\n",
    "        else:\n",
    "            logger.warning(\"No data read, returning empty DataFrame\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading CSV: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def prepare_for_llm(df: pd.DataFrame, max_rows: int = 5) -> str:\n",
    "    df_subset = df.head(max_rows).copy()\n",
    "    df_subset = df_subset.fillna('missing')\n",
    "    \n",
    "    columns = df_subset.columns.tolist()\n",
    "    data = []\n",
    "    \n",
    "    for _, row in df_subset.iterrows():\n",
    "        formatted_row = []\n",
    "        for value in row:\n",
    "            if isinstance(value, str):\n",
    "                val = value.replace('\"', '\\\\\"')  \n",
    "                formatted_value = f'\"{val}\"'\n",
    "            else:\n",
    "                formatted_value = str(value)\n",
    "            formatted_row.append(formatted_value)\n",
    "        data.append(f\"[{', '.join(formatted_row)}]\")\n",
    "    \n",
    "    columns_str = \"[\" + \", \".join([f'\"{col}\"' for col in columns]) + \"]\"\n",
    "    data_str = \",\\n\".join(data)\n",
    "    \n",
    "    return f\"columns = {columns_str}\\ndata = [\\n{data_str}\\n]\"\n",
    "\n",
    "class DeepSeekChatModel(BaseChatModel):\n",
    "    api_key: str = Field(default=DEEPSEEK_API_KEY)\n",
    "    model_name: str = \"deepseek-chat\" \n",
    "    endpoint_url: str = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "    timeout: int = TIMEOUT\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"deepseek-chat\"\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        # Преобразование сообщений в формат DeepSeek\n",
    "        api_messages = []\n",
    "        for m in messages:\n",
    "            if isinstance(m, SystemMessage):\n",
    "                api_messages.append({\"role\": \"system\", \"content\": m.content})\n",
    "            else:\n",
    "                api_messages.append({\"role\": \"user\", \"content\": m.content})\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": api_messages,\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 2048,\n",
    "            \"stop\": stop if stop else [],\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.endpoint_url,\n",
    "                headers=headers,\n",
    "                json=payload,\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            response_data = response.json()\n",
    "            \n",
    "            content = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
    "            \n",
    "            if \"```json\" in content and \"```\" in content:\n",
    "                try:\n",
    "                    content = content.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "                except IndexError:\n",
    "                    logger.warning(\"Failed to parse JSON block from content\")\n",
    "            elif \"{\" in content and \"}\" in content:\n",
    "                try:\n",
    "                    content = content[content.index(\"{\"):content.rindex(\"}\") + 1]\n",
    "                except ValueError:\n",
    "                    logger.warning(\"Failed to extract JSON object from content\")\n",
    "            \n",
    "            return ChatResult(generations=[ChatGeneration(message=HumanMessage(content=content))])\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"DeepSeek API error: {str(e)}\")\n",
    "            return ChatResult(generations=[ChatGeneration(message=HumanMessage(content=\"{}\"))])\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        return {\"model_name\": self.model_name, \"endpoint_url\": self.endpoint_url}\n",
    "\n",
    "class ClassificationResult(BaseModel):\n",
    "    primary_domain: str = Field(..., description=\"The single most appropriate domain\")\n",
    "    alternative_domains: List[str] = Field([], description=\"List of alternative relevant domains\", max_items=2)\n",
    "\n",
    "class LLMClassifier:\n",
    "    def __init__(self):\n",
    "        self.domains = self.load_domains()\n",
    "        self.llm = DeepSeekChatModel()  \n",
    "        self.parser = PydanticOutputParser(pydantic_object=ClassificationResult)\n",
    "        logger.info(f\"Initialized with {len(self.domains)} domains\")\n",
    "\n",
    "    def load_domains(self) -> List[str]:\n",
    "        if not os.path.exists(DOMAIN_MAPPING_FILE):\n",
    "            logger.error(f\"Domain file not found: {os.path.abspath(DOMAIN_MAPPING_FILE)}\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            with open(DOMAIN_MAPPING_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                domain_data = json.load(f)\n",
    "                \n",
    "                if isinstance(domain_data, dict):\n",
    "                    domains = list(domain_data.keys())\n",
    "                    logger.info(f\"Loaded {len(domains)} domains\")\n",
    "                    return domains\n",
    "                else:\n",
    "                    logger.error(f\"Unknown domain file format: {type(domain_data)}\")\n",
    "                    return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading domains: {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def create_examples_text(self, examples: List[Tuple[str, str]]) -> str:\n",
    "        examples_text = \"\"\n",
    "        for i, (data_sample, domain) in enumerate(examples, 1):\n",
    "            examples_text += f\"\\nExample {i}:\\nDataset sample:\\n{data_sample}\\nCorrect domain: {domain}\\n\"\n",
    "        return examples_text\n",
    "\n",
    "    @retry(stop=stop_after_attempt(5), wait=wait_fixed(20))\n",
    "    def classify(self, df: pd.DataFrame, mode: str = \"zero-shot\", examples: List[Tuple[str, str]] = None) -> List[str]:\n",
    "        if df.empty or not self.domains:\n",
    "            logger.warning(\"Empty DataFrame or no domains for classification\")\n",
    "            return [\"unknown\"] * 3\n",
    "\n",
    "        try:\n",
    "            data_sample = prepare_for_llm(df)\n",
    "            \n",
    "            system_message = (\n",
    "                \"You are a dataset domain classification expert. \"\n",
    "                \"Analyze the dataset sample provided as a DataFrame-like structure \"\n",
    "                \"(columns and data rows) and determine its primary domain. \"\n",
    "                \"Choose ONLY from the following domains: {allowed_labels}\\n\\n\"\n",
    "                \"Respond in strict JSON format: {{\\\"primary_domain\\\": \\\"domain\\\", \\\"alternative_domains\\\": [\\\"domain1\\\", \\\"domain2\\\"]}}\\n\"\n",
    "                \"Where:\\n\"\n",
    "                \"- primary_domain: The single most appropriate domain (MUST be from the list)\\n\"\n",
    "                \"- alternative_domains: Up to 2 other relevant domains (only if applicable)\\n\"\n",
    "                \"Use EXACT domain names. No additional comments!\"\n",
    "            )\n",
    "            \n",
    "            human_message = (\n",
    "                \"Here's a sample of a dataset represented as DataFrame:\"\n",
    "                \"{dataframe}\"\n",
    "                \"Classify this dataset. Use format: {format_instructions}\"\n",
    "            )\n",
    "            \n",
    "            if mode in [\"one-shot\", \"few-shot\"] and examples:\n",
    "                human_message = (\n",
    "                    \"Here are some examples of correct classifications:\"\n",
    "                    \"{examples}\"\n",
    "                    \"\\n\\nNow classify this new dataset sample:\"\n",
    "                    \"{dataframe}\"\n",
    "                    \"Use format: {format_instructions}\"\n",
    "                )\n",
    "            \n",
    "            prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", system_message),\n",
    "                (\"human\", human_message)\n",
    "            ])\n",
    "            \n",
    "            chain = prompt | self.llm | self.parser\n",
    "            \n",
    "            invoke_params = {\n",
    "                \"dataframe\": data_sample,\n",
    "                \"allowed_labels\": \", \".join(self.domains),\n",
    "                \"format_instructions\": self.parser.get_format_instructions()\n",
    "            }\n",
    "\n",
    "            if mode in [\"one-shot\", \"few-shot\"] and examples:\n",
    "                invoke_params[\"examples\"] = self.create_examples_text(examples)\n",
    "            \n",
    "            result = chain.invoke(invoke_params)\n",
    "\n",
    "            domains = [result.primary_domain.lower()]\n",
    "            for alt in result.alternative_domains:\n",
    "                if alt.lower() not in domains and len(domains) < 3:\n",
    "                    domains.append(alt.lower())\n",
    "            \n",
    "            while len(domains) < 3:\n",
    "                domains.append(\"unknown\")\n",
    "            \n",
    "            logger.debug(f\"Classification result ({mode}): {domains}\")\n",
    "            return domains\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification error ({mode}): {e}\", exc_info=True)\n",
    "            return [\"unknown\"] * 3\n",
    "\n",
    "def load_all_datasets() -> Dict[str, Dict]:\n",
    "    datasets = {}\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        logger.error(f\"Dataset directory not found: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "        return datasets\n",
    "\n",
    "    dataset_ids = [d for d in os.listdir(OUTPUT_DIR) \n",
    "                   if os.path.isdir(os.path.join(OUTPUT_DIR, d))]\n",
    "    \n",
    "    valid_datasets = [\n",
    "        d for d in dataset_ids\n",
    "        if os.path.exists(os.path.join(OUTPUT_DIR, d, \"metadata.json\"))\n",
    "        and os.path.exists(os.path.join(OUTPUT_DIR, d, \"random_rows.csv\"))\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Loading {len(valid_datasets)} valid datasets...\")\n",
    "    \n",
    "    for dataset_id in tqdm(valid_datasets, desc=\"Loading datasets\"):\n",
    "        try:\n",
    "            meta_path = os.path.join(OUTPUT_DIR, dataset_id, \"metadata.json\")\n",
    "            data_path = os.path.join(OUTPUT_DIR, dataset_id, \"random_rows.csv\")\n",
    "\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            if \"domain\" not in metadata:\n",
    "                logger.warning(f\"Skipping {dataset_id}: 'domain' field missing in metadata\")\n",
    "                continue\n",
    "\n",
    "            df = read_csv_with_memory_limit(data_path, dataset_id=dataset_id)\n",
    "            if df.empty:\n",
    "                logger.warning(f\"Empty DataFrame for {dataset_id}, skipping\")\n",
    "                continue\n",
    "\n",
    "            datasets[dataset_id] = {\n",
    "                'metadata': metadata,\n",
    "                'dataframe': df,\n",
    "                'domain': metadata[\"domain\"].lower(),\n",
    "                'data_sample': prepare_for_llm(df)\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {dataset_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Successfully loaded {len(datasets)} datasets\")\n",
    "    return datasets\n",
    "\n",
    "def run_zero_shot_classification(datasets: Dict[str, Dict], run_id: int) -> Tuple[Optional[float], List[str], List[str], Optional[Dict], Optional[float]]:\n",
    "    logger.info(f\"=== Starting zero-shot classification run {run_id} ===\")\n",
    "    classifier = LLMClassifier()\n",
    "\n",
    "    if not classifier.domains or not datasets:\n",
    "        logger.error(\"Classification domains or datasets not loaded\")\n",
    "        return None, [], [], {}, None\n",
    "\n",
    "    test_size = len(datasets) // K_FOLDS\n",
    "    dataset_items = list(datasets.items())\n",
    "    \n",
    "    random.shuffle(dataset_items)\n",
    "    test_datasets = dict(dataset_items[:test_size])\n",
    "    \n",
    "    matches = 0\n",
    "    top3_matches = 0\n",
    "    total = 0\n",
    "    actual_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    pbar = tqdm(test_datasets.items(), desc=f\"Zero-shot classification (Run {run_id})\", unit=\"dataset\")\n",
    "\n",
    "    for dataset_id, data_info in pbar:\n",
    "        try:\n",
    "            predicted_domains = classifier.classify(data_info['dataframe'], mode=\"zero-shot\")\n",
    "            actual = data_info['domain']\n",
    "\n",
    "            if predicted_domains[0] == actual:\n",
    "                matches += 1\n",
    "            if actual in predicted_domains:\n",
    "                top3_matches += 1\n",
    "            total += 1\n",
    "\n",
    "            actual_labels.append(actual)\n",
    "            predicted_labels.append(predicted_domains[0])\n",
    "                \n",
    "            pbar.set_postfix({\"Accuracy\": f\"{matches/total:.2%}\" if total > 0 else \"N/A\",\n",
    "                            \"Top-3 Recall\": f\"{top3_matches/total:.2%}\" if total > 0 else \"N/A\"})\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if total > 0:\n",
    "        accuracy = matches / total\n",
    "        top3_recall = top3_matches / total\n",
    "        logger.info(f\"Zero-shot accuracy: {accuracy:.2%}\")\n",
    "        logger.info(f\"Zero-shot top-3 recall: {top3_recall:.2%}\")\n",
    "        \n",
    "        report = classification_report(actual_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "        \n",
    "        return accuracy, actual_labels, predicted_labels, report, top3_recall\n",
    "    else:\n",
    "        logger.warning(\"No datasets processed for accuracy calculation\")\n",
    "        return None, [], [], {}, None\n",
    "\n",
    "def run_kfold_classification(datasets: Dict[str, Dict], mode: str, run_id: int) -> Tuple[List[float], List[float], List[Dict]]:\n",
    "    logger.info(f\"=== Starting {mode} classification with K-fold validation (Run {run_id}) ===\")\n",
    "    classifier = LLMClassifier()\n",
    "\n",
    "    if not classifier.domains or not datasets:\n",
    "        logger.error(\"Classification domains or datasets not loaded\")\n",
    "        return [], [], []\n",
    "\n",
    "    dataset_items = list(datasets.items())\n",
    "    kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42 + run_id)\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    fold_top3_recalls = []\n",
    "    fold_reports = []\n",
    "\n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(kf.split(dataset_items)):\n",
    "        logger.info(f\"Processing fold {fold_idx + 1}/{K_FOLDS}\")\n",
    "        \n",
    "        train_datasets = [dataset_items[i] for i in train_indices]\n",
    "        test_datasets = [dataset_items[i] for i in test_indices]\n",
    "        \n",
    "        examples = []\n",
    "        if mode == \"one-shot\":\n",
    "            domain_examples = {}\n",
    "            for dataset_id, data_info in train_datasets:\n",
    "                domain = data_info['domain']\n",
    "                if domain not in domain_examples:\n",
    "                    domain_examples[domain] = (data_info['data_sample'], domain)\n",
    "            examples = list(domain_examples.values())\n",
    "        \n",
    "        elif mode == \"few-shot\":\n",
    "            domain_examples = {}\n",
    "            for dataset_id, data_info in train_datasets:\n",
    "                domain = data_info['domain']\n",
    "                if domain not in domain_examples:\n",
    "                    domain_examples[domain] = []\n",
    "                if len(domain_examples[domain]) < FEW_SHOT_SIZE:\n",
    "                    domain_examples[domain].append((data_info['data_sample'], domain))\n",
    "            \n",
    "            for domain_list in domain_examples.values():\n",
    "                examples.extend(domain_list)\n",
    "        \n",
    "        matches = 0\n",
    "        top3_matches = 0\n",
    "        total = 0\n",
    "        actual_labels = []\n",
    "        predicted_labels = []\n",
    "        \n",
    "        pbar = tqdm(test_datasets, desc=f\"{mode.capitalize()} fold {fold_idx + 1}\", unit=\"dataset\")\n",
    "        \n",
    "        for dataset_id, data_info in pbar:\n",
    "            try:\n",
    "                predicted_domains = classifier.classify(\n",
    "                    data_info['dataframe'], \n",
    "                    mode=mode, \n",
    "                    examples=examples\n",
    "                )\n",
    "                actual = data_info['domain']\n",
    "\n",
    "                if predicted_domains[0] == actual:\n",
    "                    matches += 1\n",
    "                if actual in predicted_domains:\n",
    "                    top3_matches += 1\n",
    "                total += 1\n",
    "\n",
    "                actual_labels.append(actual)\n",
    "                predicted_labels.append(predicted_domains[0])\n",
    "                    \n",
    "                pbar.set_postfix({\"Accuracy\": f\"{matches/total:.2%}\" if total > 0 else \"N/A\",\n",
    "                                \"Top-3 Recall\": f\"{top3_matches/total:.2%}\" if total > 0 else \"N/A\"})\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if total > 0:\n",
    "            fold_accuracy = matches / total\n",
    "            fold_top3_recall = top3_matches / total\n",
    "            fold_report = classification_report(actual_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "            \n",
    "            fold_accuracies.append(fold_accuracy)\n",
    "            fold_top3_recalls.append(fold_top3_recall)\n",
    "            fold_reports.append(fold_report)\n",
    "            \n",
    "            logger.info(f\"Fold {fold_idx + 1} - Accuracy: {fold_accuracy:.2%}, Top-3 Recall: {fold_top3_recall:.2%}\")\n",
    "\n",
    "    return fold_accuracies, fold_top3_recalls, fold_reports\n",
    "\n",
    "def calculate_aggregated_metrics(fold_reports: List[Dict]) -> Dict[str, float]:\n",
    "    if not fold_reports:\n",
    "        return {}\n",
    "    \n",
    "    macro_precisions = [report['macro avg']['precision'] for report in fold_reports]\n",
    "    macro_recalls = [report['macro avg']['recall'] for report in fold_reports]\n",
    "    macro_f1s = [report['macro avg']['f1-score'] for report in fold_reports]\n",
    "    weighted_f1s = [report['weighted avg']['f1-score'] for report in fold_reports]\n",
    "    \n",
    "    return {\n",
    "        'macro_precision_mean': np.mean(macro_precisions),\n",
    "        'macro_precision_std': np.std(macro_precisions),\n",
    "        'macro_recall_mean': np.mean(macro_recalls),\n",
    "        'macro_recall_std': np.std(macro_recalls),\n",
    "        'macro_f1_mean': np.mean(macro_f1s),\n",
    "        'macro_f1_std': np.std(macro_f1s),\n",
    "        'weighted_f1_mean': np.mean(weighted_f1s),\n",
    "        'weighted_f1_std': np.std(weighted_f1s)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee51e1-fe84-480c-81d3-f381765c5d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logger.info(\"Loading all datasets...\")\n",
    "    datasets = load_all_datasets()\n",
    "    \n",
    "    if not datasets:\n",
    "        logger.error(\"No datasets loaded, exiting\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Loaded {len(datasets)} datasets\")\n",
    "    \n",
    "    results = {\n",
    "        'zero-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        # },\n",
    "        # 'one-shot': {\n",
    "        #     'accuracies': [],\n",
    "        #     'top3_recalls': [],\n",
    "        #     'macro_precisions': [],\n",
    "        #     'macro_recalls': [],\n",
    "        #     'macro_f1s': [],\n",
    "        #     'weighted_f1s': []\n",
    "        # },\n",
    "        # 'few-shot': {\n",
    "        #     'accuracies': [],\n",
    "        #     'top3_recalls': [],\n",
    "        #     'macro_precisions': [],\n",
    "        #     'macro_recalls': [],\n",
    "        #     'macro_f1s': [],\n",
    "        #     'weighted_f1s': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"STARTING CLASSIFICATION EXPERIMENT\")\n",
    "    print(f\"Modes: Zero-shot, One-shot, Few-shot\")\n",
    "    print(f\"Runs per mode: {N_RUNS}\")\n",
    "    print(f\"K-Folds for shot learning: {K_FOLDS}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    # Zero-shot классификация\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ZERO-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        accuracy, actual_labels, predicted_labels, report, top3_recall = run_zero_shot_classification(datasets, run_id)\n",
    "        \n",
    "        if accuracy is not None:\n",
    "            results['zero-shot']['accuracies'].append(accuracy)\n",
    "            results['zero-shot']['top3_recalls'].append(top3_recall)\n",
    "            results['zero-shot']['macro_precisions'].append(report['macro avg']['precision'])\n",
    "            results['zero-shot']['macro_recalls'].append(report['macro avg']['recall'])\n",
    "            results['zero-shot']['macro_f1s'].append(report['macro avg']['f1-score'])\n",
    "            results['zero-shot']['weighted_f1s'].append(report['weighted avg']['f1-score'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={accuracy:.4f}, Top-3 Recall={top3_recall:.4f}\")\n",
    "    \n",
    "    # # One-shot классификация\n",
    "    # print(f\"\\n{'=' * 60}\")\n",
    "    # print(\"ONE-SHOT CLASSIFICATION\".center(60))\n",
    "    # print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # for run_id in range(1, N_RUNS + 1):\n",
    "    #     fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"one-shot\", run_id)\n",
    "        \n",
    "    #     if fold_accuracies:\n",
    "    #         run_accuracy = np.mean(fold_accuracies)\n",
    "    #         run_top3_recall = np.mean(fold_top3_recalls)\n",
    "    #         run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "    #         results['one-shot']['accuracies'].append(run_accuracy)\n",
    "    #         results['one-shot']['top3_recalls'].append(run_top3_recall)\n",
    "    #         results['one-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "    #         results['one-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "    #         results['one-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "    #         results['one-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "    #         print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    # # Few-shot классификация\n",
    "    # print(f\"\\n{'=' * 60}\")\n",
    "    # print(\"FEW-SHOT CLASSIFICATION\".center(60))\n",
    "    # print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # for run_id in range(1, N_RUNS + 1):\n",
    "    #     fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"few-shot\", run_id)\n",
    "        \n",
    "    #     if fold_accuracies:\n",
    "    #         run_accuracy = np.mean(fold_accuracies)\n",
    "    #         run_top3_recall = np.mean(fold_top3_recalls)\n",
    "    #         run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "    #         results['few-shot']['accuracies'].append(run_accuracy)\n",
    "    #         results['few-shot']['top3_recalls'].append(run_top3_recall)\n",
    "    #         results['few-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "    #         results['few-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "    #         results['few-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "    #         results['few-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "    #         print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"FINAL AGGREGATED RESULTS\".center(80))\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    final_results = {}\n",
    "    \n",
    "    # for mode in ['zero-shot', 'one-shot', 'few-shot']:\n",
    "    for mode in ['zero-shot']:\n",
    "        mode_results = results[mode]\n",
    "        \n",
    "        if mode_results['accuracies']:\n",
    "            final_results[mode] = {\n",
    "                'accuracy': {\n",
    "                    'mean': np.mean(mode_results['accuracies']),\n",
    "                    'std': np.std(mode_results['accuracies'])\n",
    "                },\n",
    "                'top3_recall': {\n",
    "                    'mean': np.mean(mode_results['top3_recalls']),\n",
    "                    'std': np.std(mode_results['top3_recalls'])\n",
    "                },\n",
    "                'macro_precision': {\n",
    "                    'mean': np.mean(mode_results['macro_precisions']),\n",
    "                    'std': np.std(mode_results['macro_precisions'])\n",
    "                },\n",
    "                'macro_recall': {\n",
    "                    'mean': np.mean(mode_results['macro_recalls']),\n",
    "                    'std': np.std(mode_results['macro_recalls'])\n",
    "                },\n",
    "                'macro_f1': {\n",
    "                    'mean': np.mean(mode_results['macro_f1s']),\n",
    "                    'std': np.std(mode_results['macro_f1s'])\n",
    "                },\n",
    "                'weighted_f1': {\n",
    "                    'mean': np.mean(mode_results['weighted_f1s']),\n",
    "                    'std': np.std(mode_results['weighted_f1s'])\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{mode.upper().replace('-', ' ')} RESULTS:\")\n",
    "            print(f\"Accuracy: {final_results[mode]['accuracy']['mean']:.4f} ± {final_results[mode]['accuracy']['std']:.4f}\")\n",
    "            print(f\"Top-3 Recall: {final_results[mode]['top3_recall']['mean']:.4f} ± {final_results[mode]['top3_recall']['std']:.4f}\")\n",
    "            print(f\"Macro Precision: {final_results[mode]['macro_precision']['mean']:.4f} ± {final_results[mode]['macro_precision']['std']:.4f}\")\n",
    "            print(f\"Macro Recall: {final_results[mode]['macro_recall']['mean']:.4f} ± {final_results[mode]['macro_recall']['std']:.4f}\")\n",
    "            print(f\"Macro F1: {final_results[mode]['macro_f1']['mean']:.4f} ± {final_results[mode]['macro_f1']['std']:.4f}\")\n",
    "            print(f\"Weighted F1: {final_results[mode]['weighted_f1']['mean']:.4f} ± {final_results[mode]['weighted_f1']['std']:.4f}\")\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, \"final_all_modes_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Results saved to final_all_modes_results.json\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a67fbd-3782-4052-b653-1e897944ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEOUT = 180\n",
    "\n",
    "def main():\n",
    "    logger.info(\"Loading all datasets...\")\n",
    "    datasets = load_all_datasets()\n",
    "    \n",
    "    if not datasets:\n",
    "        logger.error(\"No datasets loaded, exiting\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Loaded {len(datasets)} datasets\")\n",
    "    \n",
    "    results = {\n",
    "        # 'zero-shot': {\n",
    "        #     'accuracies': [],\n",
    "        #     'top3_recalls': [],\n",
    "        #     'macro_precisions': [],\n",
    "        #     'macro_recalls': [],\n",
    "        #     'macro_f1s': [],\n",
    "        #     'weighted_f1s': []\n",
    "        # },\n",
    "        'one-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        # },\n",
    "        # 'few-shot': {\n",
    "        #     'accuracies': [],\n",
    "        #     'top3_recalls': [],\n",
    "        #     'macro_precisions': [],\n",
    "        #     'macro_recalls': [],\n",
    "        #     'macro_f1s': [],\n",
    "        #     'weighted_f1s': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"STARTING CLASSIFICATION EXPERIMENT\")\n",
    "    print(f\"Modes: Zero-shot, One-shot, Few-shot\")\n",
    "    print(f\"Runs per mode: {N_RUNS}\")\n",
    "    print(f\"K-Folds for shot learning: {K_FOLDS}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    # # Zero-shot классификация\n",
    "    # print(f\"\\n{'=' * 60}\")\n",
    "    # print(\"ZERO-SHOT CLASSIFICATION\".center(60))\n",
    "    # print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # for run_id in range(1, N_RUNS + 1):\n",
    "    #     accuracy, actual_labels, predicted_labels, report, top3_recall = run_zero_shot_classification(datasets, run_id)\n",
    "        \n",
    "    #     if accuracy is not None:\n",
    "    #         results['zero-shot']['accuracies'].append(accuracy)\n",
    "    #         results['zero-shot']['top3_recalls'].append(top3_recall)\n",
    "    #         results['zero-shot']['macro_precisions'].append(report['macro avg']['precision'])\n",
    "    #         results['zero-shot']['macro_recalls'].append(report['macro avg']['recall'])\n",
    "    #         results['zero-shot']['macro_f1s'].append(report['macro avg']['f1-score'])\n",
    "    #         results['zero-shot']['weighted_f1s'].append(report['weighted avg']['f1-score'])\n",
    "            \n",
    "    #         print(f\"Run {run_id}: Accuracy={accuracy:.4f}, Top-3 Recall={top3_recall:.4f}\")\n",
    "    \n",
    "    # One-shot классификация\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ONE-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"one-shot\", run_id)\n",
    "        \n",
    "        if fold_accuracies:\n",
    "            run_accuracy = np.mean(fold_accuracies)\n",
    "            run_top3_recall = np.mean(fold_top3_recalls)\n",
    "            run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "            results['one-shot']['accuracies'].append(run_accuracy)\n",
    "            results['one-shot']['top3_recalls'].append(run_top3_recall)\n",
    "            results['one-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "            results['one-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "            results['one-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "            results['one-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    # # Few-shot классификация\n",
    "    # print(f\"\\n{'=' * 60}\")\n",
    "    # print(\"FEW-SHOT CLASSIFICATION\".center(60))\n",
    "    # print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # for run_id in range(1, N_RUNS + 1):\n",
    "    #     fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"few-shot\", run_id)\n",
    "        \n",
    "    #     if fold_accuracies:\n",
    "    #         run_accuracy = np.mean(fold_accuracies)\n",
    "    #         run_top3_recall = np.mean(fold_top3_recalls)\n",
    "    #         run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "    #         results['few-shot']['accuracies'].append(run_accuracy)\n",
    "    #         results['few-shot']['top3_recalls'].append(run_top3_recall)\n",
    "    #         results['few-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "    #         results['few-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "    #         results['few-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "    #         results['few-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "    #         print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"FINAL AGGREGATED RESULTS\".center(80))\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    final_results = {}\n",
    "    \n",
    "    # for mode in ['zero-shot', 'one-shot', 'few-shot']:\n",
    "    for mode in ['one-shot']:\n",
    "        mode_results = results[mode]\n",
    "        \n",
    "        if mode_results['accuracies']:\n",
    "            final_results[mode] = {\n",
    "                'accuracy': {\n",
    "                    'mean': np.mean(mode_results['accuracies']),\n",
    "                    'std': np.std(mode_results['accuracies'])\n",
    "                },\n",
    "                'top3_recall': {\n",
    "                    'mean': np.mean(mode_results['top3_recalls']),\n",
    "                    'std': np.std(mode_results['top3_recalls'])\n",
    "                },\n",
    "                'macro_precision': {\n",
    "                    'mean': np.mean(mode_results['macro_precisions']),\n",
    "                    'std': np.std(mode_results['macro_precisions'])\n",
    "                },\n",
    "                'macro_recall': {\n",
    "                    'mean': np.mean(mode_results['macro_recalls']),\n",
    "                    'std': np.std(mode_results['macro_recalls'])\n",
    "                },\n",
    "                'macro_f1': {\n",
    "                    'mean': np.mean(mode_results['macro_f1s']),\n",
    "                    'std': np.std(mode_results['macro_f1s'])\n",
    "                },\n",
    "                'weighted_f1': {\n",
    "                    'mean': np.mean(mode_results['weighted_f1s']),\n",
    "                    'std': np.std(mode_results['weighted_f1s'])\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{mode.upper().replace('-', ' ')} RESULTS:\")\n",
    "            print(f\"Accuracy: {final_results[mode]['accuracy']['mean']:.4f} ± {final_results[mode]['accuracy']['std']:.4f}\")\n",
    "            print(f\"Top-3 Recall: {final_results[mode]['top3_recall']['mean']:.4f} ± {final_results[mode]['top3_recall']['std']:.4f}\")\n",
    "            print(f\"Macro Precision: {final_results[mode]['macro_precision']['mean']:.4f} ± {final_results[mode]['macro_precision']['std']:.4f}\")\n",
    "            print(f\"Macro Recall: {final_results[mode]['macro_recall']['mean']:.4f} ± {final_results[mode]['macro_recall']['std']:.4f}\")\n",
    "            print(f\"Macro F1: {final_results[mode]['macro_f1']['mean']:.4f} ± {final_results[mode]['macro_f1']['std']:.4f}\")\n",
    "            print(f\"Weighted F1: {final_results[mode]['weighted_f1']['mean']:.4f} ± {final_results[mode]['weighted_f1']['std']:.4f}\")\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, \"final_all_modes_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Results saved to final_all_modes_results.json\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548634b7-4127-47b5-9956-c78bf5dc305a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TIMEOUT = 180\n",
    "\n",
    "def main():\n",
    "    logger.info(\"Loading all datasets...\")\n",
    "    datasets = load_all_datasets()\n",
    "    \n",
    "    if not datasets:\n",
    "        logger.error(\"No datasets loaded, exiting\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Loaded {len(datasets)} datasets\")\n",
    "    \n",
    "    results = {\n",
    "        # 'zero-shot': {\n",
    "        #     'accuracies': [],\n",
    "        #     'top3_recalls': [],\n",
    "        #     'macro_precisions': [],\n",
    "        #     'macro_recalls': [],\n",
    "        #     'macro_f1s': [],\n",
    "        #     'weighted_f1s': []\n",
    "        # },\n",
    "        # 'one-shot': {\n",
    "        #     'accuracies': [],\n",
    "        #     'top3_recalls': [],\n",
    "        #     'macro_precisions': [],\n",
    "        #     'macro_recalls': [],\n",
    "        #     'macro_f1s': [],\n",
    "        #     'weighted_f1s': []\n",
    "        # # },\n",
    "        'few-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"STARTING CLASSIFICATION EXPERIMENT\")\n",
    "    print(f\"Modes: Zero-shot, One-shot, Few-shot\")\n",
    "    print(f\"Runs per mode: {N_RUNS}\")\n",
    "    print(f\"K-Folds for shot learning: {K_FOLDS}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    # # Zero-shot классификация\n",
    "    # print(f\"\\n{'=' * 60}\")\n",
    "    # print(\"ZERO-SHOT CLASSIFICATION\".center(60))\n",
    "    # print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # for run_id in range(1, N_RUNS + 1):\n",
    "    #     accuracy, actual_labels, predicted_labels, report, top3_recall = run_zero_shot_classification(datasets, run_id)\n",
    "        \n",
    "    #     if accuracy is not None:\n",
    "    #         results['zero-shot']['accuracies'].append(accuracy)\n",
    "    #         results['zero-shot']['top3_recalls'].append(top3_recall)\n",
    "    #         results['zero-shot']['macro_precisions'].append(report['macro avg']['precision'])\n",
    "    #         results['zero-shot']['macro_recalls'].append(report['macro avg']['recall'])\n",
    "    #         results['zero-shot']['macro_f1s'].append(report['macro avg']['f1-score'])\n",
    "    #         results['zero-shot']['weighted_f1s'].append(report['weighted avg']['f1-score'])\n",
    "            \n",
    "    #         print(f\"Run {run_id}: Accuracy={accuracy:.4f}, Top-3 Recall={top3_recall:.4f}\")\n",
    "    \n",
    "    # # One-shot классификация\n",
    "    # print(f\"\\n{'=' * 60}\")\n",
    "    # print(\"ONE-SHOT CLASSIFICATION\".center(60))\n",
    "    # print(f\"{'=' * 60}\")\n",
    "    \n",
    "    # for run_id in range(1, N_RUNS + 1):\n",
    "    #     fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"one-shot\", run_id)\n",
    "        \n",
    "    #     if fold_accuracies:\n",
    "    #         run_accuracy = np.mean(fold_accuracies)\n",
    "    #         run_top3_recall = np.mean(fold_top3_recalls)\n",
    "    #         run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "    #         results['one-shot']['accuracies'].append(run_accuracy)\n",
    "    #         results['one-shot']['top3_recalls'].append(run_top3_recall)\n",
    "    #         results['one-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "    #         results['one-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "    #         results['one-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "    #         results['one-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "    #         print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    # Few-shot классификация\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"FEW-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"few-shot\", run_id)\n",
    "        \n",
    "        if fold_accuracies:\n",
    "            run_accuracy = np.mean(fold_accuracies)\n",
    "            run_top3_recall = np.mean(fold_top3_recalls)\n",
    "            run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "            results['few-shot']['accuracies'].append(run_accuracy)\n",
    "            results['few-shot']['top3_recalls'].append(run_top3_recall)\n",
    "            results['few-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "            results['few-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "            results['few-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "            results['few-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"FINAL AGGREGATED RESULTS\".center(80))\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    final_results = {}\n",
    "    \n",
    "    # for mode in ['zero-shot', 'one-shot', 'few-shot']:\n",
    "    for mode in ['few-shot']:\n",
    "        mode_results = results[mode]\n",
    "        \n",
    "        if mode_results['accuracies']:\n",
    "            final_results[mode] = {\n",
    "                'accuracy': {\n",
    "                    'mean': np.mean(mode_results['accuracies']),\n",
    "                    'std': np.std(mode_results['accuracies'])\n",
    "                },\n",
    "                'top3_recall': {\n",
    "                    'mean': np.mean(mode_results['top3_recalls']),\n",
    "                    'std': np.std(mode_results['top3_recalls'])\n",
    "                },\n",
    "                'macro_precision': {\n",
    "                    'mean': np.mean(mode_results['macro_precisions']),\n",
    "                    'std': np.std(mode_results['macro_precisions'])\n",
    "                },\n",
    "                'macro_recall': {\n",
    "                    'mean': np.mean(mode_results['macro_recalls']),\n",
    "                    'std': np.std(mode_results['macro_recalls'])\n",
    "                },\n",
    "                'macro_f1': {\n",
    "                    'mean': np.mean(mode_results['macro_f1s']),\n",
    "                    'std': np.std(mode_results['macro_f1s'])\n",
    "                },\n",
    "                'weighted_f1': {\n",
    "                    'mean': np.mean(mode_results['weighted_f1s']),\n",
    "                    'std': np.std(mode_results['weighted_f1s'])\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{mode.upper().replace('-', ' ')} RESULTS:\")\n",
    "            print(f\"Accuracy: {final_results[mode]['accuracy']['mean']:.4f} ± {final_results[mode]['accuracy']['std']:.4f}\")\n",
    "            print(f\"Top-3 Recall: {final_results[mode]['top3_recall']['mean']:.4f} ± {final_results[mode]['top3_recall']['std']:.4f}\")\n",
    "            print(f\"Macro Precision: {final_results[mode]['macro_precision']['mean']:.4f} ± {final_results[mode]['macro_precision']['std']:.4f}\")\n",
    "            print(f\"Macro Recall: {final_results[mode]['macro_recall']['mean']:.4f} ± {final_results[mode]['macro_recall']['std']:.4f}\")\n",
    "            print(f\"Macro F1: {final_results[mode]['macro_f1']['mean']:.4f} ± {final_results[mode]['macro_f1']['std']:.4f}\")\n",
    "            print(f\"Weighted F1: {final_results[mode]['weighted_f1']['mean']:.4f} ± {final_results[mode]['weighted_f1']['std']:.4f}\")\n",
    "    \n",
    "    # Сохранение результатов\n",
    "    with open(os.path.join(RESULTS_DIR, \"final_all_modes_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Results saved to final_all_modes_results.json\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe3718-a8b7-4bc0-a21b-525d75d50e55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ablative Experiment 1: only columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b60e58-af5c-42ff-bba8-a38db6ebf7be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef3356c-ca0a-4c95-b6b6-5c784ef8218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"all_datasets\"\n",
    "RESULTS_DIR = \"classification_results\"\n",
    "DOMAIN_MAPPING_FILE = \"clustering_domain_mapping.json\"\n",
    "LLM_API_URL = \"api_url\"\n",
    "TIMEOUT = 180\n",
    "N_RUNS = 5  \n",
    "K_FOLDS = 5  \n",
    "FEW_SHOT_SIZE = 5  \n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('llm_classification.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def prepare_columns_for_llm(columns: List[str]) -> str:\n",
    "    \"\"\"Подготовка списка колонок для LLM\"\"\"\n",
    "    return \"Columns: [\" + \", \".join([f'\"{col}\"' for col in columns]) + \"]\"\n",
    "\n",
    "class ClassificationResult(BaseModel):\n",
    "    primary_domain: str = Field(..., description=\"The most appropriate domain for the dataset\")\n",
    "    alternative_domains: List[str] = Field(\n",
    "        default_factory=list, \n",
    "        description=\"List of alternative relevant domains (up to 2)\",\n",
    "        max_items=2\n",
    "    )\n",
    "\n",
    "class FastAPIChatModel(BaseChatModel):\n",
    "    endpoint_url: str\n",
    "    timeout: int = TIMEOUT\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom-fastapi-chat-model\"\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: List[str] | None = None,\n",
    "        run_manager: CallbackManagerForLLMRun | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        api_messages = [\n",
    "            {\"role\": \"system\" if isinstance(m, SystemMessage) else \"user\", \"content\": m.content}\n",
    "            for m in messages\n",
    "        ]\n",
    "        payload = {\n",
    "            \"job_id\": \"domain_classification\",\n",
    "            \"priority\": None,\n",
    "            \"source\": \"local\",\n",
    "            \"meta\": {\"temperature\": 0.3, \"tokens_limit\": 2048, \"stop_words\": [], \"model\": None},\n",
    "            \"messages\": api_messages\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.endpoint_url, \n",
    "                headers=headers, \n",
    "                json=payload, \n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            content = response.json().get(\"content\", \"{}\")\n",
    "            if not isinstance(content, str):\n",
    "                logger.warning(f\"Содержимое ответа не является строкой: {type(content)}\")\n",
    "                content = str(content)\n",
    "            \n",
    "            if \"```json\" in content and \"```\" in content:\n",
    "                try:\n",
    "                    content = content.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "                except IndexError:\n",
    "                    logger.warning(\"Не удалось разобрать JSON-блок из содержимого\")\n",
    "                    content = \"{}\"\n",
    "            elif \"{\" in content and \"}\" in content:\n",
    "                try:\n",
    "                    content = content[content.index(\"{\"):content.rindex(\"}\") + 1]\n",
    "                except ValueError:\n",
    "                    logger.warning(\"Не удалось извлечь JSON-объект из содержимого\")\n",
    "                    content = \"{}\"\n",
    "            else:\n",
    "                logger.warning(\"В содержимом не найден валидный JSON\")\n",
    "                content = \"{}\"\n",
    "            \n",
    "            return ChatResult(generations=[ChatGeneration(message=HumanMessage(content=content))])\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка API: {str(e)}\")\n",
    "            return ChatResult(generations=[ChatGeneration(message=HumanMessage(content=\"{}\"))])\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        return {\"endpoint_url\": self.endpoint_url, \"timeout\": self.timeout}\n",
    "\n",
    "class LLMClassifier:\n",
    "    def __init__(self):\n",
    "        self.domains = self.load_domains()\n",
    "        self.llm = FastAPIChatModel(endpoint_url=LLM_API_URL, timeout=TIMEOUT)\n",
    "        self.parser = PydanticOutputParser(pydantic_object=ClassificationResult)\n",
    "        logger.info(f\"Initialized with {len(self.domains)} domains\")\n",
    "\n",
    "    def load_domains(self) -> List[str]:\n",
    "        if not os.path.exists(DOMAIN_MAPPING_FILE):\n",
    "            logger.error(f\"Domain file not found: {os.path.abspath(DOMAIN_MAPPING_FILE)}\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            with open(DOMAIN_MAPPING_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                domain_data = json.load(f)\n",
    "                \n",
    "            if isinstance(domain_data, dict):\n",
    "                domains = list(domain_data.keys())\n",
    "                logger.info(f\"Loaded {len(domains)} domains\")\n",
    "                return domains\n",
    "            else:\n",
    "                logger.error(f\"Unknown domain file format: {type(domain_data)}\")\n",
    "                return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading domains: {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def create_examples_text(self, examples: List[Tuple[str, str]]) -> str:\n",
    "        examples_text = \"\"\n",
    "        for i, (columns_repr, domain) in enumerate(examples, 1):\n",
    "            examples_text += f\"\\nExample {i}:\\nColumn names:\\n{columns_repr}\\nCorrect domain: {domain}\\n\"\n",
    "        return examples_text\n",
    "\n",
    "    @retry(stop=stop_after_attempt(5), wait=wait_fixed(20))\n",
    "    def classify(self, columns: List[str], mode: str = \"zero-shot\", examples: List[Tuple[str, str]] = None) -> List[str]:\n",
    "        if not columns or not self.domains:\n",
    "            logger.warning(\"Empty columns or no domains for classification\")\n",
    "            return [\"unknown\"] * 3\n",
    "\n",
    "        try:\n",
    "            columns_repr = prepare_columns_for_llm(columns)\n",
    "            \n",
    "            # print(f\"\\n{'=' * 80}\")\n",
    "            # print(f\"COLUMN NAMES FOR LLM (mode: {mode})\")\n",
    "            # print(f\"{'=' * 80}\")\n",
    "            # print(columns_repr)\n",
    "            # print(f\"{'=' * 80}\")\n",
    "            \n",
    "            if mode == \"zero-shot\":\n",
    "                system_message = (\n",
    "                    \"You are a dataset domain classification expert. \"\n",
    "                    \"Analyze ONLY the column names of a dataset and determine its primary domain. \"\n",
    "                    \"Choose ONLY from the following domains: {allowed_labels}\\n\\n\"\n",
    "                    \"Respond in strict JSON format: {{\\\"primary_domain\\\": \\\"domain\\\", \\\"alternative_domains\\\": [\\\"domain1\\\", \\\"domain2\\\"]}}\\n\"\n",
    "                    \"Where:\\n\"\n",
    "                    \"- primary_domain: The single most appropriate domain (MUST be from the list)\\n\"\n",
    "                    \"- alternative_domains: Up to 2 other relevant domains (only if applicable)\\n\"\n",
    "                    \"Use EXACT domain names. No additional comments!\"\n",
    "                )\n",
    "                human_message = (\n",
    "                    \"Here are the column names of a dataset:\"\n",
    "                    \"{columns}\"\n",
    "                    \"What are the top-3 most appropriate domains for this dataset?\\n\"\n",
    "                    \"{format_instructions}\"\n",
    "                )\n",
    "            \n",
    "            elif mode in [\"one-shot\", \"few-shot\"]:\n",
    "                examples_text = self.create_examples_text(examples) if examples else \"\"\n",
    "                system_message = (\n",
    "                    \"You are a dataset domain classification expert. \"\n",
    "                    \"Analyze ONLY column names of datasets and determine their primary domain with optional alternatives. \"\n",
    "                    \"Choose ONLY from these domains: {allowed_labels}\\n\\n\"\n",
    "                    \"Response format (strict JSON):\\n\"\n",
    "                    \"{{\\\"primary_domain\\\": \\\"exact_domain_name\\\", \\\"alternative_domains\\\": [\\\"domain1\\\", \\\"domain2\\\"]}}\\n\\n\"\n",
    "                    \"Rules:\\n\"\n",
    "                    \"1. primary_domain: SINGLE most appropriate domain (MUST be from list)\\n\"\n",
    "                    \"2. alternative_domains: 0-2 other relevant domains (only if applicable)\\n\"\n",
    "                    \"3. Use EXACT domain names from list\\n\"\n",
    "                    \"4. No additional text/comments!\\n\\n\"\n",
    "                    \"Examples:\\n\"\n",
    "                    \"{examples}\"\n",
    "                )\n",
    "                \n",
    "                human_message = (\n",
    "                    \"Here are the column names of a dataset:\"\n",
    "                    \"{columns}\"\n",
    "                    \"Classify this dataset. Use format: {format_instructions}\"\n",
    "                )\n",
    "            \n",
    "            prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", system_message),\n",
    "                (\"human\", human_message)\n",
    "            ])\n",
    "            \n",
    "            chain = prompt | self.llm | self.parser\n",
    "            \n",
    "            invoke_params = {\n",
    "                \"columns\": columns_repr,\n",
    "                \"allowed_labels\": \", \".join(self.domains),\n",
    "                \"format_instructions\": self.parser.get_format_instructions()\n",
    "            }\n",
    "            \n",
    "            if mode in [\"one-shot\", \"few-shot\"] and examples:\n",
    "                invoke_params[\"examples\"] = self.create_examples_text(examples)\n",
    "            else:\n",
    "                invoke_params[\"examples\"] = \"\"\n",
    "            \n",
    "            result = chain.invoke(invoke_params)\n",
    "            \n",
    "            predicted_domains = [result.primary_domain.lower()]\n",
    "            predicted_domains.extend([domain.lower() for domain in result.alternative_domains])\n",
    "            while len(predicted_domains) < 3:\n",
    "                predicted_domains.append(\"unknown\")\n",
    "            \n",
    "            logger.debug(f\"Classification result ({mode}): {predicted_domains}\")\n",
    "            return predicted_domains[:3]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification error ({mode}): {e}\", exc_info=True)\n",
    "            return [\"unknown\"] * 3\n",
    "\n",
    "def load_all_datasets() -> Dict[str, Dict]:\n",
    "    datasets = {}\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        logger.error(f\"Dataset directory not found: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "        return datasets\n",
    "\n",
    "    dataset_ids = [d for d in os.listdir(OUTPUT_DIR) \n",
    "                   if os.path.isdir(os.path.join(OUTPUT_DIR, d))]\n",
    "    \n",
    "    valid_datasets = [\n",
    "        d for d in dataset_ids\n",
    "        if os.path.exists(os.path.join(OUTPUT_DIR, d, \"metadata.json\"))\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Loading {len(valid_datasets)} valid datasets...\")\n",
    "    \n",
    "    for dataset_id in tqdm(valid_datasets, desc=\"Loading datasets\"):\n",
    "        try:\n",
    "            meta_path = os.path.join(OUTPUT_DIR, dataset_id, \"metadata.json\")\n",
    "\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            if \"domain\" not in metadata:\n",
    "                logger.warning(f\"Skipping {dataset_id}: 'domain' field missing in metadata\")\n",
    "                continue\n",
    "                \n",
    "            # Получаем названия колонок из метаданных\n",
    "            columns = metadata.get(\"columns\", [])\n",
    "            if not columns:\n",
    "                logger.warning(f\"Skipping {dataset_id}: no columns in metadata\")\n",
    "                continue\n",
    "\n",
    "            datasets[dataset_id] = {\n",
    "                'metadata': metadata,\n",
    "                'columns': columns,\n",
    "                'domain': metadata[\"domain\"].lower(),\n",
    "                'columns_repr': prepare_columns_for_llm(columns)\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {dataset_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Successfully loaded {len(datasets)} datasets\")\n",
    "    return datasets\n",
    "\n",
    "def run_zero_shot_classification(datasets: Dict[str, Dict], run_id: int) -> Tuple[Optional[float], List[str], List[str], Optional[Dict], Optional[float]]:\n",
    "    logger.info(f\"=== Starting zero-shot classification run {run_id} ===\")\n",
    "    classifier = LLMClassifier()\n",
    "\n",
    "    if not classifier.domains or not datasets:\n",
    "        logger.error(\"Classification domains or datasets not loaded\")\n",
    "        return None, [], [], {}, None\n",
    "\n",
    "    test_size = len(datasets) // K_FOLDS\n",
    "    dataset_items = list(datasets.items())\n",
    "    \n",
    "    random.shuffle(dataset_items)\n",
    "    test_datasets = dict(dataset_items[:test_size])\n",
    "    \n",
    "    matches = 0\n",
    "    top3_matches = 0\n",
    "    total = 0\n",
    "    actual_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    pbar = tqdm(test_datasets.items(), desc=f\"Zero-shot classification (Run {run_id})\", unit=\"dataset\")\n",
    "\n",
    "    for dataset_id, data_info in pbar:\n",
    "        try:\n",
    "            predicted_domains = classifier.classify(data_info['columns'], mode=\"zero-shot\")\n",
    "            actual = data_info['domain']\n",
    "\n",
    "            if predicted_domains[0] == actual:\n",
    "                matches += 1\n",
    "            if actual in predicted_domains:\n",
    "                top3_matches += 1\n",
    "            total += 1\n",
    "\n",
    "            actual_labels.append(actual)\n",
    "            predicted_labels.append(predicted_domains[0])\n",
    "                \n",
    "            pbar.set_postfix({\"Accuracy\": f\"{matches/total:.2%}\" if total > 0 else \"N/A\",\n",
    "                            \"Top-3 Recall\": f\"{top3_matches/total:.2%}\" if total > 0 else \"N/A\"})\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if total > 0:\n",
    "        accuracy = matches / total\n",
    "        top3_recall = top3_matches / total\n",
    "        logger.info(f\"Zero-shot accuracy: {accuracy:.2%}\")\n",
    "        logger.info(f\"Zero-shot top-3 recall: {top3_recall:.2%}\")\n",
    "        \n",
    "        report = classification_report(actual_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "        \n",
    "        return accuracy, actual_labels, predicted_labels, report, top3_recall\n",
    "    else:\n",
    "        logger.warning(\"No datasets processed for accuracy calculation\")\n",
    "        return None, [], [], {}, None\n",
    "\n",
    "def run_kfold_classification(datasets: Dict[str, Dict], mode: str, run_id: int) -> Tuple[List[float], List[float], List[Dict]]:\n",
    "    logger.info(f\"=== Starting {mode} classification with K-fold validation (Run {run_id}) ===\")\n",
    "    classifier = LLMClassifier()\n",
    "\n",
    "    if not classifier.domains or not datasets:\n",
    "        logger.error(\"Classification domains or datasets not loaded\")\n",
    "        return [], [], []\n",
    "\n",
    "    dataset_items = list(datasets.items())\n",
    "    kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42 + run_id)\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    fold_top3_recalls = []\n",
    "    fold_reports = []\n",
    "\n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(kf.split(dataset_items)):\n",
    "        logger.info(f\"Processing fold {fold_idx + 1}/{K_FOLDS}\")\n",
    "        \n",
    "        train_datasets = [dataset_items[i] for i in train_indices]\n",
    "        test_datasets = [dataset_items[i] for i in test_indices]\n",
    "        \n",
    "        examples = []\n",
    "        if mode == \"one-shot\":\n",
    "            domain_examples = {}\n",
    "            for dataset_id, data_info in train_datasets:\n",
    "                domain = data_info['domain']\n",
    "                if domain not in domain_examples:\n",
    "                    domain_examples[domain] = (data_info['columns_repr'], domain)\n",
    "            examples = list(domain_examples.values())\n",
    "        \n",
    "        elif mode == \"few-shot\":\n",
    "            domain_examples = {}\n",
    "            for dataset_id, data_info in train_datasets:\n",
    "                domain = data_info['domain']\n",
    "                if domain not in domain_examples:\n",
    "                    domain_examples[domain] = []\n",
    "                if len(domain_examples[domain]) < FEW_SHOT_SIZE:\n",
    "                    domain_examples[domain].append((data_info['columns_repr'], domain))\n",
    "            \n",
    "            # Собираем все примеры\n",
    "            for domain_list in domain_examples.values():\n",
    "                examples.extend(domain_list)\n",
    "        \n",
    "        matches = 0\n",
    "        top3_matches = 0\n",
    "        total = 0\n",
    "        actual_labels = []\n",
    "        predicted_labels = []\n",
    "        \n",
    "        pbar = tqdm(test_datasets, desc=f\"{mode.capitalize()} fold {fold_idx + 1}\", unit=\"dataset\")\n",
    "        \n",
    "        for dataset_id, data_info in pbar:\n",
    "            try:\n",
    "                predicted_domains = classifier.classify(\n",
    "                    data_info['columns'], \n",
    "                    mode=mode, \n",
    "                    examples=examples\n",
    "                )\n",
    "                actual = data_info['domain']\n",
    "\n",
    "                if predicted_domains[0] == actual:\n",
    "                    matches += 1\n",
    "                if actual in predicted_domains:\n",
    "                    top3_matches += 1\n",
    "                total += 1\n",
    "\n",
    "                actual_labels.append(actual)\n",
    "                predicted_labels.append(predicted_domains[0])\n",
    "                    \n",
    "                pbar.set_postfix({\"Accuracy\": f\"{matches/total:.2%}\" if total > 0 else \"N/A\",\n",
    "                                \"Top-3 Recall\": f\"{top3_matches/total:.2%}\" if total > 0 else \"N/A\"})\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if total > 0:\n",
    "            fold_accuracy = matches / total\n",
    "            fold_top3_recall = top3_matches / total\n",
    "            fold_report = classification_report(actual_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "            \n",
    "            fold_accuracies.append(fold_accuracy)\n",
    "            fold_top3_recalls.append(fold_top3_recall)\n",
    "            fold_reports.append(fold_report)\n",
    "            \n",
    "            logger.info(f\"Fold {fold_idx + 1} - Accuracy: {fold_accuracy:.2%}, Top-3 Recall: {fold_top3_recall:.2%}\")\n",
    "\n",
    "    return fold_accuracies, fold_top3_recalls, fold_reports\n",
    "\n",
    "def calculate_aggregated_metrics(fold_reports: List[Dict]) -> Dict[str, float]:\n",
    "    if not fold_reports:\n",
    "        return {}\n",
    "  \n",
    "    macro_precisions = [report['macro avg']['precision'] for report in fold_reports]\n",
    "    macro_recalls = [report['macro avg']['recall'] for report in fold_reports]\n",
    "    macro_f1s = [report['macro avg']['f1-score'] for report in fold_reports]\n",
    "    weighted_f1s = [report['weighted avg']['f1-score'] for report in fold_reports]\n",
    "    \n",
    "    return {\n",
    "        'macro_precision_mean': np.mean(macro_precisions),\n",
    "        'macro_precision_std': np.std(macro_precisions),\n",
    "        'macro_recall_mean': np.mean(macro_recalls),\n",
    "        'macro_recall_std': np.std(macro_recalls),\n",
    "        'macro_f1_mean': np.mean(macro_f1s),\n",
    "        'macro_f1_std': np.std(macro_f1s),\n",
    "        'weighted_f1_mean': np.mean(weighted_f1s),\n",
    "        'weighted_f1_std': np.std(weighted_f1s)\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    logger.info(\"Loading all datasets metadata...\")\n",
    "    datasets = load_all_datasets()\n",
    "    \n",
    "    if not datasets:\n",
    "        logger.error(\"No datasets loaded, exiting\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Loaded {len(datasets)} datasets\")\n",
    "    \n",
    "    # Результаты для всех режимов\n",
    "    results = {\n",
    "        'zero-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        },\n",
    "        'one-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        },\n",
    "        'few-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"STARTING CLASSIFICATION EXPERIMENT\")\n",
    "    print(f\"Modes: Zero-shot, One-shot, Few-shot\")\n",
    "    print(f\"Runs per mode: {N_RUNS}\")\n",
    "    print(f\"K-Folds for shot learning: {K_FOLDS}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    # Zero-shot классификация\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ZERO-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        accuracy, actual_labels, predicted_labels, report, top3_recall = run_zero_shot_classification(datasets, run_id)\n",
    "        \n",
    "        if accuracy is not None:\n",
    "            results['zero-shot']['accuracies'].append(accuracy)\n",
    "            results['zero-shot']['top3_recalls'].append(top3_recall)\n",
    "            results['zero-shot']['macro_precisions'].append(report['macro avg']['precision'])\n",
    "            results['zero-shot']['macro_recalls'].append(report['macro avg']['recall'])\n",
    "            results['zero-shot']['macro_f1s'].append(report['macro avg']['f1-score'])\n",
    "            results['zero-shot']['weighted_f1s'].append(report['weighted avg']['f1-score'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={accuracy:.4f}, Top-3 Recall={top3_recall:.4f}\")\n",
    "    \n",
    "    # One-shot классификация\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ONE-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"one-shot\", run_id)\n",
    "        \n",
    "        if fold_accuracies:\n",
    "            run_accuracy = np.mean(fold_accuracies)\n",
    "            run_top3_recall = np.mean(fold_top3_recalls)\n",
    "            run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "            results['one-shot']['accuracies'].append(run_accuracy)\n",
    "            results['one-shot']['top3_recalls'].append(run_top3_recall)\n",
    "            results['one-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "            results['one-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "            results['one-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "            results['one-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    # Few-shot классификация\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"FEW-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"few-shot\", run_id)\n",
    "        \n",
    "        if fold_accuracies:\n",
    "            run_accuracy = np.mean(fold_accuracies)\n",
    "            run_top3_recall = np.mean(fold_top3_recalls)\n",
    "            run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "            results['few-shot']['accuracies'].append(run_accuracy)\n",
    "            results['few-shot']['top3_recalls'].append(run_top3_recall)\n",
    "            results['few-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "            results['few-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "            results['few-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "            results['few-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"FINAL AGGREGATED RESULTS\".center(80))\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    final_results = {}\n",
    "    \n",
    "    for mode in ['zero-shot', 'one-shot', 'few-shot']:\n",
    "        mode_results = results[mode]\n",
    "        \n",
    "        if mode_results['accuracies']:\n",
    "            final_results[mode] = {\n",
    "                'accuracy': {\n",
    "                    'mean': np.mean(mode_results['accuracies']),\n",
    "                    'std': np.std(mode_results['accuracies'])\n",
    "                },\n",
    "                'top3_recall': {\n",
    "                    'mean': np.mean(mode_results['top3_recalls']),\n",
    "                    'std': np.std(mode_results['top3_recalls'])\n",
    "                },\n",
    "                'macro_precision': {\n",
    "                    'mean': np.mean(mode_results['macro_precisions']),\n",
    "                    'std': np.std(mode_results['macro_precisions'])\n",
    "                },\n",
    "                'macro_recall': {\n",
    "                    'mean': np.mean(mode_results['macro_recalls']),\n",
    "                    'std': np.std(mode_results['macro_recalls'])\n",
    "                },\n",
    "                'macro_f1': {\n",
    "                    'mean': np.mean(mode_results['macro_f1s']),\n",
    "                    'std': np.std(mode_results['macro_f1s'])\n",
    "                },\n",
    "                'weighted_f1': {\n",
    "                    'mean': np.mean(mode_results['weighted_f1s']),\n",
    "                    'std': np.std(mode_results['weighted_f1s'])\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{mode.upper().replace('-', ' ')} RESULTS:\")\n",
    "            print(f\"Accuracy: {final_results[mode]['accuracy']['mean']:.4f} ± {final_results[mode]['accuracy']['std']:.4f}\")\n",
    "            print(f\"Top-3 Recall: {final_results[mode]['top3_recall']['mean']:.4f} ± {final_results[mode]['top3_recall']['std']:.4f}\")\n",
    "            print(f\"Macro Precision: {final_results[mode]['macro_precision']['mean']:.4f} ± {final_results[mode]['macro_precision']['std']:.4f}\")\n",
    "            print(f\"Macro Recall: {final_results[mode]['macro_recall']['mean']:.4f} ± {final_results[mode]['macro_recall']['std']:.4f}\")\n",
    "            print(f\"Macro F1: {final_results[mode]['macro_f1']['mean']:.4f} ± {final_results[mode]['macro_f1']['std']:.4f}\")\n",
    "            print(f\"Weighted F1: {final_results[mode]['weighted_f1']['mean']:.4f} ± {final_results[mode]['weighted_f1']['std']:.4f}\")\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, \"final_all_modes_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Results saved to final_all_modes_results.json\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3f3a9a-a2c6-4b82-accf-c9def0b76f81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## gpt-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69cd43d-be44-48fa-8d3b-a2c8ff50d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"all_datasets\"\n",
    "RESULTS_DIR = \"classification_results\"\n",
    "DOMAIN_MAPPING_FILE = \"clustering_domain_mapping.json\"\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "TIMEOUT = 180\n",
    "N_RUNS = 5\n",
    "K_FOLDS = 5\n",
    "FEW_SHOT_SIZE = 5\n",
    "MODEL_NAME = \"openai/gpt-4o\"  \n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('llm_classification.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ClassificationResult(BaseModel):\n",
    "    primary_domain: str = Field(..., description=\"The most appropriate domain for the dataset\")\n",
    "    alternative_domains: List[str] = Field(\n",
    "        default_factory=list, \n",
    "        description=\"List of alternative relevant domains (up to 2)\",\n",
    "        max_items=2\n",
    "    )\n",
    "\n",
    "def prepare_columns_for_llm(columns: List[str]) -> str:\n",
    "    return \"Columns: [\" + \", \".join([f'\"{col}\"' for col in columns]) + \"]\"\n",
    "\n",
    "def detect_encoding(file_path: str) -> str:\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            rawdata = f.read(50000)\n",
    "        result = chardet.detect(rawdata)\n",
    "        return result['encoding'] if result['confidence'] > 0.7 else 'utf-8'\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Encoding detection error: {e}, using utf-8\")\n",
    "        return 'utf-8'\n",
    "\n",
    "def get_columns_from_csv(file_path: str) -> List[str]:\n",
    "    try:\n",
    "        encoding = detect_encoding(file_path)\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            reader = csv.reader(f)\n",
    "            columns = next(reader)\n",
    "        return columns\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading columns from CSV: {e}\")\n",
    "        return []\n",
    "\n",
    "class GPT4oClassifier:\n",
    "    def __init__(self):\n",
    "        self.domains = self.load_domains()\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=OPENROUTER_API_KEY,\n",
    "            default_headers={\n",
    "                \"HTTP-Referer\": \"https://your-site.com\",\n",
    "                \"X-Title\": \"Dataset Classifier\"\n",
    "            }\n",
    "        )\n",
    "        self.parser = PydanticOutputParser(pydantic_object=ClassificationResult)\n",
    "        logger.info(f\"Initialized with {len(self.domains)} domains\")\n",
    "\n",
    "    def load_domains(self) -> List[str]:\n",
    "        if not os.path.exists(DOMAIN_MAPPING_FILE):\n",
    "            logger.error(f\"Domain file not found: {os.path.abspath(DOMAIN_MAPPING_FILE)}\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            with open(DOMAIN_MAPPING_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                domain_data = json.load(f)\n",
    "                \n",
    "                if isinstance(domain_data, dict):\n",
    "                    domains = list(domain_data.keys())\n",
    "                    logger.info(f\"Loaded {len(domains)} domains\")\n",
    "                    return domains\n",
    "                else:\n",
    "                    logger.error(f\"Unknown domain file format: {type(domain_data)}\")\n",
    "                    return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading domains: {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def create_examples_text(self, examples: List[Tuple[str, dict]]) -> str:\n",
    "        examples_text = \"\"\n",
    "        for i, (columns_repr, classification) in enumerate(examples, 1):\n",
    "            examples_text += (\n",
    "                f\"\\nExample {i}:\\nColumn names:\\n{columns_repr}\\n\"\n",
    "                f\"Classification: {json.dumps(classification)}\\n\"\n",
    "            )\n",
    "        return examples_text\n",
    "\n",
    "    @retry(stop=stop_after_attempt(5), wait=wait_fixed(20))\n",
    "    def classify(self, columns: List[str], mode: str = \"zero-shot\", examples: List[Tuple[str, dict]] = None) -> ClassificationResult:\n",
    "        if not columns or not self.domains:\n",
    "            logger.warning(\"Empty columns or no domains for classification\")\n",
    "            return ClassificationResult(primary_domain=\"unknown\", alternative_domains=[])\n",
    "        \n",
    "        try:\n",
    "            columns_repr = prepare_columns_for_llm(columns)\n",
    "            format_instructions = self.parser.get_format_instructions()\n",
    "            \n",
    "            # print(f\"\\n{'=' * 80}\")\n",
    "            # print(f\"COLUMN NAMES FOR LLM (mode: {mode})\")\n",
    "            # print(f\"{'=' * 80}\")\n",
    "            # print(columns_repr)\n",
    "            # print(f\"{'=' * 80}\")\n",
    "            \n",
    "            if mode == \"zero-shot\":\n",
    "                system_content = (\n",
    "                    \"You are a dataset domain classification expert. \"\n",
    "                    \"Analyze ONLY the column names of a dataset and determine its primary domain. \"\n",
    "                    \"Choose ONLY from the following domains: \" + \", \".join(self.domains) + \"\\n\\n\" +\n",
    "                    format_instructions + \"\\n\\n\" +\n",
    "                    \"Rules:\\n\"\n",
    "                    \"1. primary_domain: SINGLE most appropriate domain (MUST be from list)\\n\"\n",
    "                    \"2. alternative_domains: 0-2 other relevant domains (only if applicable)\\n\"\n",
    "                    \"3. Use EXACT domain names\\n\"\n",
    "                    \"4. No additional text/comments!\"\n",
    "                )\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_content},\n",
    "                    {\"role\": \"user\", \"content\": f\"Column names:\\n{columns_repr}\"}\n",
    "                ]\n",
    "            \n",
    "            elif mode in [\"one-shot\", \"few-shot\"] and examples:\n",
    "                examples_text = self.create_examples_text(examples)\n",
    "                system_content = (\n",
    "                    \"You are a dataset domain classification expert. \"\n",
    "                    \"Analyze column names and determine their primary domain with optional alternatives. \"\n",
    "                    \"Choose ONLY from these domains: \" + \", \".join(self.domains) + \"\\n\\n\" +\n",
    "                    format_instructions + \"\\n\\n\" +\n",
    "                    \"Rules:\\n\"\n",
    "                    \"1. primary_domain: SINGLE most appropriate domain (MUST be from list)\\n\"\n",
    "                    \"2. alternative_domains: 0-2 other relevant domains (only if applicable)\\n\"\n",
    "                    \"3. Use EXACT domain names\\n\"\n",
    "                    \"4. No additional text/comments!\\n\\n\" +\n",
    "                    \"Examples:\" + examples_text\n",
    "                )\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_content},\n",
    "                    {\"role\": \"user\", \"content\": f\"Classify this new dataset based on its column names:\\n{columns_repr}\"}\n",
    "                ]\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mode or missing examples: {mode}\")\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=messages,\n",
    "                temperature=0.3,\n",
    "                max_tokens=500,\n",
    "                response_format={\"type\": \"json_object\"}  \n",
    "            )\n",
    "            \n",
    "            content = response.choices[0].message.content\n",
    "            try:\n",
    "                result = self.parser.parse(content)\n",
    "            except ValidationError as e:\n",
    "                logger.error(f\"Validation error: {e}\\nResponse content: {content}\")\n",
    "                if \"```json\" in content:\n",
    "                    try:\n",
    "                        json_content = content.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "                        result = self.parser.parse(json_content)\n",
    "                    except Exception:\n",
    "                        result = ClassificationResult(primary_domain=\"unknown\", alternative_domains=[])\n",
    "                else:\n",
    "                    result = ClassificationResult(primary_domain=\"unknown\", alternative_domains=[])\n",
    "                \n",
    "            logger.debug(f\"Classification result ({mode}): {result}\")\n",
    "            return result\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification error ({mode}): {e}\", exc_info=True)\n",
    "            return ClassificationResult(primary_domain=\"unknown\", alternative_domains=[])\n",
    "\n",
    "def load_all_datasets() -> Dict[str, Dict]:\n",
    "    datasets = {}\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        logger.error(f\"Dataset directory not found: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "        return datasets\n",
    "\n",
    "    dataset_ids = [d for d in os.listdir(OUTPUT_DIR) \n",
    "                   if os.path.isdir(os.path.join(OUTPUT_DIR, d))]\n",
    "    \n",
    "    logger.info(f\"Loading datasets from {len(dataset_ids)} directories...\")\n",
    "    \n",
    "    for dataset_id in tqdm(dataset_ids, desc=\"Loading datasets\"):\n",
    "        try:\n",
    "            meta_path = os.path.join(OUTPUT_DIR, dataset_id, \"metadata.json\")\n",
    "            csv_path = os.path.join(OUTPUT_DIR, dataset_id, \"random_rows.csv\")\n",
    "\n",
    "            if not os.path.exists(meta_path):\n",
    "                logger.warning(f\"Skipping {dataset_id}: metadata.json not found\")\n",
    "                continue\n",
    "                \n",
    "            if not os.path.exists(csv_path):\n",
    "                logger.warning(f\"Skipping {dataset_id}: random_rows.csv not found\")\n",
    "                continue\n",
    "\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            if \"domain\" not in metadata:\n",
    "                logger.warning(f\"Skipping {dataset_id}: 'domain' field missing in metadata\")\n",
    "                continue\n",
    "                \n",
    "            columns = get_columns_from_csv(csv_path)\n",
    "            if not columns:\n",
    "                logger.warning(f\"Skipping {dataset_id}: no columns found in CSV\")\n",
    "                continue\n",
    "\n",
    "            datasets[dataset_id] = {\n",
    "                'metadata': metadata,\n",
    "                'columns': columns,\n",
    "                'domain': metadata[\"domain\"].lower(),\n",
    "                'columns_repr': prepare_columns_for_llm(columns)\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {dataset_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Successfully loaded {len(datasets)} datasets\")\n",
    "    return datasets\n",
    "\n",
    "def run_zero_shot_classification(datasets: Dict[str, Dict], run_id: int) -> Tuple[Optional[float], List[str], List[str], Optional[Dict], Optional[float]]:\n",
    "    logger.info(f\"=== Starting zero-shot classification run {run_id} ===\")\n",
    "    classifier = GPT4oClassifier()\n",
    "\n",
    "    if not classifier.domains or not datasets:\n",
    "        logger.error(\"Classification domains or datasets not loaded\")\n",
    "        return None, [], [], {}, None\n",
    "\n",
    "    test_size = len(datasets) // K_FOLDS\n",
    "    dataset_items = list(datasets.items())\n",
    "    random.shuffle(dataset_items)\n",
    "    test_datasets = dict(dataset_items[:test_size])\n",
    "    \n",
    "    matches = 0\n",
    "    top3_matches = 0\n",
    "    total = 0\n",
    "    actual_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    pbar = tqdm(test_datasets.items(), desc=f\"Zero-shot classification (Run {run_id})\", unit=\"dataset\")\n",
    "\n",
    "    for dataset_id, data_info in pbar:\n",
    "        try:\n",
    "            result = classifier.classify(data_info['columns'], mode=\"zero-shot\")\n",
    "            actual = data_info['domain']\n",
    "            predicted_primary = result.primary_domain.lower()\n",
    "            \n",
    "            all_predictions = [predicted_primary] + [alt.lower() for alt in result.alternative_domains]\n",
    "            top3 = all_predictions[:3]\n",
    "\n",
    "            if predicted_primary == actual:\n",
    "                matches += 1\n",
    "            if actual in top3:\n",
    "                top3_matches += 1\n",
    "                \n",
    "            total += 1\n",
    "            actual_labels.append(actual)\n",
    "            predicted_labels.append(predicted_primary)\n",
    "                \n",
    "            pbar.set_postfix({\n",
    "                \"Accuracy\": f\"{matches/total:.2%}\" if total > 0 else \"N/A\",\n",
    "                \"Top-3 Recall\": f\"{top3_matches/total:.2%}\" if total > 0 else \"N/A\"\n",
    "            })\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if total > 0:\n",
    "        accuracy = matches / total\n",
    "        top3_recall = top3_matches / total\n",
    "        logger.info(f\"Zero-shot accuracy: {accuracy:.2%}\")\n",
    "        logger.info(f\"Zero-shot top-3 recall: {top3_recall:.2%}\")\n",
    "        \n",
    "        report = classification_report(actual_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "        return accuracy, actual_labels, predicted_labels, report, top3_recall\n",
    "    else:\n",
    "        logger.warning(\"No datasets processed for accuracy calculation\")\n",
    "        return None, [], [], {}, None\n",
    "\n",
    "def run_kfold_classification(datasets: Dict[str, Dict], mode: str, run_id: int) -> Tuple[List[float], List[float], List[Dict]]:\n",
    "    logger.info(f\"=== Starting {mode} classification with K-fold validation (Run {run_id}) ===\")\n",
    "    classifier = GPT4oClassifier()\n",
    "\n",
    "    if not classifier.domains or not datasets:\n",
    "        logger.error(\"Classification domains or datasets not loaded\")\n",
    "        return [], [], []\n",
    "\n",
    "    dataset_items = list(datasets.items())\n",
    "    kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42 + run_id)\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    fold_top3_recalls = []\n",
    "    fold_reports = []\n",
    "\n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(kf.split(dataset_items)):\n",
    "        logger.info(f\"Processing fold {fold_idx + 1}/{K_FOLDS}\")\n",
    "        \n",
    "        train_datasets = [dataset_items[i] for i in train_indices]\n",
    "        test_datasets = [dataset_items[i] for i in test_indices]\n",
    "        \n",
    "        examples = []\n",
    "        if mode == \"one-shot\":\n",
    "            domain_examples = {}\n",
    "            for dataset_id, data_info in train_datasets:\n",
    "                domain = data_info['domain']\n",
    "                if domain not in domain_examples:\n",
    "                    classification = {\n",
    "                        \"primary_domain\": domain,\n",
    "                        \"alternative_domains\": []\n",
    "                    }\n",
    "                    domain_examples[domain] = (data_info['columns_repr'], classification)\n",
    "            examples = list(domain_examples.values())\n",
    "        \n",
    "        elif mode == \"few-shot\":\n",
    "            domain_examples = {}\n",
    "            for dataset_id, data_info in train_datasets:\n",
    "                domain = data_info['domain']\n",
    "                if domain not in domain_examples:\n",
    "                    domain_examples[domain] = []\n",
    "                \n",
    "                if len(domain_examples[domain]) < FEW_SHOT_SIZE:\n",
    "                    classification = {\n",
    "                        \"primary_domain\": domain,\n",
    "                        \"alternative_domains\": []\n",
    "                    }\n",
    "                    domain_examples[domain].append((data_info['columns_repr'], classification))\n",
    "            \n",
    "            for domain_list in domain_examples.values():\n",
    "                examples.extend(domain_list)\n",
    "        \n",
    "        matches = 0\n",
    "        top3_matches = 0\n",
    "        total = 0\n",
    "        actual_labels = []\n",
    "        predicted_labels = []\n",
    "        \n",
    "        pbar = tqdm(test_datasets, desc=f\"{mode.capitalize()} fold {fold_idx + 1}\", unit=\"dataset\")\n",
    "        \n",
    "        for dataset_id, data_info in pbar:\n",
    "            try:\n",
    "                result = classifier.classify(\n",
    "                    data_info['columns'], \n",
    "                    mode=mode, \n",
    "                    examples=examples\n",
    "                )\n",
    "                actual = data_info['domain']\n",
    "                predicted_primary = result.primary_domain.lower()\n",
    "                \n",
    "                all_predictions = [predicted_primary] + [alt.lower() for alt in result.alternative_domains]\n",
    "                top3 = all_predictions[:3]\n",
    "\n",
    "                if predicted_primary == actual:\n",
    "                    matches += 1\n",
    "                if actual in top3:\n",
    "                    top3_matches += 1\n",
    "                    \n",
    "                total += 1\n",
    "                actual_labels.append(actual)\n",
    "                predicted_labels.append(predicted_primary)\n",
    "                    \n",
    "                pbar.set_postfix({\n",
    "                    \"Accuracy\": f\"{matches/total:.2%}\" if total > 0 else \"N/A\",\n",
    "                    \"Top-3 Recall\": f\"{top3_matches/total:.2%}\" if total > 0 else \"N/A\"\n",
    "                })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if total > 0:\n",
    "            fold_accuracy = matches / total\n",
    "            fold_top3_recall = top3_matches / total\n",
    "            fold_report = classification_report(actual_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "            \n",
    "            fold_accuracies.append(fold_accuracy)\n",
    "            fold_top3_recalls.append(fold_top3_recall)\n",
    "            fold_reports.append(fold_report)\n",
    "            \n",
    "            logger.info(f\"Fold {fold_idx + 1} - Accuracy: {fold_accuracy:.2%}, Top-3 Recall: {fold_top3_recall:.2%}\")\n",
    "\n",
    "    return fold_accuracies, fold_top3_recalls, fold_reports\n",
    "\n",
    "def calculate_aggregated_metrics(fold_reports: List[Dict]) -> Dict[str, float]:\n",
    "    if not fold_reports:\n",
    "        return {}\n",
    "    \n",
    "    macro_precisions = [report['macro avg']['precision'] for report in fold_reports]\n",
    "    macro_recalls = [report['macro avg']['recall'] for report in fold_reports]\n",
    "    macro_f1s = [report['macro avg']['f1-score'] for report in fold_reports]\n",
    "    weighted_f1s = [report['weighted avg']['f1-score'] for report in fold_reports]\n",
    "    \n",
    "    return {\n",
    "        'macro_precision_mean': np.mean(macro_precisions),\n",
    "        'macro_precision_std': np.std(macro_precisions),\n",
    "        'macro_recall_mean': np.mean(macro_recalls),\n",
    "        'macro_recall_std': np.std(macro_recalls),\n",
    "        'macro_f1_mean': np.mean(macro_f1s),\n",
    "        'macro_f1_std': np.std(macro_f1s),\n",
    "        'weighted_f1_mean': np.mean(weighted_f1s),\n",
    "        'weighted_f1_std': np.std(weighted_f1s)\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    logger.info(\"Loading all datasets metadata and column names...\")\n",
    "    datasets = load_all_datasets()\n",
    "    \n",
    "    if not datasets:\n",
    "        logger.error(\"No datasets loaded, exiting\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Loaded {len(datasets)} datasets\")\n",
    "    \n",
    "    results = {\n",
    "        'zero-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        },\n",
    "        'one-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        },\n",
    "        'few-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"STARTING CLASSIFICATION EXPERIMENT\")\n",
    "    print(f\"Model: {MODEL_NAME}\")\n",
    "    print(f\"Modes: Zero-shot, One-shot, Few-shot\")\n",
    "    print(f\"Runs per mode: {N_RUNS}\")\n",
    "    print(f\"K-Folds for shot learning: {K_FOLDS}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ZERO-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        accuracy, actual_labels, predicted_labels, report, top3_recall = run_zero_shot_classification(datasets, run_id)\n",
    "        \n",
    "        if accuracy is not None:\n",
    "            results['zero-shot']['accuracies'].append(accuracy)\n",
    "            results['zero-shot']['top3_recalls'].append(top3_recall)\n",
    "            results['zero-shot']['macro_precisions'].append(report['macro avg']['precision'])\n",
    "            results['zero-shot']['macro_recalls'].append(report['macro avg']['recall'])\n",
    "            results['zero-shot']['macro_f1s'].append(report['macro avg']['f1-score'])\n",
    "            results['zero-shot']['weighted_f1s'].append(report['weighted avg']['f1-score'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={accuracy:.4f}, Top-3 Recall={top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ONE-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"one-shot\", run_id)\n",
    "        \n",
    "        if fold_accuracies:\n",
    "            run_accuracy = np.mean(fold_accuracies)\n",
    "            run_top3_recall = np.mean(fold_top3_recalls)\n",
    "            run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "            results['one-shot']['accuracies'].append(run_accuracy)\n",
    "            results['one-shot']['top3_recalls'].append(run_top3_recall)\n",
    "            results['one-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "            results['one-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "            results['one-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "            results['one-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"FEW-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"few-shot\", run_id)\n",
    "        \n",
    "        if fold_accuracies:\n",
    "            run_accuracy = np.mean(fold_accuracies)\n",
    "            run_top3_recall = np.mean(fold_top3_recalls)\n",
    "            run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "            results['few-shot']['accuracies'].append(run_accuracy)\n",
    "            results['few-shot']['top3_recalls'].append(run_top3_recall)\n",
    "            results['few-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "            results['few-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "            results['few-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "            results['few-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"FINAL AGGREGATED RESULTS\".center(80))\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    final_results = {}\n",
    "    \n",
    "    for mode in ['zero-shot', 'one-shot', 'few-shot']:\n",
    "        mode_results = results[mode]\n",
    "        \n",
    "        if mode_results['accuracies']:\n",
    "            final_results[mode] = {\n",
    "                'accuracy': {\n",
    "                    'mean': np.mean(mode_results['accuracies']),\n",
    "                    'std': np.std(mode_results['accuracies'])\n",
    "                },\n",
    "                'top3_recall': {\n",
    "                    'mean': np.mean(mode_results['top3_recalls']),\n",
    "                    'std': np.std(mode_results['top3_recalls'])\n",
    "                },\n",
    "                'macro_precision': {\n",
    "                    'mean': np.mean(mode_results['macro_precisions']),\n",
    "                    'std': np.std(mode_results['macro_precisions'])\n",
    "                },\n",
    "                'macro_recall': {\n",
    "                    'mean': np.mean(mode_results['macro_recalls']),\n",
    "                    'std': np.std(mode_results['macro_recalls'])\n",
    "                },\n",
    "                'macro_f1': {\n",
    "                    'mean': np.mean(mode_results['macro_f1s']),\n",
    "                    'std': np.std(mode_results['macro_f1s'])\n",
    "                },\n",
    "                'weighted_f1': {\n",
    "                    'mean': np.mean(mode_results['weighted_f1s']),\n",
    "                    'std': np.std(mode_results['weighted_f1s'])\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{mode.upper().replace('-', ' ')} RESULTS:\")\n",
    "            print(f\"Accuracy: {final_results[mode]['accuracy']['mean']:.4f} ± {final_results[mode]['accuracy']['std']:.4f}\")\n",
    "            print(f\"Top-3 Recall: {final_results[mode]['top3_recall']['mean']:.4f} ± {final_results[mode]['top3_recall']['std']:.4f}\")\n",
    "            print(f\"Macro Precision: {final_results[mode]['macro_precision']['mean']:.4f} ± {final_results[mode]['macro_precision']['std']:.4f}\")\n",
    "            print(f\"Macro Recall: {final_results[mode]['macro_recall']['mean']:.4f} ± {final_results[mode]['macro_recall']['std']:.4f}\")\n",
    "            print(f\"Macro F1: {final_results[mode]['macro_f1']['mean']:.4f} ± {final_results[mode]['macro_f1']['std']:.4f}\")\n",
    "            print(f\"Weighted F1: {final_results[mode]['weighted_f1']['mean']:.4f} ± {final_results[mode]['weighted_f1']['std']:.4f}\")\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, \"final_all_modes_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Results saved to final_all_modes_results.json\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e7a2fb-ee46-4de4-8413-f159336ec8eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d90a4-a932-458e-993b-55b35af439e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"all_datasets\"\n",
    "RESULTS_DIR = \"classification_results\"\n",
    "DOMAIN_MAPPING_FILE = \"clustering_domain_mapping.json\"\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "TIMEOUT = 180\n",
    "N_RUNS = 5\n",
    "K_FOLDS = 5\n",
    "FEW_SHOT_SIZE = 5\n",
    "MODEL_NAME = \"deepseek/deepseek-chat\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('llm_classification.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ClassificationResult(BaseModel):\n",
    "    primary_domain: str = Field(..., description=\"The most appropriate domain for the dataset\")\n",
    "    alternative_domains: List[str] = Field(\n",
    "        default_factory=list, \n",
    "        description=\"List of alternative relevant domains (up to 2)\",\n",
    "        max_items=2\n",
    "    )\n",
    "\n",
    "def prepare_columns_for_llm(columns: List[str]) -> str:\n",
    "    return \"Columns: [\" + \", \".join([f'\"{col}\"' for col in columns]) + \"]\"\n",
    "\n",
    "def detect_encoding(file_path: str) -> str:\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            rawdata = f.read(50000)\n",
    "        result = chardet.detect(rawdata)\n",
    "        return result['encoding'] if result['confidence'] > 0.7 else 'utf-8'\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Encoding detection error: {e}, using utf-8\")\n",
    "        return 'utf-8'\n",
    "\n",
    "def get_columns_from_csv(file_path: str) -> List[str]:\n",
    "    \"\"\"Extract column names from CSV file\"\"\"\n",
    "    try:\n",
    "        encoding = detect_encoding(file_path)\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            reader = csv.reader(f)\n",
    "            columns = next(reader)\n",
    "        return columns\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading columns from CSV: {e}\")\n",
    "        return []\n",
    "\n",
    "class DeepSeekClassifier:\n",
    "    def __init__(self):\n",
    "        self.domains = self.load_domains()\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=OPENROUTER_API_KEY,\n",
    "            default_headers={\n",
    "                \"HTTP-Referer\": \"https://your-site.com\",\n",
    "                \"X-Title\": \"Dataset Classifier\"\n",
    "            }\n",
    "        )\n",
    "        self.parser = PydanticOutputParser(pydantic_object=ClassificationResult)\n",
    "        logger.info(f\"Initialized with {len(self.domains)} domains\")\n",
    "\n",
    "    def load_domains(self) -> List[str]:\n",
    "        if not os.path.exists(DOMAIN_MAPPING_FILE):\n",
    "            logger.error(f\"Domain file not found: {os.path.abspath(DOMAIN_MAPPING_FILE)}\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            with open(DOMAIN_MAPPING_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                domain_data = json.load(f)\n",
    "                \n",
    "                if isinstance(domain_data, dict):\n",
    "                    domains = list(domain_data.keys())\n",
    "                    logger.info(f\"Loaded {len(domains)} domains\")\n",
    "                    return domains\n",
    "                else:\n",
    "                    logger.error(f\"Unknown domain file format: {type(domain_data)}\")\n",
    "                    return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading domains: {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def create_examples_text(self, examples: List[Tuple[str, dict]]) -> str:\n",
    "        examples_text = \"\"\n",
    "        for i, (columns_repr, classification) in enumerate(examples, 1):\n",
    "            examples_text += (\n",
    "                f\"\\nExample {i}:\\nColumn names:\\n{columns_repr}\\n\"\n",
    "                f\"Classification: {json.dumps(classification)}\\n\"\n",
    "            )\n",
    "        return examples_text\n",
    "\n",
    "    @retry(stop=stop_after_attempt(5), wait=wait_fixed(20))\n",
    "    def classify(self, columns: List[str], mode: str = \"zero-shot\", examples: List[Tuple[str, dict]] = None) -> ClassificationResult:\n",
    "        if not columns or not self.domains:\n",
    "            logger.warning(\"Empty columns or no domains for classification\")\n",
    "            return ClassificationResult(primary_domain=\"unknown\", alternative_domains=[])\n",
    "        \n",
    "        try:\n",
    "            columns_repr = prepare_columns_for_llm(columns)\n",
    "            format_instructions = self.parser.get_format_instructions()\n",
    "            \n",
    "            # print(f\"\\n{'=' * 80}\")\n",
    "            # print(f\"COLUMN NAMES FOR LLM (mode: {mode})\")\n",
    "            # print(f\"{'=' * 80}\")\n",
    "            # print(columns_repr)\n",
    "            # print(f\"{'=' * 80}\")\n",
    "            \n",
    "            if mode == \"zero-shot\":\n",
    "                system_content = (\n",
    "                    \"You are a dataset domain classification expert. \"\n",
    "                    \"Analyze ONLY the column names of a dataset and determine its primary domain. \"\n",
    "                    \"Choose ONLY from the following domains: \" + \", \".join(self.domains) + \"\\n\\n\" +\n",
    "                    format_instructions + \"\\n\\n\" +\n",
    "                    \"Rules:\\n\"\n",
    "                    \"1. primary_domain: SINGLE most appropriate domain (MUST be from list)\\n\"\n",
    "                    \"2. alternative_domains: 0-2 other relevant domains (only if applicable)\\n\"\n",
    "                    \"3. Use EXACT domain names\\n\"\n",
    "                    \"4. No additional text/comments!\"\n",
    "                )\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_content},\n",
    "                    {\"role\": \"user\", \"content\": f\"Column names:\\n{columns_repr}\"}\n",
    "                ]\n",
    "            \n",
    "            elif mode in [\"one-shot\", \"few-shot\"] and examples:\n",
    "                examples_text = self.create_examples_text(examples)\n",
    "                system_content = (\n",
    "                    \"You are a dataset domain classification expert. \"\n",
    "                    \"Analyze column names and determine their primary domain with optional alternatives. \"\n",
    "                    \"Choose ONLY from these domains: \" + \", \".join(self.domains) + \"\\n\\n\" +\n",
    "                    format_instructions + \"\\n\\n\" +\n",
    "                    \"Rules:\\n\"\n",
    "                    \"1. primary_domain: SINGLE most appropriate domain (MUST be from list)\\n\"\n",
    "                    \"2. alternative_domains: 0-2 other relevant domains (only if applicable)\\n\"\n",
    "                    \"3. Use EXACT domain names\\n\"\n",
    "                    \"4. No additional text/comments!\\n\\n\" +\n",
    "                    \"Examples:\" + examples_text\n",
    "                )\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_content},\n",
    "                    {\"role\": \"user\", \"content\": f\"Classify this new dataset based on its column names:\\n{columns_repr}\"}\n",
    "                ]\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mode or missing examples: {mode}\")\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=messages,\n",
    "                temperature=0.3,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            \n",
    "            content = response.choices[0].message.content\n",
    "            \n",
    "            if \"```json\" in content:\n",
    "                try:\n",
    "                    json_content = content.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "                    result = self.parser.parse(json_content)\n",
    "                except Exception:\n",
    "                    result = self.parser.parse(content)\n",
    "            else:\n",
    "                result = self.parser.parse(content)\n",
    "                \n",
    "            logger.debug(f\"Classification result ({mode}): {result}\")\n",
    "            return result\n",
    "                \n",
    "        except ValidationError as e:\n",
    "            logger.error(f\"Validation error: {e}\\nResponse content: {content}\")\n",
    "            return ClassificationResult(primary_domain=\"unknown\", alternative_domains=[])\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification error ({mode}): {e}\", exc_info=True)\n",
    "            return ClassificationResult(primary_domain=\"unknown\", alternative_domains=[])\n",
    "\n",
    "def load_all_datasets() -> Dict[str, Dict]:\n",
    "    datasets = {}\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        logger.error(f\"Dataset directory not found: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "        return datasets\n",
    "\n",
    "    dataset_ids = [d for d in os.listdir(OUTPUT_DIR) \n",
    "                   if os.path.isdir(os.path.join(OUTPUT_DIR, d))]\n",
    "    \n",
    "    logger.info(f\"Loading datasets from {len(dataset_ids)} directories...\")\n",
    "    \n",
    "    for dataset_id in tqdm(dataset_ids, desc=\"Loading datasets\"):\n",
    "        try:\n",
    "            meta_path = os.path.join(OUTPUT_DIR, dataset_id, \"metadata.json\")\n",
    "            csv_path = os.path.join(OUTPUT_DIR, dataset_id, \"random_rows.csv\")\n",
    "\n",
    "            if not os.path.exists(meta_path):\n",
    "                logger.warning(f\"Skipping {dataset_id}: metadata.json not found\")\n",
    "                continue\n",
    "                \n",
    "            if not os.path.exists(csv_path):\n",
    "                logger.warning(f\"Skipping {dataset_id}: random_rows.csv not found\")\n",
    "                continue\n",
    "\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            if \"domain\" not in metadata:\n",
    "                logger.warning(f\"Skipping {dataset_id}: 'domain' field missing in metadata\")\n",
    "                continue\n",
    "                \n",
    "            columns = get_columns_from_csv(csv_path)\n",
    "            if not columns:\n",
    "                logger.warning(f\"Skipping {dataset_id}: no columns found in CSV\")\n",
    "                continue\n",
    "\n",
    "            datasets[dataset_id] = {\n",
    "                'metadata': metadata,\n",
    "                'columns': columns,\n",
    "                'domain': metadata[\"domain\"].lower(),\n",
    "                'columns_repr': prepare_columns_for_llm(columns)\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {dataset_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Successfully loaded {len(datasets)} datasets\")\n",
    "    return datasets\n",
    "\n",
    "def run_zero_shot_classification(datasets: Dict[str, Dict], run_id: int) -> Tuple[Optional[float], List[str], List[str], Optional[Dict], Optional[float]]:\n",
    "    logger.info(f\"=== Starting zero-shot classification run {run_id} ===\")\n",
    "    classifier = DeepSeekClassifier()\n",
    "\n",
    "    if not classifier.domains or not datasets:\n",
    "        logger.error(\"Classification domains or datasets not loaded\")\n",
    "        return None, [], [], {}, None\n",
    "\n",
    "    test_size = len(datasets) // K_FOLDS\n",
    "    dataset_items = list(datasets.items())\n",
    "    random.shuffle(dataset_items)\n",
    "    test_datasets = dict(dataset_items[:test_size])\n",
    "    \n",
    "    matches = 0\n",
    "    top3_matches = 0\n",
    "    total = 0\n",
    "    actual_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    pbar = tqdm(test_datasets.items(), desc=f\"Zero-shot classification (Run {run_id})\", unit=\"dataset\")\n",
    "\n",
    "    for dataset_id, data_info in pbar:\n",
    "        try:\n",
    "            result = classifier.classify(data_info['columns'], mode=\"zero-shot\")\n",
    "            actual = data_info['domain']\n",
    "            predicted_primary = result.primary_domain.lower()\n",
    "            \n",
    "            all_predictions = [predicted_primary] + [alt.lower() for alt in result.alternative_domains]\n",
    "            top3 = all_predictions[:3]\n",
    "\n",
    "            if predicted_primary == actual:\n",
    "                matches += 1\n",
    "            if actual in top3:\n",
    "                top3_matches += 1\n",
    "                \n",
    "            total += 1\n",
    "            actual_labels.append(actual)\n",
    "            predicted_labels.append(predicted_primary)\n",
    "                \n",
    "            pbar.set_postfix({\n",
    "                \"Accuracy\": f\"{matches/total:.2%}\" if total > 0 else \"N/A\",\n",
    "                \"Top-3 Recall\": f\"{top3_matches/total:.2%}\" if total > 0 else \"N/A\"\n",
    "            })\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if total > 0:\n",
    "        accuracy = matches / total\n",
    "        top3_recall = top3_matches / total\n",
    "        logger.info(f\"Zero-shot accuracy: {accuracy:.2%}\")\n",
    "        logger.info(f\"Zero-shot top-3 recall: {top3_recall:.2%}\")\n",
    "        \n",
    "        report = classification_report(actual_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "        return accuracy, actual_labels, predicted_labels, report, top3_recall\n",
    "    else:\n",
    "        logger.warning(\"No datasets processed for accuracy calculation\")\n",
    "        return None, [], [], {}, None\n",
    "\n",
    "def run_kfold_classification(datasets: Dict[str, Dict], mode: str, run_id: int) -> Tuple[List[float], List[float], List[Dict]]:\n",
    "    logger.info(f\"=== Starting {mode} classification with K-fold validation (Run {run_id}) ===\")\n",
    "    classifier = DeepSeekClassifier()\n",
    "\n",
    "    if not classifier.domains or not datasets:\n",
    "        logger.error(\"Classification domains or datasets not loaded\")\n",
    "        return [], [], []\n",
    "\n",
    "    dataset_items = list(datasets.items())\n",
    "    kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42 + run_id)\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    fold_top3_recalls = []\n",
    "    fold_reports = []\n",
    "\n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(kf.split(dataset_items)):\n",
    "        logger.info(f\"Processing fold {fold_idx + 1}/{K_FOLDS}\")\n",
    "        \n",
    "        train_datasets = [dataset_items[i] for i in train_indices]\n",
    "        test_datasets = [dataset_items[i] for i in test_indices]\n",
    "        \n",
    "        examples = []\n",
    "        if mode == \"one-shot\":\n",
    "            domain_examples = {}\n",
    "            for dataset_id, data_info in train_datasets:\n",
    "                domain = data_info['domain']\n",
    "                if domain not in domain_examples:\n",
    "                    classification = {\n",
    "                        \"primary_domain\": domain,\n",
    "                        \"alternative_domains\": []\n",
    "                    }\n",
    "                    domain_examples[domain] = (data_info['columns_repr'], classification)\n",
    "            examples = list(domain_examples.values())\n",
    "        \n",
    "        elif mode == \"few-shot\":\n",
    "            domain_examples = {}\n",
    "            for dataset_id, data_info in train_datasets:\n",
    "                domain = data_info['domain']\n",
    "                if domain not in domain_examples:\n",
    "                    domain_examples[domain] = []\n",
    "                \n",
    "                if len(domain_examples[domain]) < FEW_SHOT_SIZE:\n",
    "                    classification = {\n",
    "                        \"primary_domain\": domain,\n",
    "                        \"alternative_domains\": []\n",
    "                    }\n",
    "                    domain_examples[domain].append((data_info['columns_repr'], classification))\n",
    "            \n",
    "            for domain_list in domain_examples.values():\n",
    "                examples.extend(domain_list)\n",
    "        \n",
    "        matches = 0\n",
    "        top3_matches = 0\n",
    "        total = 0\n",
    "        actual_labels = []\n",
    "        predicted_labels = []\n",
    "        \n",
    "        pbar = tqdm(test_datasets, desc=f\"{mode.capitalize()} fold {fold_idx + 1}\", unit=\"dataset\")\n",
    "        \n",
    "        for dataset_id, data_info in pbar:\n",
    "            try:\n",
    "                result = classifier.classify(\n",
    "                    data_info['columns'], \n",
    "                    mode=mode, \n",
    "                    examples=examples\n",
    "                )\n",
    "                actual = data_info['domain']\n",
    "                predicted_primary = result.primary_domain.lower()\n",
    "                \n",
    "                all_predictions = [predicted_primary] + [alt.lower() for alt in result.alternative_domains]\n",
    "                top3 = all_predictions[:3]\n",
    "\n",
    "                if predicted_primary == actual:\n",
    "                    matches += 1\n",
    "                if actual in top3:\n",
    "                    top3_matches += 1\n",
    "                    \n",
    "                total += 1\n",
    "                actual_labels.append(actual)\n",
    "                predicted_labels.append(predicted_primary)\n",
    "                    \n",
    "                pbar.set_postfix({\n",
    "                    \"Accuracy\": f\"{matches/total:.2%}\" if total > 0 else \"N/A\",\n",
    "                    \"Top-3 Recall\": f\"{top3_matches/total:.2%}\" if total > 0 else \"N/A\"\n",
    "                })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if total > 0:\n",
    "            fold_accuracy = matches / total\n",
    "            fold_top3_recall = top3_matches / total\n",
    "            fold_report = classification_report(actual_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "            \n",
    "            fold_accuracies.append(fold_accuracy)\n",
    "            fold_top3_recalls.append(fold_top3_recall)\n",
    "            fold_reports.append(fold_report)\n",
    "            \n",
    "            logger.info(f\"Fold {fold_idx + 1} - Accuracy: {fold_accuracy:.2%}, Top-3 Recall: {fold_top3_recall:.2%}\")\n",
    "\n",
    "    return fold_accuracies, fold_top3_recalls, fold_reports\n",
    "\n",
    "def calculate_aggregated_metrics(fold_reports: List[Dict]) -> Dict[str, float]:\n",
    "    if not fold_reports:\n",
    "        return {}\n",
    "    \n",
    "    macro_precisions = [report['macro avg']['precision'] for report in fold_reports]\n",
    "    macro_recalls = [report['macro avg']['recall'] for report in fold_reports]\n",
    "    macro_f1s = [report['macro avg']['f1-score'] for report in fold_reports]\n",
    "    weighted_f1s = [report['weighted avg']['f1-score'] for report in fold_reports]\n",
    "    \n",
    "    return {\n",
    "        'macro_precision_mean': np.mean(macro_precisions),\n",
    "        'macro_precision_std': np.std(macro_precisions),\n",
    "        'macro_recall_mean': np.mean(macro_recalls),\n",
    "        'macro_recall_std': np.std(macro_recalls),\n",
    "        'macro_f1_mean': np.mean(macro_f1s),\n",
    "        'macro_f1_std': np.std(macro_f1s),\n",
    "        'weighted_f1_mean': np.mean(weighted_f1s),\n",
    "        'weighted_f1_std': np.std(weighted_f1s)\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    logger.info(\"Loading all datasets metadata and column names...\")\n",
    "    datasets = load_all_datasets()\n",
    "    \n",
    "    if not datasets:\n",
    "        logger.error(\"No datasets loaded, exiting\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Loaded {len(datasets)} datasets\")\n",
    "    \n",
    "    results = {\n",
    "        'zero-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        },\n",
    "        'one-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        },\n",
    "        'few-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"STARTING CLASSIFICATION EXPERIMENT\")\n",
    "    print(f\"Model: {MODEL_NAME}\")\n",
    "    print(f\"Modes: Zero-shot, One-shot, Few-shot\")\n",
    "    print(f\"Runs per mode: {N_RUNS}\")\n",
    "    print(f\"K-Folds for shot learning: {K_FOLDS}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ZERO-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        accuracy, actual_labels, predicted_labels, report, top3_recall = run_zero_shot_classification(datasets, run_id)\n",
    "        \n",
    "        if accuracy is not None:\n",
    "            results['zero-shot']['accuracies'].append(accuracy)\n",
    "            results['zero-shot']['top3_recalls'].append(top3_recall)\n",
    "            results['zero-shot']['macro_precisions'].append(report['macro avg']['precision'])\n",
    "            results['zero-shot']['macro_recalls'].append(report['macro avg']['recall'])\n",
    "            results['zero-shot']['macro_f1s'].append(report['macro avg']['f1-score'])\n",
    "            results['zero-shot']['weighted_f1s'].append(report['weighted avg']['f1-score'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={accuracy:.4f}, Top-3 Recall={top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ONE-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"one-shot\", run_id)\n",
    "        \n",
    "        if fold_accuracies:\n",
    "            run_accuracy = np.mean(fold_accuracies)\n",
    "            run_top3_recall = np.mean(fold_top3_recalls)\n",
    "            run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "            results['one-shot']['accuracies'].append(run_accuracy)\n",
    "            results['one-shot']['top3_recalls'].append(run_top3_recall)\n",
    "            results['one-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "            results['one-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "            results['one-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "            results['one-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"FEW-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"few-shot\", run_id)\n",
    "        \n",
    "        if fold_accuracies:\n",
    "            run_accuracy = np.mean(fold_accuracies)\n",
    "            run_top3_recall = np.mean(fold_top3_recalls)\n",
    "            run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "            results['few-shot']['accuracies'].append(run_accuracy)\n",
    "            results['few-shot']['top3_recalls'].append(run_top3_recall)\n",
    "            results['few-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "            results['few-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "            results['few-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "            results['few-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"FINAL AGGREGATED RESULTS\".center(80))\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    final_results = {}\n",
    "    \n",
    "    for mode in ['zero-shot', 'one-shot', 'few-shot']:\n",
    "        mode_results = results[mode]\n",
    "        \n",
    "        if mode_results['accuracies']:\n",
    "            final_results[mode] = {\n",
    "                'accuracy': {\n",
    "                    'mean': np.mean(mode_results['accuracies']),\n",
    "                    'std': np.std(mode_results['accuracies'])\n",
    "                },\n",
    "                'top3_recall': {\n",
    "                    'mean': np.mean(mode_results['top3_recalls']),\n",
    "                    'std': np.std(mode_results['top3_recalls'])\n",
    "                },\n",
    "                'macro_precision': {\n",
    "                    'mean': np.mean(mode_results['macro_precisions']),\n",
    "                    'std': np.std(mode_results['macro_precisions'])\n",
    "                },\n",
    "                'macro_recall': {\n",
    "                    'mean': np.mean(mode_results['macro_recalls']),\n",
    "                    'std': np.std(mode_results['macro_recalls'])\n",
    "                },\n",
    "                'macro_f1': {\n",
    "                    'mean': np.mean(mode_results['macro_f1s']),\n",
    "                    'std': np.std(mode_results['macro_f1s'])\n",
    "                },\n",
    "                'weighted_f1': {\n",
    "                    'mean': np.mean(mode_results['weighted_f1s']),\n",
    "                    'std': np.std(mode_results['weighted_f1s'])\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{mode.upper().replace('-', ' ')} RESULTS:\")\n",
    "            print(f\"Accuracy: {final_results[mode]['accuracy']['mean']:.4f} ± {final_results[mode]['accuracy']['std']:.4f}\")\n",
    "            print(f\"Top-3 Recall: {final_results[mode]['top3_recall']['mean']:.4f} ± {final_results[mode]['top3_recall']['std']:.4f}\")\n",
    "            print(f\"Macro Precision: {final_results[mode]['macro_precision']['mean']:.4f} ± {final_results[mode]['macro_precision']['std']:.4f}\")\n",
    "            print(f\"Macro Recall: {final_results[mode]['macro_recall']['mean']:.4f} ± {final_results[mode]['macro_recall']['std']:.4f}\")\n",
    "            print(f\"Macro F1: {final_results[mode]['macro_f1']['mean']:.4f} ± {final_results[mode]['macro_f1']['std']:.4f}\")\n",
    "            print(f\"Weighted F1: {final_results[mode]['weighted_f1']['mean']:.4f} ± {final_results[mode]['weighted_f1']['std']:.4f}\")\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, \"final_all_modes_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Results saved to final_all_modes_results.json\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83326a65-f3c3-48c3-bcab-7024c1df4af2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Ablative Experiment 1: 5 diverse rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4910b-4ea9-4316-a388-3ffd250c4f52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## gpt-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a49c4d-735a-4bf2-9c63-c8a9356c5916",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"all_datasets\"\n",
    "RESULTS_DIR = \"classification_results\"\n",
    "DOMAIN_MAPPING_FILE = \"clustering_domain_mapping.json\"\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")  \n",
    "TIMEOUT = 180\n",
    "N_RUNS = 5\n",
    "K_FOLDS = 5\n",
    "FEW_SHOT_SIZE = 5\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('llm_classification.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ClassificationResult(BaseModel):\n",
    "    primary_domain: str = Field(..., description=\"The most appropriate domain for the dataset\")\n",
    "    alternative_domains: List[str] = Field(\n",
    "        default_factory=list, \n",
    "        description=\"List of alternative relevant domains (up to 2)\",\n",
    "        max_items=2\n",
    "    )\n",
    "\n",
    "def detect_encoding(file_path: str) -> str:\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            rawdata = f.read(50000)\n",
    "        result = chardet.detect(rawdata)\n",
    "        return result['encoding'] if result['confidence'] > 0.7 else 'utf-8'\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Encoding detection error: {e}, using utf-8\")\n",
    "        return 'utf-8'\n",
    "\n",
    "def read_csv_with_memory_limit(file_path: str, dataset_id: str = None) -> pd.DataFrame:\n",
    "    try:\n",
    "        if dataset_id:\n",
    "            random_path = os.path.join(OUTPUT_DIR, dataset_id, \"diverse_rows.csv\")\n",
    "            if os.path.exists(random_path):\n",
    "                encoding = detect_encoding(random_path)\n",
    "                df = pd.read_csv(random_path, encoding=encoding)\n",
    "                logger.debug(f\"Read diverse rows for {dataset_id} ({len(df)} rows, {len(df.columns)} columns)\")\n",
    "                return df\n",
    "\n",
    "        encoding = detect_encoding(file_path)\n",
    "        logger.debug(f\"Detected encoding: {encoding} for {os.path.basename(file_path)}\")\n",
    "        \n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            preview = pd.read_csv(f, nrows=1000)\n",
    "        \n",
    "        dtype_dict = preview.dtypes.to_dict()\n",
    "        \n",
    "        chunks = []\n",
    "        chunk_size = 10000\n",
    "        \n",
    "        for chunk in pd.read_csv(\n",
    "            file_path, \n",
    "            encoding=encoding,\n",
    "            chunksize=chunk_size,\n",
    "            dtype=dtype_dict\n",
    "        ):\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        if chunks:\n",
    "            df = pd.concat(chunks, ignore_index=True)\n",
    "            logger.debug(f\"Read dataset with {len(df)} rows and {len(df.columns)} columns\")\n",
    "            return df\n",
    "        else:\n",
    "            logger.warning(\"No data read, returning empty DataFrame\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading CSV: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def prepare_for_llm(df: pd.DataFrame, max_rows: int = 5) -> str:\n",
    "    df_subset = df.head(max_rows).copy()\n",
    "    df_subset = df_subset.fillna('missing')\n",
    "    \n",
    "    columns = df_subset.columns.tolist()\n",
    "    data = []\n",
    "    \n",
    "    for _, row in df_subset.iterrows():\n",
    "        formatted_row = []\n",
    "        for value in row:\n",
    "            if isinstance(value, str):\n",
    "                val = value.replace('\"', '\\\\\"')\n",
    "                formatted_value = f'\"{val}\"'\n",
    "            else:\n",
    "                formatted_value = str(value)\n",
    "            formatted_row.append(formatted_value)\n",
    "        data.append(f\"[{', '.join(formatted_row)}]\")\n",
    "    \n",
    "    columns_str = \"[\" + \", \".join([f'\"{col}\"' for col in columns]) + \"]\"\n",
    "    data_str = \",\\n\".join(data)\n",
    "    \n",
    "    return f\"columns = {columns_str}\\ndata = [\\n{data_str}\\n]\"\n",
    "\n",
    "class GPT4oClassifier:\n",
    "    def __init__(self):\n",
    "        self.domains = self.load_domains()\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=OPENROUTER_API_KEY,\n",
    "            default_headers={\n",
    "                \"HTTP-Referer\": \"https://your-site.com\",  \n",
    "                \"X-Title\": \"Dataset Classifier\"  \n",
    "            }\n",
    "        )\n",
    "        self.parser = PydanticOutputParser(pydantic_object=ClassificationResult)\n",
    "        logger.info(f\"Initialized with {len(self.domains)} domains\")\n",
    "\n",
    "    def load_domains(self) -> List[str]:\n",
    "        if not os.path.exists(DOMAIN_MAPPING_FILE):\n",
    "            logger.error(f\"Domain file not found: {os.path.abspath(DOMAIN_MAPPING_FILE)}\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            with open(DOMAIN_MAPPING_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                domain_data = json.load(f)\n",
    "                \n",
    "                if isinstance(domain_data, dict):\n",
    "                    domains = list(domain_data.keys())\n",
    "                    logger.info(f\"Loaded {len(domains)} domains\")\n",
    "                    return domains\n",
    "                else:\n",
    "                    logger.error(f\"Unknown domain file format: {type(domain_data)}\")\n",
    "                    return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading domains: {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def create_examples_text(self, examples: List[Tuple[str, dict]]) -> str:\n",
    "        examples_text = \"\"\n",
    "        for i, (data_sample, classification) in enumerate(examples, 1):\n",
    "            examples_text += (\n",
    "                f\"\\nExample {i}:\\nDataset sample:\\n{data_sample}\\n\"\n",
    "                f\"Classification: {json.dumps(classification)}\\n\"\n",
    "            )\n",
    "        return examples_text\n",
    "\n",
    "    @retry(stop=stop_after_attempt(5), wait=wait_fixed(20))\n",
    "    def classify(self, df: pd.DataFrame, mode: str = \"zero-shot\", examples: List[Tuple[str, dict]] = None) -> ClassificationResult:\n",
    "        if df.empty or not self.domains:\n",
    "            logger.warning(\"Empty DataFrame or no domains for classification\")\n",
    "            return ClassificationResult(primary_domain=\"unknown\", alternative_domains=[])\n",
    "        \n",
    "        try:\n",
    "            data_sample = prepare_for_llm(df)\n",
    "            format_instructions = self.parser.get_format_instructions()\n",
    "            \n",
    "            if mode == \"zero-shot\":\n",
    "                system_content = (\n",
    "                    \"You are a dataset domain classification expert. \"\n",
    "                    \"Analyze the dataset sample provided as a DataFrame-like structure \"\n",
    "                    \"and determine its primary domain with optional alternatives. \"\n",
    "                    \"Choose ONLY from the following domains: \" + \", \".join(self.domains) + \"\\n\\n\" +\n",
    "                    format_instructions + \"\\n\\n\" +\n",
    "                    \"Rules:\\n\"\n",
    "                    \"1. primary_domain: SINGLE most appropriate domain (MUST be from list)\\n\"\n",
    "                    \"2. alternative_domains: 0-2 other relevant domains (only if applicable)\\n\"\n",
    "                    \"3. Use EXACT domain names\\n\"\n",
    "                    \"4. No additional text/comments!\"\n",
    "                )\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_content},\n",
    "                    {\"role\": \"user\", \"content\": f\"Dataset sample:\\n{data_sample}\"}\n",
    "                ]\n",
    "            \n",
    "            elif mode in [\"one-shot\", \"few-shot\"] and examples:\n",
    "                examples_text = self.create_examples_text(examples)\n",
    "                system_content = (\n",
    "                    \"You are a dataset domain classification expert. \"\n",
    "                    \"Analyze dataset samples and determine their primary domain with optional alternatives. \"\n",
    "                    \"Choose ONLY from these domains: \" + \", \".join(self.domains) + \"\\n\\n\" +\n",
    "                    format_instructions + \"\\n\\n\" +\n",
    "                    \"Rules:\\n\"\n",
    "                    \"1. primary_domain: SINGLE most appropriate domain (MUST be from list)\\n\"\n",
    "                    \"2. alternative_domains: 0-2 other relevant domains (only if applicable)\\n\"\n",
    "                    \"3. Use EXACT domain names\\n\"\n",
    "                    \"4. No additional text/comments!\\n\\n\" +\n",
    "                    \"Examples:\" + examples_text\n",
    "                )\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_content},\n",
    "                    {\"role\": \"user\", \"content\": f\"Classify this new dataset:\\n{data_sample}\"}\n",
    "                ]\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mode or missing examples: {mode}\")\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"openai/gpt-4o\",\n",
    "                messages=messages,\n",
    "                temperature=0.3,\n",
    "                max_tokens=500,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            content = response.choices[0].message.content\n",
    "            try:\n",
    "                result = self.parser.parse(content)\n",
    "                logger.debug(f\"Classification result ({mode}): {result}\")\n",
    "                return result\n",
    "            except ValidationError as e:\n",
    "                logger.error(f\"Validation error: {e}\\nResponse content: {content}\")\n",
    "                return ClassificationResult(primary_domain=\"unknown\", alternative_domains=[])\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification error ({mode}): {e}\", exc_info=True)\n",
    "            return ClassificationResult(primary_domain=\"unknown\", alternative_domains=[])\n",
    "\n",
    "def load_all_datasets() -> Dict[str, Dict]:\n",
    "    datasets = {}\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        logger.error(f\"Dataset directory not found: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "        return datasets\n",
    "\n",
    "    dataset_ids = [d for d in os.listdir(OUTPUT_DIR) \n",
    "                   if os.path.isdir(os.path.join(OUTPUT_DIR, d))]\n",
    "    \n",
    "    valid_datasets = [\n",
    "        d for d in dataset_ids\n",
    "        if os.path.exists(os.path.join(OUTPUT_DIR, d, \"metadata.json\"))\n",
    "        and os.path.exists(os.path.join(OUTPUT_DIR, d, \"diverse_rows.csv\"))\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Loading {len(valid_datasets)} valid datasets...\")\n",
    "    \n",
    "    for dataset_id in tqdm(valid_datasets, desc=\"Loading datasets\"):\n",
    "        try:\n",
    "            meta_path = os.path.join(OUTPUT_DIR, dataset_id, \"metadata.json\")\n",
    "            data_path = os.path.join(OUTPUT_DIR, dataset_id, \"diverse_rows.csv\")\n",
    "\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            if \"domain\" not in metadata:\n",
    "                logger.warning(f\"Skipping {dataset_id}: 'domain' field missing in metadata\")\n",
    "                continue\n",
    "\n",
    "            df = read_csv_with_memory_limit(data_path, dataset_id=dataset_id)\n",
    "            if df.empty:\n",
    "                logger.warning(f\"Empty DataFrame for {dataset_id}, skipping\")\n",
    "                continue\n",
    "\n",
    "            datasets[dataset_id] = {\n",
    "                'metadata': metadata,\n",
    "                'dataframe': df,\n",
    "                'domain': metadata[\"domain\"].lower(),\n",
    "                'data_sample': prepare_for_llm(df)\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {dataset_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Successfully loaded {len(datasets)} datasets\")\n",
    "    return datasets\n",
    "\n",
    "def run_zero_shot_classification(datasets: Dict[str, Dict], run_id: int) -> Tuple[Optional[float], List[str], List[str], Optional[Dict], Optional[float]]:\n",
    "    logger.info(f\"=== Starting zero-shot classification run {run_id} ===\")\n",
    "    classifier = GPT4oClassifier()\n",
    "\n",
    "    if not classifier.domains or not datasets:\n",
    "        logger.error(\"Classification domains or datasets not loaded\")\n",
    "        return None, [], [], {}, None\n",
    "\n",
    "    test_size = len(datasets) // K_FOLDS\n",
    "    dataset_items = list(datasets.items())\n",
    "    random.shuffle(dataset_items)\n",
    "    test_datasets = dict(dataset_items[:test_size])\n",
    "    \n",
    "    matches = 0\n",
    "    top3_matches = 0\n",
    "    total = 0\n",
    "    actual_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    pbar = tqdm(test_datasets.items(), desc=f\"Zero-shot classification (Run {run_id})\", unit=\"dataset\")\n",
    "\n",
    "    for dataset_id, data_info in pbar:\n",
    "        try:\n",
    "            result = classifier.classify(data_info['dataframe'], mode=\"zero-shot\")\n",
    "            actual = data_info['domain']\n",
    "            predicted_primary = result.primary_domain.lower()\n",
    "            \n",
    "            all_predictions = [predicted_primary] + [alt.lower() for alt in result.alternative_domains]\n",
    "            top3 = all_predictions[:3]\n",
    "\n",
    "            if predicted_primary == actual:\n",
    "                matches += 1\n",
    "            if actual in top3:\n",
    "                top3_matches += 1\n",
    "                \n",
    "            total += 1\n",
    "            actual_labels.append(actual)\n",
    "            predicted_labels.append(predicted_primary)\n",
    "                \n",
    "            pbar.set_postfix({\n",
    "                \"Accuracy\": f\"{matches/total:.2%}\" if total > 0 else \"N/A\",\n",
    "                \"Top-3 Recall\": f\"{top3_matches/total:.2%}\" if total > 0 else \"N/A\"\n",
    "            })\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if total > 0:\n",
    "        accuracy = matches / total\n",
    "        top3_recall = top3_matches / total\n",
    "        logger.info(f\"Zero-shot accuracy: {accuracy:.2%}\")\n",
    "        logger.info(f\"Zero-shot top-3 recall: {top3_recall:.2%}\")\n",
    "        \n",
    "        report = classification_report(actual_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "        return accuracy, actual_labels, predicted_labels, report, top3_recall\n",
    "    else:\n",
    "        logger.warning(\"No datasets processed for accuracy calculation\")\n",
    "        return None, [], [], {}, None\n",
    "\n",
    "def run_kfold_classification(datasets: Dict[str, Dict], mode: str, run_id: int) -> Tuple[List[float], List[float], List[Dict]]:\n",
    "    logger.info(f\"=== Starting {mode} classification with K-fold validation (Run {run_id}) ===\")\n",
    "    classifier = GPT4oClassifier()\n",
    "\n",
    "    if not classifier.domains or not datasets:\n",
    "        logger.error(\"Classification domains or datasets not loaded\")\n",
    "        return [], [], []\n",
    "\n",
    "    dataset_items = list(datasets.items())\n",
    "    kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42 + run_id)\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    fold_top3_recalls = []\n",
    "    fold_reports = []\n",
    "\n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(kf.split(dataset_items)):\n",
    "        logger.info(f\"Processing fold {fold_idx + 1}/{K_FOLDS}\")\n",
    "        \n",
    "        train_datasets = [dataset_items[i] for i in train_indices]\n",
    "        test_datasets = [dataset_items[i] for i in test_indices]\n",
    "        \n",
    "        examples = []\n",
    "        if mode == \"one-shot\":\n",
    "            domain_examples = {}\n",
    "            for dataset_id, data_info in train_datasets:\n",
    "                domain = data_info['domain']\n",
    "                if domain not in domain_examples:\n",
    "                    classification = {\n",
    "                        \"primary_domain\": domain,\n",
    "                        \"alternative_domains\": []\n",
    "                    }\n",
    "                    domain_examples[domain] = (data_info['data_sample'], classification)\n",
    "            examples = list(domain_examples.values())\n",
    "        \n",
    "        elif mode == \"few-shot\":\n",
    "            domain_examples = {}\n",
    "            for dataset_id, data_info in train_datasets:\n",
    "                domain = data_info['domain']\n",
    "                if domain not in domain_examples:\n",
    "                    domain_examples[domain] = []\n",
    "                \n",
    "                if len(domain_examples[domain]) < FEW_SHOT_SIZE:\n",
    "                    classification = {\n",
    "                        \"primary_domain\": domain,\n",
    "                        \"alternative_domains\": []\n",
    "                    }\n",
    "                    domain_examples[domain].append((data_info['data_sample'], classification))\n",
    "            \n",
    "            for domain_list in domain_examples.values():\n",
    "                examples.extend(domain_list)\n",
    "        \n",
    "        matches = 0\n",
    "        top3_matches = 0\n",
    "        total = 0\n",
    "        actual_labels = []\n",
    "        predicted_labels = []\n",
    "        \n",
    "        pbar = tqdm(test_datasets, desc=f\"{mode.capitalize()} fold {fold_idx + 1}\", unit=\"dataset\")\n",
    "        \n",
    "        for dataset_id, data_info in pbar:\n",
    "            try:\n",
    "                result = classifier.classify(\n",
    "                    data_info['dataframe'], \n",
    "                    mode=mode, \n",
    "                    examples=examples\n",
    "                )\n",
    "                actual = data_info['domain']\n",
    "                predicted_primary = result.primary_domain.lower()\n",
    "                \n",
    "                all_predictions = [predicted_primary] + [alt.lower() for alt in result.alternative_domains]\n",
    "                top3 = all_predictions[:3]\n",
    "\n",
    "                if predicted_primary == actual:\n",
    "                    matches += 1\n",
    "                if actual in top3:\n",
    "                    top3_matches += 1\n",
    "                    \n",
    "                total += 1\n",
    "                actual_labels.append(actual)\n",
    "                predicted_labels.append(predicted_primary)\n",
    "                    \n",
    "                pbar.set_postfix({\n",
    "                    \"Accuracy\": f\"{matches/total:.2%}\" if total > 0 else \"N/A\",\n",
    "                    \"Top-3 Recall\": f\"{top3_matches/total:.2%}\" if total > 0 else \"N/A\"\n",
    "                })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if total > 0:\n",
    "            fold_accuracy = matches / total\n",
    "            fold_top3_recall = top3_matches / total\n",
    "            fold_report = classification_report(actual_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "            \n",
    "            fold_accuracies.append(fold_accuracy)\n",
    "            fold_top3_recalls.append(fold_top3_recall)\n",
    "            fold_reports.append(fold_report)\n",
    "            \n",
    "            logger.info(f\"Fold {fold_idx + 1} - Accuracy: {fold_accuracy:.2%}, Top-3 Recall: {fold_top3_recall:.2%}\")\n",
    "\n",
    "    return fold_accuracies, fold_top3_recalls, fold_reports\n",
    "\n",
    "def calculate_aggregated_metrics(fold_reports: List[Dict]) -> Dict[str, float]:\n",
    "    if not fold_reports:\n",
    "        return {}\n",
    "    \n",
    "    macro_precisions = [report['macro avg']['precision'] for report in fold_reports]\n",
    "    macro_recalls = [report['macro avg']['recall'] for report in fold_reports]\n",
    "    macro_f1s = [report['macro avg']['f1-score'] for report in fold_reports]\n",
    "    weighted_f1s = [report['weighted avg']['f1-score'] for report in fold_reports]\n",
    "    \n",
    "    return {\n",
    "        'macro_precision_mean': np.mean(macro_precisions),\n",
    "        'macro_precision_std': np.std(macro_precisions),\n",
    "        'macro_recall_mean': np.mean(macro_recalls),\n",
    "        'macro_recall_std': np.std(macro_recalls),\n",
    "        'macro_f1_mean': np.mean(macro_f1s),\n",
    "        'macro_f1_std': np.std(macro_f1s),\n",
    "        'weighted_f1_mean': np.mean(weighted_f1s),\n",
    "        'weighted_f1_std': np.std(weighted_f1s)\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    logger.info(\"Loading all datasets...\")\n",
    "    datasets = load_all_datasets()\n",
    "    \n",
    "    if not datasets:\n",
    "        logger.error(\"No datasets loaded, exiting\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Loaded {len(datasets)} datasets\")\n",
    "    \n",
    "    results = {\n",
    "        'zero-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        },\n",
    "        'one-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        },\n",
    "        'few-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"STARTING CLASSIFICATION EXPERIMENT\")\n",
    "    print(f\"Modes: Zero-shot, One-shot, Few-shot\")\n",
    "    print(f\"Runs per mode: {N_RUNS}\")\n",
    "    print(f\"K-Folds for shot learning: {K_FOLDS}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ZERO-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        accuracy, actual_labels, predicted_labels, report, top3_recall = run_zero_shot_classification(datasets, run_id)\n",
    "        \n",
    "        if accuracy is not None:\n",
    "            results['zero-shot']['accuracies'].append(accuracy)\n",
    "            results['zero-shot']['top3_recalls'].append(top3_recall)\n",
    "            results['zero-shot']['macro_precisions'].append(report['macro avg']['precision'])\n",
    "            results['zero-shot']['macro_recalls'].append(report['macro avg']['recall'])\n",
    "            results['zero-shot']['macro_f1s'].append(report['macro avg']['f1-score'])\n",
    "            results['zero-shot']['weighted_f1s'].append(report['weighted avg']['f1-score'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={accuracy:.4f}, Top-3 Recall={top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ONE-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"one-shot\", run_id)\n",
    "        \n",
    "        if fold_accuracies:\n",
    "            run_accuracy = np.mean(fold_accuracies)\n",
    "            run_top3_recall = np.mean(fold_top3_recalls)\n",
    "            run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "            results['one-shot']['accuracies'].append(run_accuracy)\n",
    "            results['one-shot']['top3_recalls'].append(run_top3_recall)\n",
    "            results['one-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "            results['one-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "            results['one-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "            results['one-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"FEW-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"few-shot\", run_id)\n",
    "        \n",
    "        if fold_accuracies:\n",
    "            run_accuracy = np.mean(fold_accuracies)\n",
    "            run_top3_recall = np.mean(fold_top3_recalls)\n",
    "            run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "            results['few-shot']['accuracies'].append(run_accuracy)\n",
    "            results['few-shot']['top3_recalls'].append(run_top3_recall)\n",
    "            results['few-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "            results['few-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "            results['few-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "            results['few-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"FINAL AGGREGATED RESULTS\".center(80))\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    final_results = {}\n",
    "    \n",
    "    for mode in ['zero-shot', 'one-shot', 'few-shot']:\n",
    "        mode_results = results[mode]\n",
    "        \n",
    "        if mode_results['accuracies']:\n",
    "            final_results[mode] = {\n",
    "                'accuracy': {\n",
    "                    'mean': np.mean(mode_results['accuracies']),\n",
    "                    'std': np.std(mode_results['accuracies'])\n",
    "                },\n",
    "                'top3_recall': {\n",
    "                    'mean': np.mean(mode_results['top3_recalls']),\n",
    "                    'std': np.std(mode_results['top3_recalls'])\n",
    "                },\n",
    "                'macro_precision': {\n",
    "                    'mean': np.mean(mode_results['macro_precisions']),\n",
    "                    'std': np.std(mode_results['macro_precisions'])\n",
    "                },\n",
    "                'macro_recall': {\n",
    "                    'mean': np.mean(mode_results['macro_recalls']),\n",
    "                    'std': np.std(mode_results['macro_recalls'])\n",
    "                },\n",
    "                'macro_f1': {\n",
    "                    'mean': np.mean(mode_results['macro_f1s']),\n",
    "                    'std': np.std(mode_results['macro_f1s'])\n",
    "                },\n",
    "                'weighted_f1': {\n",
    "                    'mean': np.mean(mode_results['weighted_f1s']),\n",
    "                    'std': np.std(mode_results['weighted_f1s'])\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{mode.upper().replace('-', ' ')} RESULTS:\")\n",
    "            print(f\"Accuracy: {final_results[mode]['accuracy']['mean']:.4f} ± {final_results[mode]['accuracy']['std']:.4f}\")\n",
    "            print(f\"Top-3 Recall: {final_results[mode]['top3_recall']['mean']:.4f} ± {final_results[mode]['top3_recall']['std']:.4f}\")\n",
    "            print(f\"Macro Precision: {final_results[mode]['macro_precision']['mean']:.4f} ± {final_results[mode]['macro_precision']['std']:.4f}\")\n",
    "            print(f\"Macro Recall: {final_results[mode]['macro_recall']['mean']:.4f} ± {final_results[mode]['macro_recall']['std']:.4f}\")\n",
    "            print(f\"Macro F1: {final_results[mode]['macro_f1']['mean']:.4f} ± {final_results[mode]['macro_f1']['std']:.4f}\")\n",
    "            print(f\"Weighted F1: {final_results[mode]['weighted_f1']['mean']:.4f} ± {final_results[mode]['weighted_f1']['std']:.4f}\")\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, \"final_all_modes_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Results saved to final_all_modes_results.json\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546cc2b8-1cae-4687-869a-a27f13346dde",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aeb1dc-8ac0-46e5-a73e-f479a44934fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"all_datasets\"\n",
    "RESULTS_DIR = \"classification_results\"\n",
    "DOMAIN_MAPPING_FILE = \"clustering_domain_mapping.json\"\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "TIMEOUT = 180\n",
    "N_RUNS = 5\n",
    "K_FOLDS = 5\n",
    "FEW_SHOT_SIZE = 5\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('llm_classification.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ClassificationResult(BaseModel):\n",
    "    primary_domain: str = Field(..., description=\"The most appropriate domain for the dataset\")\n",
    "    alternative_domains: List[str] = Field(\n",
    "        default_factory=list, \n",
    "        description=\"List of alternative relevant domains (up to 2)\",\n",
    "        max_items=2\n",
    "    )\n",
    "\n",
    "def detect_encoding(file_path: str) -> str:\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            rawdata = f.read(50000)\n",
    "        result = chardet.detect(rawdata)\n",
    "        return result['encoding'] if result['confidence'] > 0.7 else 'utf-8'\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Encoding detection error: {e}, using utf-8\")\n",
    "        return 'utf-8'\n",
    "\n",
    "def read_csv_with_memory_limit(file_path: str, dataset_id: str = None) -> pd.DataFrame:\n",
    "    try:\n",
    "        if dataset_id:\n",
    "            random_path = os.path.join(OUTPUT_DIR, dataset_id, \"diverse_rows.csv\")\n",
    "            if os.path.exists(random_path):\n",
    "                encoding = detect_encoding(random_path)\n",
    "                df = pd.read_csv(random_path, encoding=encoding)\n",
    "                logger.debug(f\"Read diverse rows for {dataset_id} ({len(df)} rows, {len(df.columns)} columns)\")\n",
    "                return df\n",
    "\n",
    "        encoding = detect_encoding(file_path)\n",
    "        logger.debug(f\"Detected encoding: {encoding} for {os.path.basename(file_path)}\")\n",
    "        \n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            preview = pd.read_csv(f, nrows=1000)\n",
    "        \n",
    "        dtype_dict = preview.dtypes.to_dict()\n",
    "        \n",
    "        chunks = []\n",
    "        chunk_size = 10000\n",
    "        \n",
    "        for chunk in pd.read_csv(\n",
    "            file_path, \n",
    "            encoding=encoding,\n",
    "            chunksize=chunk_size,\n",
    "            dtype=dtype_dict\n",
    "        ):\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        if chunks:\n",
    "            df = pd.concat(chunks, ignore_index=True)\n",
    "            logger.debug(f\"Read dataset with {len(df)} rows and {len(df.columns)} columns\")\n",
    "            return df\n",
    "        else:\n",
    "            logger.warning(\"No data read, returning empty DataFrame\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading CSV: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def prepare_for_llm(df: pd.DataFrame, max_rows: int = 5) -> str:\n",
    "    df_subset = df.head(max_rows).copy()\n",
    "    df_subset = df_subset.fillna('missing')\n",
    "    \n",
    "    columns = df_subset.columns.tolist()\n",
    "    data = []\n",
    "    \n",
    "    for _, row in df_subset.iterrows():\n",
    "        formatted_row = []\n",
    "        for value in row:\n",
    "            if isinstance(value, str):\n",
    "                val = value.replace('\"', '\\\\\"')\n",
    "                formatted_value = f'\"{val}\"'\n",
    "            else:\n",
    "                formatted_value = str(value)\n",
    "            formatted_row.append(formatted_value)\n",
    "        data.append(f\"[{', '.join(formatted_row)}]\")\n",
    "    \n",
    "    columns_str = \"[\" + \", \".join([f'\"{col}\"' for col in columns]) + \"]\"\n",
    "    data_str = \",\\n\".join(data)\n",
    "    \n",
    "    return f\"columns = {columns_str}\\ndata = [\\n{data_str}\\n]\"\n",
    "\n",
    "class DeepSeekClassifier:\n",
    "    def __init__(self):\n",
    "        self.domains = self.load_domains()\n",
    "        self.client = OpenAI(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            api_key=OPENROUTER_API_KEY,\n",
    "            default_headers={\n",
    "                \"HTTP-Referer\": \"https://your-site.com\",\n",
    "                \"X-Title\": \"Dataset Classifier\"\n",
    "            }\n",
    "        )\n",
    "        self.parser = PydanticOutputParser(pydantic_object=ClassificationResult)\n",
    "        logger.info(f\"Initialized with {len(self.domains)} domains\")\n",
    "\n",
    "    def load_domains(self) -> List[str]:\n",
    "        if not os.path.exists(DOMAIN_MAPPING_FILE):\n",
    "            logger.error(f\"Domain file not found: {os.path.abspath(DOMAIN_MAPPING_FILE)}\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            with open(DOMAIN_MAPPING_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                domain_data = json.load(f)\n",
    "                \n",
    "                if isinstance(domain_data, dict):\n",
    "                    domains = list(domain_data.keys())\n",
    "                    logger.info(f\"Loaded {len(domains)} domains\")\n",
    "                    return domains\n",
    "                else:\n",
    "                    logger.error(f\"Unknown domain file format: {type(domain_data)}\")\n",
    "                    return []\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading domains: {e}\", exc_info=True)\n",
    "            return []\n",
    "\n",
    "    def create_examples_text(self, examples: List[Tuple[str, dict]]) -> str:\n",
    "        examples_text = \"\"\n",
    "        for i, (data_sample, classification) in enumerate(examples, 1):\n",
    "            examples_text += (\n",
    "                f\"\\nExample {i}:\\nDataset sample:\\n{data_sample}\\n\"\n",
    "                f\"Classification: {json.dumps(classification)}\\n\"\n",
    "            )\n",
    "        return examples_text\n",
    "\n",
    "    @retry(stop=stop_after_attempt(5), wait=wait_fixed(20))\n",
    "    def classify(self, df: pd.DataFrame, mode: str = \"zero-shot\", examples: List[Tuple[str, dict]] = None) -> ClassificationResult:\n",
    "        if df.empty or not self.domains:\n",
    "            logger.warning(\"Empty DataFrame or no domains for classification\")\n",
    "            return ClassificationResult(primary_domain=\"unknown\", alternative_domains=[])\n",
    "        \n",
    "        try:\n",
    "            data_sample = prepare_for_llm(df)\n",
    "            format_instructions = self.parser.get_format_instructions()\n",
    "            \n",
    "            if mode == \"zero-shot\":\n",
    "                system_content = (\n",
    "                    \"You are a dataset domain classification expert. \"\n",
    "                    \"Analyze the dataset sample provided as a DataFrame-like structure \"\n",
    "                    \"and determine its primary domain with optional alternatives. \"\n",
    "                    \"Choose ONLY from the following domains: \" + \", \".join(self.domains) + \"\\n\\n\" +\n",
    "                    format_instructions + \"\\n\\n\" +\n",
    "                    \"Rules:\\n\"\n",
    "                    \"1. primary_domain: SINGLE most appropriate domain (MUST be from list)\\n\"\n",
    "                    \"2. alternative_domains: 0-2 other relevant domains (only if applicable)\\n\"\n",
    "                    \"3. Use EXACT domain names\\n\"\n",
    "                    \"4. No additional text/comments!\"\n",
    "                )\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_content},\n",
    "                    {\"role\": \"user\", \"content\": f\"Dataset sample:\\n{data_sample}\"}\n",
    "                ]\n",
    "            \n",
    "            elif mode in [\"one-shot\", \"few-shot\"] and examples:\n",
    "                examples_text = self.create_examples_text(examples)\n",
    "                system_content = (\n",
    "                    \"You are a dataset domain classification expert. \"\n",
    "                    \"Analyze dataset samples and determine their primary domain with optional alternatives. \"\n",
    "                    \"Choose ONLY from these domains: \" + \", \".join(self.domains) + \"\\n\\n\" +\n",
    "                    format_instructions + \"\\n\\n\" +\n",
    "                    \"Rules:\\n\"\n",
    "                    \"1. primary_domain: SINGLE most appropriate domain (MUST be from list)\\n\"\n",
    "                    \"2. alternative_domains: 0-2 other relevant domains (only if applicable)\\n\"\n",
    "                    \"3. Use EXACT domain names\\n\"\n",
    "                    \"4. No additional text/comments!\\n\\n\" +\n",
    "                    \"Examples:\" + examples_text\n",
    "                )\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_content},\n",
    "                    {\"role\": \"user\", \"content\": f\"Classify this new dataset:\\n{data_sample}\"}\n",
    "                ]\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid mode or missing examples: {mode}\")\n",
    "\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"deepseek/deepseek-chat\",\n",
    "                messages=messages,\n",
    "                temperature=0.3,\n",
    "                max_tokens=500,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                top_p=0.95\n",
    "            )\n",
    "            \n",
    "            content = response.choices[0].message.content\n",
    "            \n",
    "            try:\n",
    "                json_start = content.find('{')\n",
    "                json_end = content.rfind('}') + 1\n",
    "                if json_start != -1 and json_end != -1:\n",
    "                    json_str = content[json_start:json_end]\n",
    "                    result = self.parser.parse(json_str)\n",
    "                else:\n",
    "                    result = self.parser.parse(content)\n",
    "                    \n",
    "                logger.debug(f\"Classification result ({mode}): {result}\")\n",
    "                return result\n",
    "            except ValidationError as e:\n",
    "                logger.error(f\"Validation error: {e}\\nResponse content: {content}\")\n",
    "                return ClassificationResult(primary_domain=\"unknown\", alternative_domains=[])\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Classification error ({mode}): {e}\", exc_info=True)\n",
    "            return ClassificationResult(primary_domain=\"unknown\", alternative_domains=[])\n",
    "\n",
    "def load_all_datasets() -> Dict[str, Dict]:\n",
    "    datasets = {}\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        logger.error(f\"Dataset directory not found: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "        return datasets\n",
    "\n",
    "    dataset_ids = [d for d in os.listdir(OUTPUT_DIR) \n",
    "                   if os.path.isdir(os.path.join(OUTPUT_DIR, d))]\n",
    "    \n",
    "    valid_datasets = [\n",
    "        d for d in dataset_ids\n",
    "        if os.path.exists(os.path.join(OUTPUT_DIR, d, \"metadata.json\"))\n",
    "        and os.path.exists(os.path.join(OUTPUT_DIR, d, \"diverse_rows.csv\"))\n",
    "    ]\n",
    "    \n",
    "    logger.info(f\"Loading {len(valid_datasets)} valid datasets...\")\n",
    "    \n",
    "    for dataset_id in tqdm(valid_datasets, desc=\"Loading datasets\"):\n",
    "        try:\n",
    "            meta_path = os.path.join(OUTPUT_DIR, dataset_id, \"metadata.json\")\n",
    "            data_path = os.path.join(OUTPUT_DIR, dataset_id, \"diverse_rows.csv\")\n",
    "\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "            \n",
    "            if \"domain\" not in metadata:\n",
    "                logger.warning(f\"Skipping {dataset_id}: 'domain' field missing in metadata\")\n",
    "                continue\n",
    "\n",
    "            df = read_csv_with_memory_limit(data_path, dataset_id=dataset_id)\n",
    "            if df.empty:\n",
    "                logger.warning(f\"Empty DataFrame for {dataset_id}, skipping\")\n",
    "                continue\n",
    "\n",
    "            datasets[dataset_id] = {\n",
    "                'metadata': metadata,\n",
    "                'dataframe': df,\n",
    "                'domain': metadata[\"domain\"].lower(),\n",
    "                'data_sample': prepare_for_llm(df)\n",
    "            }\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {dataset_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Successfully loaded {len(datasets)} datasets\")\n",
    "    return datasets\n",
    "\n",
    "def run_zero_shot_classification(datasets: Dict[str, Dict], run_id: int) -> Tuple[Optional[float], List[str], List[str], Optional[Dict], Optional[float]]:\n",
    "    logger.info(f\"=== Starting zero-shot classification run {run_id} ===\")\n",
    "    classifier = DeepSeekClassifier()\n",
    "\n",
    "    if not classifier.domains or not datasets:\n",
    "        logger.error(\"Classification domains or datasets not loaded\")\n",
    "        return None, [], [], {}, None\n",
    "\n",
    "    test_size = len(datasets) // K_FOLDS\n",
    "    dataset_items = list(datasets.items())\n",
    "    random.shuffle(dataset_items)\n",
    "    test_datasets = dict(dataset_items[:test_size])\n",
    "    \n",
    "    matches = 0\n",
    "    top3_matches = 0\n",
    "    total = 0\n",
    "    actual_labels = []\n",
    "    predicted_labels = []\n",
    "    \n",
    "    pbar = tqdm(test_datasets.items(), desc=f\"Zero-shot classification (Run {run_id})\", unit=\"dataset\")\n",
    "\n",
    "    for dataset_id, data_info in pbar:\n",
    "        try:\n",
    "            result = classifier.classify(data_info['dataframe'], mode=\"zero-shot\")\n",
    "            actual = data_info['domain']\n",
    "            predicted_primary = result.primary_domain.lower()\n",
    "            \n",
    "            all_predictions = [predicted_primary] + [alt.lower() for alt in result.alternative_domains]\n",
    "            top3 = all_predictions[:3]\n",
    "\n",
    "            if predicted_primary == actual:\n",
    "                matches += 1\n",
    "            if actual in top3:\n",
    "                top3_matches += 1\n",
    "                \n",
    "            total += 1\n",
    "            actual_labels.append(actual)\n",
    "            predicted_labels.append(predicted_primary)\n",
    "                \n",
    "            pbar.set_postfix({\n",
    "                \"Accuracy\": f\"{matches/total:.2%}\" if total > 0 else \"N/A\",\n",
    "                \"Top-3 Recall\": f\"{top3_matches/total:.2%}\" if total > 0 else \"N/A\"\n",
    "            })\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if total > 0:\n",
    "        accuracy = matches / total\n",
    "        top3_recall = top3_matches / total\n",
    "        logger.info(f\"Zero-shot accuracy: {accuracy:.2%}\")\n",
    "        logger.info(f\"Zero-shot top-3 recall: {top3_recall:.2%}\")\n",
    "        \n",
    "        report = classification_report(actual_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "        return accuracy, actual_labels, predicted_labels, report, top3_recall\n",
    "    else:\n",
    "        logger.warning(\"No datasets processed for accuracy calculation\")\n",
    "        return None, [], [], {}, None\n",
    "\n",
    "def run_kfold_classification(datasets: Dict[str, Dict], mode: str, run_id: int) -> Tuple[List[float], List[float], List[Dict]]:\n",
    "    logger.info(f\"=== Starting {mode} classification with K-fold validation (Run {run_id}) ===\")\n",
    "    classifier = DeepSeekClassifier()\n",
    "\n",
    "    if not classifier.domains or not datasets:\n",
    "        logger.error(\"Classification domains or datasets not loaded\")\n",
    "        return [], [], []\n",
    "\n",
    "    dataset_items = list(datasets.items())\n",
    "    kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=42 + run_id)\n",
    "    \n",
    "    fold_accuracies = []\n",
    "    fold_top3_recalls = []\n",
    "    fold_reports = []\n",
    "\n",
    "    for fold_idx, (train_indices, test_indices) in enumerate(kf.split(dataset_items)):\n",
    "        logger.info(f\"Processing fold {fold_idx + 1}/{K_FOLDS}\")\n",
    "        \n",
    "        train_datasets = [dataset_items[i] for i in train_indices]\n",
    "        test_datasets = [dataset_items[i] for i in test_indices]\n",
    "        \n",
    "        examples = []\n",
    "        if mode == \"one-shot\":\n",
    "            domain_examples = {}\n",
    "            for dataset_id, data_info in train_datasets:\n",
    "                domain = data_info['domain']\n",
    "                if domain not in domain_examples:\n",
    "                    classification = {\n",
    "                        \"primary_domain\": domain,\n",
    "                        \"alternative_domains\": []\n",
    "                    }\n",
    "                    domain_examples[domain] = (data_info['data_sample'], classification)\n",
    "            examples = list(domain_examples.values())\n",
    "        \n",
    "        elif mode == \"few-shot\":\n",
    "            domain_examples = {}\n",
    "            for dataset_id, data_info in train_datasets:\n",
    "                domain = data_info['domain']\n",
    "                if domain not in domain_examples:\n",
    "                    domain_examples[domain] = []\n",
    "                \n",
    "                if len(domain_examples[domain]) < FEW_SHOT_SIZE:\n",
    "                    classification = {\n",
    "                        \"primary_domain\": domain,\n",
    "                        \"alternative_domains\": []\n",
    "                    }\n",
    "                    domain_examples[domain].append((data_info['data_sample'], classification))\n",
    "            \n",
    "            for domain_list in domain_examples.values():\n",
    "                examples.extend(domain_list)\n",
    "        \n",
    "        matches = 0\n",
    "        top3_matches = 0\n",
    "        total = 0\n",
    "        actual_labels = []\n",
    "        predicted_labels = []\n",
    "        \n",
    "        pbar = tqdm(test_datasets, desc=f\"{mode.capitalize()} fold {fold_idx + 1}\", unit=\"dataset\")\n",
    "        \n",
    "        for dataset_id, data_info in pbar:\n",
    "            try:\n",
    "                result = classifier.classify(\n",
    "                    data_info['dataframe'], \n",
    "                    mode=mode, \n",
    "                    examples=examples\n",
    "                )\n",
    "                actual = data_info['domain']\n",
    "                predicted_primary = result.primary_domain.lower()\n",
    "                \n",
    "                all_predictions = [predicted_primary] + [alt.lower() for alt in result.alternative_domains]\n",
    "                top3 = all_predictions[:3]\n",
    "\n",
    "                if predicted_primary == actual:\n",
    "                    matches += 1\n",
    "                if actual in top3:\n",
    "                    top3_matches += 1\n",
    "                    \n",
    "                total += 1\n",
    "                actual_labels.append(actual)\n",
    "                predicted_labels.append(predicted_primary)\n",
    "                    \n",
    "                pbar.set_postfix({\n",
    "                    \"Accuracy\": f\"{matches/total:.2%}\" if total > 0 else \"N/A\",\n",
    "                    \"Top-3 Recall\": f\"{top3_matches/total:.2%}\" if total > 0 else \"N/A\"\n",
    "                })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if total > 0:\n",
    "            fold_accuracy = matches / total\n",
    "            fold_top3_recall = top3_matches / total\n",
    "            fold_report = classification_report(actual_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "            \n",
    "            fold_accuracies.append(fold_accuracy)\n",
    "            fold_top3_recalls.append(fold_top3_recall)\n",
    "            fold_reports.append(fold_report)\n",
    "            \n",
    "            logger.info(f\"Fold {fold_idx + 1} - Accuracy: {fold_accuracy:.2%}, Top-3 Recall: {fold_top3_recall:.2%}\")\n",
    "\n",
    "    return fold_accuracies, fold_top3_recalls, fold_reports\n",
    "\n",
    "def calculate_aggregated_metrics(fold_reports: List[Dict]) -> Dict[str, float]:\n",
    "    if not fold_reports:\n",
    "        return {}\n",
    "    \n",
    "    macro_precisions = [report['macro avg']['precision'] for report in fold_reports]\n",
    "    macro_recalls = [report['macro avg']['recall'] for report in fold_reports]\n",
    "    macro_f1s = [report['macro avg']['f1-score'] for report in fold_reports]\n",
    "    weighted_f1s = [report['weighted avg']['f1-score'] for report in fold_reports]\n",
    "    \n",
    "    return {\n",
    "        'macro_precision_mean': np.mean(macro_precisions),\n",
    "        'macro_precision_std': np.std(macro_precisions),\n",
    "        'macro_recall_mean': np.mean(macro_recalls),\n",
    "        'macro_recall_std': np.std(macro_recalls),\n",
    "        'macro_f1_mean': np.mean(macro_f1s),\n",
    "        'macro_f1_std': np.std(macro_f1s),\n",
    "        'weighted_f1_mean': np.mean(weighted_f1s),\n",
    "        'weighted_f1_std': np.std(weighted_f1s)\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    logger.info(\"Loading all datasets...\")\n",
    "    datasets = load_all_datasets()\n",
    "    \n",
    "    if not datasets:\n",
    "        logger.error(\"No datasets loaded, exiting\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Loaded {len(datasets)} datasets\")\n",
    "    \n",
    "    results = {\n",
    "        'zero-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        },\n",
    "        'one-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        },\n",
    "        'few-shot': {\n",
    "            'accuracies': [],\n",
    "            'top3_recalls': [],\n",
    "            'macro_precisions': [],\n",
    "            'macro_recalls': [],\n",
    "            'macro_f1s': [],\n",
    "            'weighted_f1s': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"STARTING CLASSIFICATION EXPERIMENT\")\n",
    "    print(f\"Modes: Zero-shot, One-shot, Few-shot\")\n",
    "    print(f\"Runs per mode: {N_RUNS}\")\n",
    "    print(f\"K-Folds for shot learning: {K_FOLDS}\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ZERO-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        accuracy, actual_labels, predicted_labels, report, top3_recall = run_zero_shot_classification(datasets, run_id)\n",
    "        \n",
    "        if accuracy is not None:\n",
    "            results['zero-shot']['accuracies'].append(accuracy)\n",
    "            results['zero-shot']['top3_recalls'].append(top3_recall)\n",
    "            results['zero-shot']['macro_precisions'].append(report['macro avg']['precision'])\n",
    "            results['zero-shot']['macro_recalls'].append(report['macro avg']['recall'])\n",
    "            results['zero-shot']['macro_f1s'].append(report['macro avg']['f1-score'])\n",
    "            results['zero-shot']['weighted_f1s'].append(report['weighted avg']['f1-score'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={accuracy:.4f}, Top-3 Recall={top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"ONE-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"one-shot\", run_id)\n",
    "        \n",
    "        if fold_accuracies:\n",
    "            run_accuracy = np.mean(fold_accuracies)\n",
    "            run_top3_recall = np.mean(fold_top3_recalls)\n",
    "            run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "            results['one-shot']['accuracies'].append(run_accuracy)\n",
    "            results['one-shot']['top3_recalls'].append(run_top3_recall)\n",
    "            results['one-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "            results['one-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "            results['one-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "            results['one-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(\"FEW-SHOT CLASSIFICATION\".center(60))\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    for run_id in range(1, N_RUNS + 1):\n",
    "        fold_accuracies, fold_top3_recalls, fold_reports = run_kfold_classification(datasets, \"few-shot\", run_id)\n",
    "        \n",
    "        if fold_accuracies:\n",
    "            run_accuracy = np.mean(fold_accuracies)\n",
    "            run_top3_recall = np.mean(fold_top3_recalls)\n",
    "            run_metrics = calculate_aggregated_metrics(fold_reports)\n",
    "            \n",
    "            results['few-shot']['accuracies'].append(run_accuracy)\n",
    "            results['few-shot']['top3_recalls'].append(run_top3_recall)\n",
    "            results['few-shot']['macro_precisions'].append(run_metrics['macro_precision_mean'])\n",
    "            results['few-shot']['macro_recalls'].append(run_metrics['macro_recall_mean'])\n",
    "            results['few-shot']['macro_f1s'].append(run_metrics['macro_f1_mean'])\n",
    "            results['few-shot']['weighted_f1s'].append(run_metrics['weighted_f1_mean'])\n",
    "            \n",
    "            print(f\"Run {run_id}: Accuracy={run_accuracy:.4f}, Top-3 Recall={run_top3_recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"FINAL AGGREGATED RESULTS\".center(80))\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    final_results = {}\n",
    "    \n",
    "    for mode in ['zero-shot', 'one-shot', 'few-shot']:\n",
    "        mode_results = results[mode]\n",
    "        \n",
    "        if mode_results['accuracies']:\n",
    "            final_results[mode] = {\n",
    "                'accuracy': {\n",
    "                    'mean': np.mean(mode_results['accuracies']),\n",
    "                    'std': np.std(mode_results['accuracies'])\n",
    "                },\n",
    "                'top3_recall': {\n",
    "                    'mean': np.mean(mode_results['top3_recalls']),\n",
    "                    'std': np.std(mode_results['top3_recalls'])\n",
    "                },\n",
    "                'macro_precision': {\n",
    "                    'mean': np.mean(mode_results['macro_precisions']),\n",
    "                    'std': np.std(mode_results['macro_precisions'])\n",
    "                },\n",
    "                'macro_recall': {\n",
    "                    'mean': np.mean(mode_results['macro_recalls']),\n",
    "                    'std': np.std(mode_results['macro_recalls'])\n",
    "                },\n",
    "                'macro_f1': {\n",
    "                    'mean': np.mean(mode_results['macro_f1s']),\n",
    "                    'std': np.std(mode_results['macro_f1s'])\n",
    "                },\n",
    "                'weighted_f1': {\n",
    "                    'mean': np.mean(mode_results['weighted_f1s']),\n",
    "                    'std': np.std(mode_results['weighted_f1s'])\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{mode.upper().replace('-', ' ')} RESULTS:\")\n",
    "            print(f\"Accuracy: {final_results[mode]['accuracy']['mean']:.4f} ± {final_results[mode]['accuracy']['std']:.4f}\")\n",
    "            print(f\"Top-3 Recall: {final_results[mode]['top3_recall']['mean']:.4f} ± {final_results[mode]['top3_recall']['std']:.4f}\")\n",
    "            print(f\"Macro Precision: {final_results[mode]['macro_precision']['mean']:.4f} ± {final_results[mode]['macro_precision']['std']:.4f}\")\n",
    "            print(f\"Macro Recall: {final_results[mode]['macro_recall']['mean']:.4f} ± {final_results[mode]['macro_recall']['std']:.4f}\")\n",
    "            print(f\"Macro F1: {final_results[mode]['macro_f1']['mean']:.4f} ± {final_results[mode]['macro_f1']['std']:.4f}\")\n",
    "            print(f\"Weighted F1: {final_results[mode]['weighted_f1']['mean']:.4f} ± {final_results[mode]['weighted_f1']['std']:.4f}\")\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIR, \"final_all_modes_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Results saved to final_all_modes_results.json\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
