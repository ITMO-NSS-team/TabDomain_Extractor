{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1205dfcf-fedd-461a-ba26-f892ba59146e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Imports & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7abb8-5f59-46a2-bed1-d499bde6d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import gc\n",
    "import shutil\n",
    "import warnings\n",
    "from typing import Dict, List, Any, Callable, Set, Tuple\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import openml\n",
    "from tqdm import tqdm\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import HDBSCAN, KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gensim.models import Word2Vec\n",
    "import chardet\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f18ef-22e3-4e51-9866-b8bcf9d88a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Логирование\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('dataset_processing.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f6ffd4-7a09-4253-81a1-6824682135e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loading datasets from OpenML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6327a1b-9cad-44ed-8e04-5e12ddf98344",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"all_datasets\"\n",
    "TAG_FILE = \"unique_tags.json\"\n",
    "MAX_DATASETS = 100\n",
    "MAX_CHECK_LIMIT = 1000\n",
    "openml.config.apikey = \"OPENML_API_KEY\"\n",
    "openml.config.cache_directory = OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b95de-fc76-4dd4-9435-5b51cfe104e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(20))\n",
    "def get_dataset_ids_with_tags():\n",
    "    \"\"\"Получение ID датасетов с тегами\"\"\"\n",
    "    try:\n",
    "        batch_size = 100\n",
    "        tagged_ids = []\n",
    "        total_pages = (MAX_CHECK_LIMIT + batch_size - 1) // batch_size \n",
    "\n",
    "        for page in range(total_pages):\n",
    "            # Рассчёт лимит для последней партии\n",
    "            current_limit = min(batch_size, MAX_CHECK_LIMIT - page * batch_size)\n",
    "            \n",
    "            datasets = openml.datasets.list_datasets(\n",
    "                output_format=\"dataframe\",\n",
    "                offset=page * batch_size,\n",
    "                limit=current_limit\n",
    "            )\n",
    "\n",
    "            if datasets.empty:\n",
    "                break\n",
    "\n",
    "            for did in tqdm(datasets['did'], desc=f\"Проверка партии {page+1}/{total_pages}\"):\n",
    "                try:\n",
    "                    dataset = openml.datasets.get_dataset(did, download_data=False)\n",
    "                    tags = getattr(dataset, 'tag', []) or getattr(dataset, 'tag', []) or []\n",
    "                    if tags:\n",
    "                        tagged_ids.append(did)\n",
    "                        if len(tagged_ids) >= MAX_DATASETS:\n",
    "                            return tagged_ids\n",
    "                    time.sleep(0.3)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "            if len(tagged_ids) >= MAX_DATASETS:\n",
    "                break\n",
    "\n",
    "        return tagged_ids[:MAX_DATASETS]\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка получения ID: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(20))\n",
    "def process_dataset(dataset_id):\n",
    "    \"\"\"Обработка отдельного датасета\"\"\"\n",
    "    try:\n",
    "        dataset = openml.datasets.get_dataset(dataset_id)\n",
    "        data = dataset.get_data(dataset_format=\"dataframe\")[0]\n",
    "\n",
    "        # Получение тегов\n",
    "        tags = getattr(dataset, 'tag', []) or getattr(dataset, 'tag', []) or []\n",
    "\n",
    "        # Создание директории\n",
    "        dataset_dir = os.path.join(OUTPUT_DIR, str(dataset_id))\n",
    "        os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "        # Сохранение полного набора данных\n",
    "        data.to_csv(os.path.join(dataset_dir, \"full_dataset.csv\"), index=False)\n",
    "\n",
    "        # Метаданные\n",
    "        metadata = {\n",
    "            \"id\": dataset_id,\n",
    "            \"name\": dataset.name,\n",
    "            \"tags\": tags,\n",
    "            \"features\": list(data.columns),\n",
    "            \"rows\": len(data)\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(dataset_dir, \"metadata.json\"), 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "        return True, tags\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ошибка обработки {dataset_id}: {str(e)}\")\n",
    "        return False, []\n",
    "\n",
    "def download_datasets():\n",
    "    \"\"\"Основная функция загрузки датасетов\"\"\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    tagged_ids = get_dataset_ids_with_tags()\n",
    "\n",
    "    if not tagged_ids:\n",
    "        logger.error(\"Не найдено датасетов с тегами!\")\n",
    "        return\n",
    "\n",
    "    if len(tagged_ids) < MAX_DATASETS:\n",
    "        logger.warning(f\"Найдено только {len(tagged_ids)} датасетов с тегами\")\n",
    "\n",
    "    unique_tags = set()\n",
    "    success = 0\n",
    "\n",
    "    for dataset_id in tqdm(tagged_ids[:MAX_DATASETS], desc=\"Обработка датасетов\"):\n",
    "        result, tags = process_dataset(dataset_id)\n",
    "        if result:\n",
    "            success += 1\n",
    "            unique_tags.update(tags)\n",
    "        time.sleep(1)\n",
    "\n",
    "    with open(TAG_FILE, 'w') as f:\n",
    "        json.dump(sorted(unique_tags), f, indent=2)\n",
    "\n",
    "    logger.info(f\"Успешно обработано: {success}/{len(tagged_ids[:MAX_DATASETS])}\")\n",
    "    logger.info(f\"Уникальных тегов собрано: {len(unique_tags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060360f6-efdf-4319-97a2-9f7609428703",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Making dataset samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c03c4a-fa39-41b4-aebf-c39194ed8fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"all_datasets\"\n",
    "MAX_PROCESSING_COLUMNS = 50\n",
    "MAX_CLUSTERING_ROWS = 1000  \n",
    "SAMPLE_ROWS = 5\n",
    "MAX_DATASET_SIZE = 100000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a62cca-9531-4f34-8601-5df81ecf2907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path: str) -> str:\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            rawdata = f.read(50000)\n",
    "        result = chardet.detect(rawdata)\n",
    "        return result['encoding'] if result['confidence'] > 0.7 else 'utf-8'\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Encoding detection error: {e}, using utf-8\")\n",
    "        return 'utf-8'\n",
    "\n",
    "def read_csv_with_memory_limit(file_path: str, dataset_id: str = None) -> pd.DataFrame:\n",
    "    try:\n",
    "        encoding = detect_encoding(file_path)\n",
    "        logger.info(f\"Detected encoding: {encoding} for {os.path.basename(file_path)}\")\n",
    "        \n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            first_line = f.readline()\n",
    "            column_names = first_line.strip().split(',')\n",
    "        columns_to_use = column_names[:MAX_PROCESSING_COLUMNS]\n",
    "        \n",
    "        chunks = []\n",
    "        chunk_size = 10000\n",
    "        total_rows = 0\n",
    "        \n",
    "        for chunk in pd.read_csv(file_path, usecols=columns_to_use, encoding=encoding, chunksize=chunk_size, dtype='object'):\n",
    "            chunks.append(chunk)\n",
    "            total_rows += len(chunk)\n",
    "            if total_rows > MAX_DATASET_SIZE:\n",
    "                logger.warning(f\"Reached max dataset size ({MAX_DATASET_SIZE} rows)\")\n",
    "                break\n",
    "        \n",
    "        if chunks:\n",
    "            df = pd.concat(chunks, ignore_index=True)\n",
    "            if len(df) > MAX_DATASET_SIZE:\n",
    "                df = df.sample(n=MAX_DATASET_SIZE, random_state=42)\n",
    "                logger.info(f\"Sampled {MAX_DATASET_SIZE} rows from large dataset\")\n",
    "            logger.info(f\"Read dataset with {len(df)} rows and {len(df.columns)} columns\")\n",
    "            return df\n",
    "        else:\n",
    "            logger.warning(\"No data read, returning empty DataFrame\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading CSV: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff47caf-1a5c-45fa-ae7c-7255fc70af02",
   "metadata": {},
   "source": [
    "## Sample of diverse rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5edc1ee-9840-491b-961d-6ad53d2fed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(column: pd.Series) -> float:\n",
    "    \"\"\"Вычисление энтропии\"\"\"\n",
    "    from collections import Counter\n",
    "    from math import log2\n",
    "    counts = Counter(column)\n",
    "    total = len(column)\n",
    "    entropy = 0.0\n",
    "    for count in counts.values():\n",
    "        p = count / total\n",
    "        entropy -= p * log2(p) if p > 0 else 0\n",
    "    return entropy\n",
    "\n",
    " \n",
    "def select_diverse_rows(df: pd.DataFrame, n_rows: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Отбор разнообразных строк\"\"\"\n",
    "    if df.empty:\n",
    "        logger.warning(\"Empty DataFrame provided for row selection\")\n",
    "        return df\n",
    "    if len(df) <= n_rows:\n",
    "        logger.info(f\"DataFrame small ({len(df)} rows), returning all rows\")\n",
    "        return df\n",
    "    \n",
    "    try:\n",
    "        if len(df) > MAX_CLUSTERING_ROWS:\n",
    "            df_sampled = df.sample(n=MAX_CLUSTERING_ROWS, random_state=42)\n",
    "            logger.info(f\"Downsampled to {MAX_CLUSTERING_ROWS} rows for entropy selection\")\n",
    "        else:\n",
    "            df_sampled = df.copy()\n",
    "        \n",
    "        if len(df_sampled.columns) > MAX_PROCESSING_COLUMNS:\n",
    "            columns_to_use = df_sampled.columns[:MAX_PROCESSING_COLUMNS]\n",
    "            df_processed = df_sampled[columns_to_use]\n",
    "            logger.info(f\"Limited to {len(columns_to_use)} columns for selection\")\n",
    "        else:\n",
    "            df_processed = df_sampled.copy()\n",
    "        \n",
    "        for col in df_processed.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df_processed[col]):\n",
    "                median_val = df_processed[col].median()\n",
    "                if pd.isna(median_val):\n",
    "                    median_val = 0\n",
    "                df_processed[col].fillna(median_val, inplace=True)\n",
    "            else:\n",
    "                mode_val = df_processed[col].mode()\n",
    "                df_processed[col].fillna(mode_val[0] if not mode_val.empty else \"missing\", inplace=True)\n",
    "        \n",
    "        # Жадный алгоритм отбора строк\n",
    "        selected_indices = []\n",
    "        remaining_indices = set(df_processed.index)\n",
    "        \n",
    "        # Инициализация: выбираем строку с максимальной энтропией\n",
    "        max_entropy = -1\n",
    "        best_index = None\n",
    "        for idx in df_processed.index:\n",
    "            entropy_sum = sum(calculate_entropy(df_processed.loc[[idx], col]) for col in df_processed.columns)\n",
    "            if entropy_sum > max_entropy:\n",
    "                max_entropy = entropy_sum\n",
    "                best_index = idx\n",
    "        \n",
    "        if best_index is None:\n",
    "            return df_sampled.sample(n=min(n_rows, len(df_sampled)), random_state=42)\n",
    "        \n",
    "        selected_indices.append(best_index)\n",
    "        remaining_indices.remove(best_index)\n",
    "        \n",
    "        # Последовательный выбор строк\n",
    "        for i in range(1, n_rows):\n",
    "            max_entropy_gain = -1\n",
    "            best_candidate = None\n",
    "            \n",
    "            for candidate in remaining_indices:\n",
    "                temp_set = selected_indices + [candidate]\n",
    "                total_entropy = sum(calculate_entropy(df_processed.loc[temp_set, col]) for col in df_processed.columns)\n",
    "                if total_entropy > max_entropy_gain:\n",
    "                    max_entropy_gain = total_entropy\n",
    "                    best_candidate = candidate\n",
    "            \n",
    "            if best_candidate is not None:\n",
    "                selected_indices.append(best_candidate)\n",
    "                remaining_indices.remove(best_candidate)\n",
    "            else:\n",
    "                best_candidate = next(iter(remaining_indices), None)\n",
    "                if best_candidate is not None:\n",
    "                    selected_indices.append(best_candidate)\n",
    "                    remaining_indices.remove(best_candidate)\n",
    "        \n",
    "        logger.info(f\"Selected {len(selected_indices)} diverse rows\")\n",
    "        return df_sampled.loc[selected_indices]\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Entropy-based selection error: {e}\", exc_info=True)\n",
    "        return df.sample(n=min(n_rows, len(df)), random_state=42)\n",
    "\n",
    "def save_diverse_rows():\n",
    "    \"\"\"Сохранение сэмпла разнообразных строк датасета\"\"\"\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "    # Получение списка датасетов\n",
    "    dataset_ids = [d for d in os.listdir(OUTPUT_DIR) if os.path.isdir(os.path.join(OUTPUT_DIR, d))]\n",
    "    valid_datasets = [\n",
    "        d for d in dataset_ids\n",
    "        if os.path.exists(os.path.join(OUTPUT_DIR, d, \"metadata.json\"))\n",
    "        and os.path.exists(os.path.join(OUTPUT_DIR, d, \"full_dataset.csv\"))\n",
    "    ]\n",
    "    logger.info(f\"Found {len(valid_datasets)} valid datasets\")\n",
    "\n",
    "    # Обработка каждого датасета\n",
    "    for dataset_id in tqdm(valid_datasets, desc=\"Processing datasets\", unit=\"dataset\"):\n",
    "        try:\n",
    "            data_path = os.path.join(OUTPUT_DIR, dataset_id, \"full_dataset.csv\")\n",
    "            meta_path = os.path.join(OUTPUT_DIR, dataset_id, \"metadata.json\")\n",
    "\n",
    "            # Чтение метаданных\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "\n",
    "            # Чтение полного датасета\n",
    "            df = read_csv_with_memory_limit(data_path, dataset_id=dataset_id)\n",
    "            if df.empty:\n",
    "                logger.warning(f\"Empty DataFrame for {dataset_id}, skipping\")\n",
    "                continue\n",
    "\n",
    "            # Выбор 5 самых разнообразных строк\n",
    "            diverse_df = select_diverse_rows(df, n_rows=SAMPLE_ROWS)\n",
    "            if diverse_df.empty:\n",
    "                logger.warning(f\"No diverse rows selected for {dataset_id}, skipping\")\n",
    "                continue\n",
    "\n",
    "            # Сохранение выбранных строк\n",
    "            diverse_path = os.path.join(OUTPUT_DIR, dataset_id, \"diverse_rows.csv\")\n",
    "            diverse_df.to_csv(diverse_path, index=False)\n",
    "            logger.info(f\"Saved {len(diverse_df)} diverse rows for dataset {dataset_id}\")\n",
    "\n",
    "            # Обновление метаданных\n",
    "            metadata[\"diverse_rows_file\"] = \"diverse_rows.csv\"\n",
    "            with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "            # Очистка памяти\n",
    "            del df, diverse_df\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing dataset {dataset_id}: {e}\", exc_info=True)\n",
    "            continue\n",
    "\n",
    "    logger.info(f\"Completed processing {len(valid_datasets)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60657964-2df0-4084-a11b-36204d0bdab6",
   "metadata": {},
   "source": [
    "## Sample of random rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f909e23-da4f-4098-9c86-ca84e0d961fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_rows(df: pd.DataFrame, n_rows: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Отбор случайных строк датасета\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    n = min(n_rows, len(df))\n",
    "    return df.sample(n=n, random_state=42)\n",
    "\n",
    "def save_random_rows():\n",
    "    \"\"\"Сохранение сэмпла случайных строк датасета\"\"\"\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    dataset_ids = [d for d in os.listdir(OUTPUT_DIR) \n",
    "                 if os.path.isdir(os.path.join(OUTPUT_DIR, d))]\n",
    "    \n",
    "    valid_datasets = [\n",
    "        d for d in dataset_ids\n",
    "        if os.path.exists(os.path.join(OUTPUT_DIR, d, \"metadata.json\"))\n",
    "        and os.path.exists(os.path.join(OUTPUT_DIR, d, \"full_dataset.csv\"))\n",
    "    ]\n",
    "    logger.info(f\"Found {len(valid_datasets)} valid datasets\")\n",
    "\n",
    "    for dataset_id in tqdm(valid_datasets, desc=\"Processing datasets\"):\n",
    "        try:\n",
    "            data_path = os.path.join(OUTPUT_DIR, dataset_id, \"full_dataset.csv\")\n",
    "            meta_path = os.path.join(OUTPUT_DIR, dataset_id, \"metadata.json\")\n",
    "\n",
    "            with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata = json.load(f)\n",
    "\n",
    "            df = read_csv_with_memory_limit(data_path)\n",
    "            if df.empty:\n",
    "                logger.warning(f\"Empty DataFrame for {dataset_id}, skipping\")\n",
    "                continue\n",
    "\n",
    "            # Выбор случайных строк \n",
    "            random_df = select_random_rows(df, n_rows=SAMPLE_ROWS)\n",
    "            if random_df.empty:\n",
    "                logger.warning(f\"No rows selected for {dataset_id}, skipping\")\n",
    "                continue\n",
    "\n",
    "            random_path = os.path.join(OUTPUT_DIR, dataset_id, \"random_rows.csv\")\n",
    "            random_df.to_csv(random_path, index=False)\n",
    "            logger.info(f\"Saved {len(random_df)} random rows for {dataset_id}\")\n",
    "\n",
    "            metadata[\"random_rows_file\"] = \"random_rows.csv\"  \n",
    "            with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "            del df, random_df\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {dataset_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(f\"Completed processing {len(valid_datasets)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1443d5-fe4a-4b57-989f-e2feb5ef7a40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Creating domains and mapping datatsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ef6a9-8eb9-4ebd-a819-2c7f12a6fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"all_datasets\"\n",
    "OUTPUT_DIR = \"clustering_results\"\n",
    "DOMAIN_MAPPING_FILE = \"clustering_domain_mapping.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1380f045-7ff2-483c-9b05-e55797db3239",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERING_METHODS = {\n",
    "    \"hdbscan\": {\n",
    "        \"class\": HDBSCAN,\n",
    "        \"params\": {\n",
    "            \"min_cluster_size\": 3,\n",
    "            \"metric\": \"cosine\",\n",
    "            \"cluster_selection_method\": \"leaf\"\n",
    "        }\n",
    "    },\n",
    "    \"kmeans\": {\n",
    "        \"class\": KMeans,\n",
    "        \"params\": {\n",
    "            \"n_clusters\": 8,\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "    },\n",
    "    \"agglomerative\": {\n",
    "        \"class\": AgglomerativeClustering,\n",
    "        \"params\": {\n",
    "            \"n_clusters\": None,\n",
    "            \"distance_threshold\": 0.6,\n",
    "            \"linkage\": \"average\",\n",
    "            \"metric\": \"cosine\"\n",
    "        }\n",
    "    },\n",
    "    \"dbscan\": {\n",
    "        \"class\": DBSCAN,\n",
    "        \"params\": {\n",
    "            \"eps\": 0.5,\n",
    "            \"min_samples\": 3,\n",
    "            \"metric\": \"cosine\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "EMBEDDING_METHODS = {\n",
    "    \"sentence_transformers\": {\n",
    "        \"model\": \"all-mpnet-base-v2\",\n",
    "        \"dim\": 768\n",
    "    },\n",
    "    \"tfidf\": {\n",
    "        \"max_features\": 500,\n",
    "        \"dim\": 500\n",
    "    },\n",
    "    \"gensim_word2vec\": {\n",
    "        \"vector_size\": 100,\n",
    "        \"window\": 5,\n",
    "        \"min_count\": 1,\n",
    "        \"workers\": 4,\n",
    "        \"dim\": 100\n",
    "    }\n",
    "}\n",
    "\n",
    "STOP_PATTERNS = {\n",
    "    r'^study_?\\d+', r'test', r'temp', r'demo', r'^kaggle$', r'^uci$',\n",
    "    r'example', r'azure', r'tutorial', r'^data$',\n",
    "    r'_', r'\\d+$', r'mythbusting'\n",
    "}\n",
    "\n",
    "def normalize_tag(tag: str, apply_filtering: bool = True) -> str:\n",
    "    \"\"\"Нормализация тегов\"\"\"\n",
    "    if not isinstance(tag, str) or not tag.strip():\n",
    "        return None\n",
    "    \n",
    "    tag = tag.lower().strip()\n",
    "    tag = re.sub(r'[^\\w\\s-]', '', tag)  \n",
    "    normalized = re.sub(r'[\\s_]+', ' ', tag) \n",
    "    \n",
    "    if apply_filtering:\n",
    "        if len(normalized) < 3 or any(re.search(p, normalized) for p in STOP_PATTERNS):\n",
    "            return None\n",
    "    return normalized\n",
    "\n",
    "def filter_tags(tags: List[str]) -> List[str]:\n",
    "    \"\"\"Фильтрация списка тегов по правилам нормализации с удалением дубликатов\"\"\"\n",
    "    unique_tags = set()\n",
    "    for tag in tags:\n",
    "        normalized = normalize_tag(tag)\n",
    "        if normalized:\n",
    "            unique_tags.add(normalized)\n",
    "    return list(unique_tags)\n",
    "\n",
    "def load_and_filter_tags():\n",
    "    \"\"\"Загрузка и фильтрация тегов с удалением дубликатов\"\"\"\n",
    "    with open(TAG_FILE, 'r') as f:\n",
    "        tags = json.load(f)\n",
    "    \n",
    "    filtered = filter_tags(tags)\n",
    "    return filtered\n",
    "\n",
    "def cache_datasets():\n",
    "    \"\"\"Кэширование списка датасетов\"\"\"\n",
    "    datasets = openml.datasets.list_datasets(output_format=\"dataframe\")\n",
    "    cache_file = \"datasets_cache.json\"\n",
    "    with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "        datasets.to_json(f, orient=\"records\")\n",
    "    logger.info(f\"Datasets cached to {cache_file}\")\n",
    "    return datasets\n",
    "\n",
    "def load_cached_datasets():\n",
    "    \"\"\"Загрузка кэшированного списка датасетов\"\"\"\n",
    "    cache_file = \"datasets_cache.json\"\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            return pd.read_json(f, orient=\"records\")\n",
    "    return cache_datasets()\n",
    "\n",
    "def load_cached_datasets_by_tag(tag: str) -> List[Dict]:\n",
    "    \"\"\"Кэширование и загрузка датасетов по конкретному тегу\"\"\"\n",
    "    cache_file = f\"datasets_cache_{tag}.json\"\n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    logger.info(f\"Fetching datasets for tag '{tag}' from OpenML...\")\n",
    "    datasets = openml.datasets.list_datasets(tag=tag, output_format='dataframe')\n",
    "    datasets = datasets[['did']].to_dict(orient='records')\n",
    "    with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(datasets, f, indent=2, ensure_ascii=False)\n",
    "    logger.info(f\"Datasets for tag '{tag}' cached to {cache_file}\")\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775e2226-625c-4f8b-b41c-a54f20b447b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дополнительные функции для кластеризации и формирования доменов\n",
    "def enrich_small_domains(\n",
    "    domain_distribution: Dict[str, int],\n",
    "    domain_tags: Dict[str, List[str]],\n",
    "    min_datasets: int = 5,\n",
    "    max_new_datasets_per_domain: int = 10,\n",
    "    output_dir: str = OUTPUT_DIR_DATASETS\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Поиск и добавление датасетов для доменов с малым количеством датасетов, с фильтрацией по тегам.\n",
    "\n",
    "    Args:\n",
    "        domain_distribution: Словарь с распределением датасетов по доменам {домен: кол-во}.\n",
    "        domain_tags: Словарь с тегами для каждого домена {домен: [теги]}.\n",
    "        min_datasets: Минимальное количество датасетов, ниже которого домен считается малочисленным.\n",
    "        max_new_datasets_per_domain: Максимальное количество новых датасетов для одного домена.\n",
    "        output_dir: Директория для сохранения датасетов.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(\"Starting enrichment of small domains\")\n",
    "\n",
    "    small_domains = {domain: count for domain, count in domain_distribution.items() if count < min_datasets}\n",
    "    logger.info(f\"Found {len(small_domains)} small domains: {small_domains}\")\n",
    "\n",
    "    if not small_domains:\n",
    "        logger.info(\"No small domains to enrich\")\n",
    "        return\n",
    "\n",
    "    processed_ids = set(os.listdir(output_dir))\n",
    "    logger.info(f\"Found {len(processed_ids)} already processed datasets\")\n",
    "\n",
    "    for domain, current_count in small_domains.items():\n",
    "        logger.info(f\"Enriching domain '{domain}' (current datasets: {current_count})\")\n",
    "        target_tags = set([normalize_tag(tag, apply_filtering=False) for tag in domain_tags.get(domain, []) if normalize_tag(tag, apply_filtering=False)])\n",
    "        if not target_tags:\n",
    "            logger.warning(f\"No tags found for domain '{domain}', skipping\")\n",
    "            continue\n",
    "\n",
    "        # Запрашиваем датасеты, отфильтрованные по тегам домена\n",
    "        potential_datasets = []\n",
    "        dataset_ids = set()\n",
    "        for tag in target_tags:\n",
    "            try:\n",
    "                # Используем кэширование для запросов по тегу\n",
    "                datasets = load_cached_datasets_by_tag(tag)\n",
    "                for record in datasets:\n",
    "                    did = record['did']\n",
    "                    if str(did) not in processed_ids and did not in dataset_ids:\n",
    "                        dataset_ids.add(did)\n",
    "                        potential_datasets.append((did, 1))  # Начальный вес = 1\n",
    "                time.sleep(0.05) \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error fetching datasets for tag '{tag}': {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        logger.info(f\"Found {len(potential_datasets)} potential datasets for '{domain}'\")\n",
    "\n",
    "        # Проверяем пересечение тегов для приоритизации\n",
    "        ranked_datasets = []\n",
    "        for did, _ in tqdm(potential_datasets, desc=f\"Ranking datasets for domain '{domain}'\"):\n",
    "            try:\n",
    "                dataset = openml.datasets.get_dataset(did, download_data=False)\n",
    "                tags = getattr(dataset, 'tag', []) or []\n",
    "                if not isinstance(tags, (list, tuple)):\n",
    "                    logger.warning(f\"Dataset {did} has invalid tags: {tags}, skipping\")\n",
    "                    continue\n",
    "                tags = set([normalize_tag(tag, apply_filtering=False) for tag in tags if normalize_tag(tag, apply_filtering=False)])\n",
    "                overlap = len(tags & target_tags)\n",
    "                if overlap > 0:\n",
    "                    ranked_datasets.append((did, overlap))\n",
    "                time.sleep(0.05) \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error checking dataset {did}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Сортируем по количеству совпадающих тегов и выбираем до max_new_datasets_per_domain\n",
    "        ranked_datasets.sort(key=lambda x: x[1], reverse=True)\n",
    "        selected_datasets = [did for did, _ in ranked_datasets[:max_new_datasets_per_domain]]\n",
    "        logger.info(f\"Selected {len(selected_datasets)} datasets for '{domain}'\")\n",
    "\n",
    "        new_datasets_added = 0\n",
    "        for dataset_id in tqdm(selected_datasets, desc=f\"Processing datasets for '{domain}'\"):\n",
    "            try:\n",
    "                result, tags = process_dataset(dataset_id)\n",
    "                if result:\n",
    "                    new_datasets_added += 1\n",
    "                    processed_ids.add(str(dataset_id))\n",
    "                    meta_path = os.path.join(output_dir, str(dataset_id), \"metadata.json\")\n",
    "                    with open(meta_path, 'r+', encoding='utf-8') as f:\n",
    "                        metadata = json.load(f)\n",
    "                        metadata['domain'] = domain\n",
    "                        metadata['cleaned_tags'] = filter_tags(tags)\n",
    "                        f.seek(0)\n",
    "                        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "                        f.truncate()\n",
    "                    logger.info(f\"Added dataset {dataset_id} to domain '{domain}'\")\n",
    "                time.sleep(0.5)  \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to process dataset {dataset_id}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        logger.info(f\"Added {new_datasets_added} new datasets to domain '{domain}'\")\n",
    "\n",
    "    logger.info(\"Enrichment completed\")\n",
    "\n",
    "def merge_small_clusters(clusters: Dict[int, List[str]], embeddings: np.ndarray, min_cluster_size: int, merge_threshold: float) -> Dict[int, List[str]]:\n",
    "    \"\"\"Объединение маленьких кластеров с семантически близкими крупными кластерами на основе косинусного расстояния\"\"\"\n",
    "    logger.info(f\"Initial number of clusters: {len(clusters)}\")\n",
    "    logger.info(f\"Cluster sizes before merging: { {k: len(v) for k, v in clusters.items()} }\")\n",
    "    cluster_centroids = {}\n",
    "    tag_to_index = {tag: idx for idx, tag in enumerate(load_and_filter_tags())}\n",
    "    for cluster_id, tags in clusters.items():\n",
    "        tag_indices = [tag_to_index[tag] for tag in tags if tag in tag_to_index]\n",
    "        if tag_indices:\n",
    "            cluster_centroids[cluster_id] = np.mean(embeddings[tag_indices], axis=0)\n",
    "        else:\n",
    "            cluster_centroids[cluster_id] = np.zeros(embeddings.shape[1])\n",
    "\n",
    "    small_clusters = {k: v for k, v in clusters.items() if len(v) < min_cluster_size}\n",
    "    large_clusters = {k: v for k, v in clusters.items() if len(v) >= min_cluster_size}\n",
    "    logger.info(f\"Small clusters (<{min_cluster_size} tags): {len(small_clusters)}\")\n",
    "    logger.info(f\"Large clusters (>= {min_cluster_size} tags): {len(large_clusters)}\")\n",
    "    \n",
    "    if not small_clusters or not large_clusters:\n",
    "        logger.info(\"No merging needed: either no small clusters or no large clusters.\")\n",
    "        return clusters\n",
    "\n",
    "    def cosine_distance(a: np.ndarray, b: np.ndarray) -> float:\n",
    "        if np.all(a == 0) or np.all(b == 0):\n",
    "            return 1.0\n",
    "        dot_product = np.dot(a, b)\n",
    "        norm_a = np.linalg.norm(a)\n",
    "        norm_b = np.linalg.norm(b)\n",
    "        return 1 - dot_product / (norm_a * norm_b)\n",
    "\n",
    "    merged_clusters = large_clusters.copy()\n",
    "    merged_count = 0\n",
    "    for small_id, small_tags in small_clusters.items():\n",
    "        min_distance = float('inf')\n",
    "        nearest_cluster_id = None\n",
    "        for large_id, _ in large_clusters.items():\n",
    "            distance = cosine_distance(cluster_centroids[small_id], cluster_centroids[large_id])\n",
    "            if distance < min_distance and distance <= merge_threshold:\n",
    "                min_distance = distance\n",
    "                nearest_cluster_id = large_id\n",
    "        if nearest_cluster_id is not None:\n",
    "            merged_clusters[nearest_cluster_id].extend(small_tags)\n",
    "            merged_count += 1\n",
    "        else:\n",
    "            merged_clusters[small_id] = small_tags\n",
    "    logger.info(f\"Merged {merged_count} small clusters into large ones.\")\n",
    "\n",
    "    final_clusters = {}\n",
    "    for idx, (old_id, tags) in enumerate(merged_clusters.items()):\n",
    "        final_clusters[idx] = list(set(tags))\n",
    "    logger.info(f\"Final number of clusters after merging: {len(final_clusters)}\")\n",
    "\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fefb3f8-a1e2-45e4-9338-55fc110d35e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Классы для клатсеризации\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Генерация векторных представлений для тегов\"\"\"\n",
    "    def __init__(self, method: str):\n",
    "        self.method = method\n",
    "        self.vectorizer = None\n",
    "        self.w2v_model = None\n",
    "\n",
    "    def generate(self, tags: List[str]) -> np.ndarray:\n",
    "        if self.method == \"sentence_transformers\":\n",
    "            return self._sentence_transformers(tags)\n",
    "        elif self.method == \"tfidf\":\n",
    "            return self._tfidf(tags)\n",
    "        elif self.method == \"gensim_word2vec\":\n",
    "            return self._gensim_word2vec(tags)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {self.method}\")\n",
    "\n",
    "    def _sentence_transformers(self, tags):\n",
    "        model = SentenceTransformer(EMBEDDING_METHODS[self.method][\"model\"])\n",
    "        return model.encode(tags, convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "    def _tfidf(self, tags):\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            max_features=EMBEDDING_METHODS[self.method][\"max_features\"],\n",
    "            tokenizer=lambda x: x.split(),\n",
    "            token_pattern=None\n",
    "        )\n",
    "        return self.vectorizer.fit_transform(tags).toarray()\n",
    "\n",
    "    def _gensim_word2vec(self, tags):\n",
    "        tokenized_tags = [tag.split() for tag in tags]\n",
    "        \n",
    "        self.w2v_model = Word2Vec(\n",
    "            sentences=tokenized_tags,\n",
    "            vector_size=EMBEDDING_METHODS[\"gensim_word2vec\"][\"vector_size\"],\n",
    "            window=EMBEDDING_METHODS[\"gensim_word2vec\"][\"window\"],\n",
    "            min_count=EMBEDDING_METHODS[\"gensim_word2vec\"][\"min_count\"],\n",
    "            workers=EMBEDDING_METHODS[\"gensim_word2vec\"][\"workers\"]\n",
    "        )\n",
    "        \n",
    "        embeddings = []\n",
    "        for tag in tags:\n",
    "            vectors = []\n",
    "            for word in tag.split():\n",
    "                if word in self.w2v_model.wv:\n",
    "                    vectors.append(self.w2v_model.wv[word])\n",
    "            if vectors:\n",
    "                embeddings.append(np.mean(vectors, axis=0))\n",
    "            else:\n",
    "                embeddings.append(np.zeros(\n",
    "                    EMBEDDING_METHODS[\"gensim_word2vec\"][\"dim\"]\n",
    "                ))\n",
    "        return np.array(embeddings)\n",
    "\n",
    "class ClusterAnalyzer:\n",
    "    \"\"\"Анализ и кластеризация тегов\"\"\"\n",
    "    def __init__(self, tags: List[str], embedding_method: str):\n",
    "        self.tags = tags\n",
    "        self.embedding_method = embedding_method\n",
    "        self.embeddings = None\n",
    "        self.results = {}\n",
    "\n",
    "    def generate_embeddings(self):\n",
    "        generator = EmbeddingGenerator(self.embedding_method)\n",
    "        self.embeddings = generator.generate(self.tags)\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine_distance(a: np.ndarray, b: np.ndarray) -> float:\n",
    "        \"\"\"Вычисляет косинусное расстояние между двумя векторами\"\"\"\n",
    "        if np.all(a == 0) or np.all(b == 0):\n",
    "            return 1.0\n",
    "        dot_product = np.dot(a, b)\n",
    "        norm_a = np.linalg.norm(a)\n",
    "        norm_b = np.linalg.norm(b)\n",
    "        return 1 - dot_product / (norm_a * norm_b)\n",
    "\n",
    "    def cluster_tags(self, method: str, min_cluster_size: int, merge_threshold: float) -> Dict:\n",
    "        config = CLUSTERING_METHODS[method]\n",
    "        \n",
    "        if method == \"kmeans\" and len(self.tags) < config[\"params\"][\"n_clusters\"]:\n",
    "            raise ValueError(f\"Not enough samples ({len(self.tags)}) for {method}\")\n",
    "            \n",
    "        clusterer = config[\"class\"](**config[\"params\"])\n",
    "        labels = clusterer.fit_predict(self.embeddings)\n",
    "        \n",
    "        valid_labels = set()\n",
    "        cluster_counts = {}\n",
    "        for label in set(labels):\n",
    "            if label == -1:\n",
    "                continue\n",
    "            count = np.sum(labels == label)\n",
    "            cluster_counts[label] = count\n",
    "            if count >= 2:\n",
    "                valid_labels.add(label)\n",
    "        \n",
    "        if not valid_labels:\n",
    "            labels = np.zeros(len(labels), dtype=labels.dtype)\n",
    "            cluster_centroids = {0: np.mean(self.embeddings, axis=0)}\n",
    "            valid_labels = {0}\n",
    "        else:\n",
    "            cluster_centroids = {}\n",
    "            for label in valid_labels:\n",
    "                cluster_indices = np.where(labels == label)[0]\n",
    "                cluster_embeddings = self.embeddings[cluster_indices]\n",
    "                cluster_centroids[label] = np.mean(cluster_embeddings, axis=0)\n",
    "            \n",
    "            for idx, label in enumerate(labels):\n",
    "                if label == -1 or (label != -1 and label not in valid_labels):\n",
    "                    point_embedding = self.embeddings[idx]\n",
    "                    min_distance = float('inf')\n",
    "                    nearest_cluster = -1\n",
    "                    \n",
    "                    for cluster_label, centroid in cluster_centroids.items():\n",
    "                        distance = self.cosine_distance(point_embedding, centroid)\n",
    "                        if distance < min_distance:\n",
    "                            min_distance = distance\n",
    "                            nearest_cluster = cluster_label\n",
    "                    \n",
    "                    if nearest_cluster != -1:\n",
    "                        labels[idx] = nearest_cluster\n",
    "                    else:\n",
    "                        labels[idx] = next(iter(valid_labels))\n",
    "        \n",
    "        clusters = defaultdict(list)\n",
    "        for idx, label in enumerate(labels):\n",
    "            label_int = int(label)\n",
    "            clusters[label_int].append(self.tags[idx])\n",
    "        \n",
    "        logger.info(f\"Cluster sizes before merging: { {k: len(v) for k, v in clusters.items()} }\")\n",
    "        clusters = merge_small_clusters(dict(clusters), self.embeddings, min_cluster_size, merge_threshold)\n",
    "        \n",
    "        metrics = self._calculate_metrics(labels)\n",
    "        \n",
    "        return {\n",
    "            \"clusters\": clusters,\n",
    "            \"metrics\": metrics,\n",
    "            \"params\": config[\"params\"]\n",
    "        }\n",
    "\n",
    "    def _calculate_metrics(self, labels):\n",
    "        \"\"\"Вычисление метрик качества кластеризации\"\"\"\n",
    "        unique_labels = len(set(labels))\n",
    "        metrics = {}\n",
    "        \n",
    "        if unique_labels > 1:\n",
    "            try:\n",
    "                metrics[\"silhouette\"] = silhouette_score(self.embeddings, labels)\n",
    "                metrics[\"davies_bouldin\"] = davies_bouldin_score(self.embeddings, labels)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Metric calculation failed: {str(e)}\")\n",
    "        \n",
    "        metrics[\"n_clusters\"] = unique_labels - (1 if -1 in labels else 0)\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f005048-28b3-445c-9c51-0966dafa9878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции для разметки датасетов и сохранения результатов\n",
    "def manual_annotate_clusters(clusters: Dict[int, List[str]], callback: Callable[[Dict], None]):\n",
    "    \"\"\"Интерактивная разметка кластеров пользователем с использованием виджетов\"\"\"\n",
    "    annotated_domains = {}\n",
    "    sorted_clusters = sorted(clusters.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    output_area = widgets.Output()\n",
    "    result_container = widgets.Output()\n",
    "    display(output_area, result_container)\n",
    "    \n",
    "    domain_input = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Введите имя домена',\n",
    "        description='Имя домена:',\n",
    "        layout=widgets.Layout(width='500px')\n",
    "    )\n",
    "    \n",
    "    submit_button = widgets.Button(description=\"Подтвердить\", button_style='success')\n",
    "    skip_button = widgets.Button(description=\"Пропустить\", button_style='')\n",
    "    progress = widgets.IntProgress(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=len(sorted_clusters),\n",
    "        description='Прогресс:',\n",
    "        bar_style='info',\n",
    "        style={'bar_color': '#4CAF50'}\n",
    "    )\n",
    "    \n",
    "    buttons_box = widgets.HBox([domain_input, submit_button, skip_button])\n",
    "    display(buttons_box, progress)\n",
    "    \n",
    "    current_idx = [0]\n",
    "    \n",
    "    def show_cluster():\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "            if current_idx[0] < len(sorted_clusters):\n",
    "                cluster_id, tags = sorted_clusters[current_idx[0]]\n",
    "                unique_tags = list(set(tags))\n",
    "                print(f\"Кластер #{cluster_id} ({len(unique_tags)} уникальных тегов):\")\n",
    "                print(\"-\"*50)\n",
    "                for i, tag in enumerate(unique_tags[:10]):\n",
    "                    print(f\"{i+1}. {tag}\")\n",
    "                if len(unique_tags) > 10:\n",
    "                    print(f\"... и еще {len(unique_tags)-10} тегов\")\n",
    "            else:\n",
    "                print(\"Все кластеры обработаны!\")\n",
    "                submit_button.disabled = True\n",
    "                skip_button.disabled = True\n",
    "                domain_input.disabled = True\n",
    "                callback(annotated_domains)\n",
    "    \n",
    "    def handle_submit(b):\n",
    "        domain = domain_input.value.strip()\n",
    "        cluster_id, tags = sorted_clusters[current_idx[0]]\n",
    "        \n",
    "        if domain and domain.lower() != 'skip':\n",
    "            unique_tags = list(set(tags))\n",
    "            annotated_domains[domain] = unique_tags\n",
    "            with result_container:\n",
    "                result_container.clear_output()\n",
    "                print(f\"Кластер #{cluster_id} сохранен как: {domain}\")\n",
    "        else:\n",
    "            with result_container:\n",
    "                result_container.clear_output()\n",
    "                print(f\"Кластер #{cluster_id} пропущен\")\n",
    "        \n",
    "        domain_input.value = ''\n",
    "        current_idx[0] += 1\n",
    "        progress.value = current_idx[0]\n",
    "        show_cluster()\n",
    "\n",
    "    def handle_skip(b):\n",
    "        cluster_id, tags = sorted_clusters[current_idx[0]]\n",
    "        \n",
    "        with result_container:\n",
    "            result_container.clear_output()\n",
    "            print(f\"Кластер #{cluster_id} пропущен\")\n",
    "        \n",
    "        domain_input.value = ''\n",
    "        current_idx[0] += 1\n",
    "        progress.value = current_idx[0]\n",
    "        show_cluster()\n",
    "\n",
    "    submit_button.on_click(handle_submit)\n",
    "    skip_button.on_click(handle_skip)\n",
    "    \n",
    "    show_cluster()\n",
    "\n",
    "def assign_unused_domains(final_domains: Dict, input_dir: str = INPUT_DIR):\n",
    "    \"\"\"Гарантия того, что каждый домен будет назначен хотя бы одному датасету\"\"\"\n",
    "    logger.info(\"Checking for unused domains\")\n",
    "    \n",
    "    dataset_info = {}\n",
    "    used_domains = set()\n",
    "    \n",
    "    for dataset_id in os.listdir(input_dir):\n",
    "        meta_path = os.path.join(input_dir, dataset_id, \"metadata.json\")\n",
    "        if not os.path.exists(meta_path):\n",
    "            logger.warning(f\"Metadata file not found for dataset {dataset_id}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            with open(meta_path, 'r', encoding='utf-8') as f:\n",
    "                metadata = json.load(f)\n",
    "                tags = set(metadata.get('cleaned_tags', []))\n",
    "                domain = metadata.get('domain', 'Unknown')\n",
    "                dataset_info[dataset_id] = {'tags': tags, 'domain': domain}\n",
    "                used_domains.add(domain)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to read metadata for dataset {dataset_id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    all_domains = set(final_domains.keys())\n",
    "    unused_domains = all_domains - used_domains\n",
    "    logger.info(f\"Found {len(unused_domains)} unused domains: {unused_domains}\")\n",
    "    \n",
    "    if not unused_domains:\n",
    "        logger.info(\"All domains are used\")\n",
    "        return\n",
    "    \n",
    "    assignment_report = []\n",
    "    \n",
    "    for domain in unused_domains:\n",
    "        domain_tags = set([normalize_tag(t, apply_filtering=False) for t in final_domains[domain] if normalize_tag(t, apply_filtering=False)])\n",
    "        logger.info(f\"Processing unused domain '{domain}' with tags: {domain_tags}\")\n",
    "        best_dataset = None\n",
    "        max_overlap = 0\n",
    "        \n",
    "        for dataset_id, info in dataset_info.items():\n",
    "            overlap = len(domain_tags & info['tags'])\n",
    "            logger.info(f\"Unused domain '{domain}' vs Dataset {dataset_id}: overlap = {overlap}\")\n",
    "            if overlap > max_overlap:\n",
    "                max_overlap = overlap\n",
    "                best_dataset = dataset_id\n",
    "        \n",
    "        if not best_dataset:\n",
    "            best_dataset = random.choice(list(dataset_info.keys()))\n",
    "            logger.info(f\"No overlap for domain '{domain}', assigning to random dataset {best_dataset}\")\n",
    "        \n",
    "        try:\n",
    "            meta_path = os.path.join(input_dir, best_dataset, \"metadata.json\")\n",
    "            with open(meta_path, 'r+', encoding='utf-8') as f:\n",
    "                metadata = json.load(f)\n",
    "                previous_domain = metadata.get('domain', 'None')\n",
    "                metadata['domain'] = domain\n",
    "                f.seek(0)\n",
    "                json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "                f.truncate()\n",
    "                logger.info(f\"Assigned unused domain '{domain}' to dataset {best_dataset} (overlap: {max_overlap}, previous: '{previous_domain}')\")\n",
    "                \n",
    "                assignment_report.append({\n",
    "                    'domain': domain,\n",
    "                    'dataset': best_dataset,\n",
    "                    'overlap': max_overlap\n",
    "                })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to assign domain '{domain}' to dataset {best_dataset}: {str(e)}\")\n",
    "    \n",
    "    if assignment_report:\n",
    "        logger.info(\"Assignments for unused domains:\")\n",
    "        for assignment in assignment_report:\n",
    "            logger.info(f\"- Domain '{assignment['domain']}' assigned to dataset '{assignment['dataset']}' (overlap: {assignment['overlap']})\")\n",
    "\n",
    "def update_metadata(final_domains: Dict):\n",
    "    \"\"\"Обновление метаданных датасетов на основе разметки доменов\"\"\"\n",
    "    logger.info(f\"Starting metadata update with {len(final_domains)} domains: {list(final_domains.keys())}\")\n",
    "    \n",
    "    normalized_domains = {}\n",
    "    for domain, tags in final_domains.items():\n",
    "        unique_tags = set()\n",
    "        for t in tags:\n",
    "            normalized = normalize_tag(t, apply_filtering=False)\n",
    "            if normalized:\n",
    "                unique_tags.add(normalized)\n",
    "        normalized_domains[domain] = list(unique_tags)\n",
    "        logger.info(f\"Domain '{domain}': {len(unique_tags)} unique tags: {unique_tags}\")\n",
    "    \n",
    "    updated_datasets = 0\n",
    "    for dataset_id in tqdm(os.listdir(INPUT_DIR), desc=\"Updating Metadata\"):\n",
    "        meta_path = os.path.join(INPUT_DIR, dataset_id, \"metadata.json\")\n",
    "        if not os.path.exists(meta_path):\n",
    "            logger.warning(f\"Metadata file not found for dataset {dataset_id}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            with open(meta_path, 'r+', encoding='utf-8') as f:\n",
    "                metadata = json.load(f)\n",
    "                \n",
    "                raw_tags = metadata.get('tags', [])\n",
    "                filtered_tags = filter_tags(raw_tags)\n",
    "                logger.info(f\"Dataset {dataset_id}: {len(raw_tags)} raw tags: {raw_tags}\")\n",
    "                logger.info(f\"Dataset {dataset_id}: {len(filtered_tags)} filtered tags: {filtered_tags}\")\n",
    "                \n",
    "                best_domain = None\n",
    "                max_overlap = 0\n",
    "                \n",
    "                for domain, domain_tags in normalized_domains.items():\n",
    "                    overlap = len(set(filtered_tags) & set(domain_tags))\n",
    "                    logger.info(f\"Dataset {dataset_id} vs Domain '{domain}': overlap = {overlap}\")\n",
    "                    if overlap > max_overlap:\n",
    "                        max_overlap = overlap\n",
    "                        best_domain = domain\n",
    "                \n",
    "                previous_domain = metadata.get('domain', 'None')\n",
    "                metadata['domain'] = best_domain if best_domain and max_overlap > 0 else \"Other\"\n",
    "                metadata['cleaned_tags'] = filtered_tags\n",
    "                \n",
    "                logger.info(f\"Dataset {dataset_id}: assigned domain '{metadata['domain']}' (previous: '{previous_domain}', overlap: {max_overlap})\")\n",
    "                \n",
    "                f.seek(0)\n",
    "                json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "                f.truncate()\n",
    "                updated_datasets += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to update metadata for dataset {dataset_id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Updated metadata for {updated_datasets} datasets\")\n",
    "    \n",
    "    assign_unused_domains(final_domains)\n",
    "\n",
    "def calculate_domain_distribution(annotated_domains: Dict[str, List[str]], input_dir: str = INPUT_DIR) -> tuple:\n",
    "    \"\"\"\n",
    "    Рассчёт распределения датасетов по доменам.\n",
    "\n",
    "    Args:\n",
    "        annotated_domains: Словарь с тегами для каждого домена.\n",
    "        input_dir: Директория с датасетами.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (распределение в виде строки, путь к файлу статистики, словарь распределения)\n",
    "    \"\"\"\n",
    "    domain_count = defaultdict(int)\n",
    "    \n",
    "    for dataset_id in tqdm(os.listdir(input_dir), desc=\"Calculating Distribution\"):\n",
    "        meta_path = os.path.join(input_dir, dataset_id, \"metadata.json\")\n",
    "        if not os.path.exists(meta_path):\n",
    "            continue\n",
    "            \n",
    "        with open(meta_path, 'r', encoding='utf-8') as f:\n",
    "            metadata = json.load(f)\n",
    "            domain = metadata.get('domain', 'Unknown')\n",
    "            domain_count[domain] += 1\n",
    "    \n",
    "    sorted_distribution = sorted(\n",
    "        domain_count.items(), \n",
    "        key=lambda x: x[1], \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    distribution_str = \"\\n Распределение датасетов по доменам:\\n\"\n",
    "    distribution_str += \"-\" * 50 + \"\\n\"\n",
    "    for domain, count in sorted_distribution:\n",
    "        distribution_str += f\"{domain}: {count} датасетов\\n\"\n",
    "    \n",
    "    stats_file = os.path.join(OUTPUT_DIR, \"domain_distribution.txt\")\n",
    "    with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(distribution_str)\n",
    "    \n",
    "    return distribution_str, stats_file, dict(domain_count)\n",
    "\n",
    "def select_best_result(all_results: Dict) -> tuple:\n",
    "    \"\"\"Выбор лучшего результата по метрике силуэта\"\"\"\n",
    "    best_score = -1\n",
    "    best_emb = None\n",
    "    best_clust = None\n",
    "    \n",
    "    for emb_method, results in all_results.items():\n",
    "        for clust_method, metrics in results.items():\n",
    "            if 'silhouette' in metrics and metrics['silhouette'] > best_score:\n",
    "                best_score = metrics['silhouette']\n",
    "                best_emb = emb_method\n",
    "                best_clust = clust_method\n",
    "    \n",
    "    return best_emb, best_clust\n",
    "\n",
    "def run_clustering(embedding_method=None, clustering_method=None):\n",
    "    \"\"\"Основная функция для запуска кластеризации с объединением маленьких кластеров\"\"\"\n",
    "    MIN_CLUSTER_SIZE = 3\n",
    "    MERGE_THRESHOLD = 0.6\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    tags = load_and_filter_tags()\n",
    "    \n",
    "    if len(tags) < 2:\n",
    "        logger.error(\"Not enough data for clustering\")\n",
    "        return None\n",
    "\n",
    "    all_results = {}\n",
    "    cluster_files = {}\n",
    "\n",
    "    if embedding_method and clustering_method:\n",
    "        emb_methods = [embedding_method]\n",
    "        clust_methods = [clustering_method]\n",
    "        user_selected = True\n",
    "    else:\n",
    "        emb_methods = EMBEDDING_METHODS.keys()\n",
    "        clust_methods = CLUSTERING_METHODS.keys()\n",
    "        user_selected = False\n",
    "\n",
    "    for emb_method in emb_methods:\n",
    "        logger.info(f\"\\n{'='*50}\")\n",
    "        logger.info(f\"Processing embeddings: {emb_method.upper()}\")\n",
    "        \n",
    "        try:\n",
    "            analyzer = ClusterAnalyzer(tags, emb_method)\n",
    "            analyzer.generate_embeddings()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Embedding failed: {str(e)}\")\n",
    "            continue\n",
    "        \n",
    "        results = {}\n",
    "        for clust_method in clust_methods:\n",
    "            try:\n",
    "                result = analyzer.cluster_tags(clust_method, MIN_CLUSTER_SIZE, MERGE_THRESHOLD)\n",
    "                results[clust_method] = result[\"metrics\"]\n",
    "                \n",
    "                cluster_file = f\"{OUTPUT_DIR}/{emb_method}_{clust_method}_clusters.json\"\n",
    "                with open(cluster_file, 'w') as f:\n",
    "                    json.dump(result[\"clusters\"], f, indent=2)\n",
    "                \n",
    "                cluster_files[(emb_method, clust_method)] = cluster_file\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Clustering failed: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if results:\n",
    "            all_results[emb_method] = results\n",
    "            logger.info(f\"Results for {emb_method}: {results}\")\n",
    "\n",
    "    if user_selected:\n",
    "        best_emb = embedding_method\n",
    "        best_clust = clustering_method\n",
    "        logger.info(f\"Using selected method: {best_emb} + {best_clust}\")\n",
    "    else:\n",
    "        best_emb, best_clust = select_best_result(all_results)\n",
    "        if not best_emb:\n",
    "            logger.error(\"No valid clustering results found\")\n",
    "            return None\n",
    "        logger.info(f\"Best method: {best_emb} + {best_clust}\")\n",
    "    \n",
    "    cluster_file = cluster_files.get((best_emb, best_clust))\n",
    "    if not cluster_file or not os.path.exists(cluster_file):\n",
    "        logger.error(\"Cluster file not found\")\n",
    "        return None\n",
    "    \n",
    "    with open(cluster_file, 'r') as f:\n",
    "        clusters = json.load(f)\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "def save_domain_assignments(annotated_domains: Dict[str, List[str]]):\n",
    "    \"\"\"Сохранение разметки доменов и обновление метаданных\"\"\"\n",
    "    output_file = DOMAIN_MAPPING_FILE\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(annotated_domains, f, indent=2, ensure_ascii=False)\n",
    "    logger.info(f\"Domain assignments saved to {os.path.abspath(output_file)}\")\n",
    "    \n",
    "    update_metadata(annotated_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8554f9-00b2-47d9-8a33-1f2113d26685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clustering_interface():\n",
    "    \"\"\"Создание интерактивного интерфейса для кластеризации и разметки\"\"\"\n",
    "    embedding_selector = widgets.Dropdown(\n",
    "        options=list(EMBEDDING_METHODS.keys()),\n",
    "        value='sentence_transformers',\n",
    "        description='Embedding:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    clustering_selector = widgets.Dropdown(\n",
    "        options=list(CLUSTERING_METHODS.keys()),\n",
    "        value='hdbscan',\n",
    "        description='Clustering:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    min_datasets_input = widgets.IntText(\n",
    "        value=5,\n",
    "        description='Мин. датасетов:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '200px'}\n",
    "    )\n",
    "\n",
    "    max_new_datasets_per_domain_input = widgets.IntText(\n",
    "        value=10,\n",
    "        description='Макс. новых на домен:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout={'width': '200px'}\n",
    "    )\n",
    "\n",
    "    run_button = widgets.Button(\n",
    "        description=\"Запустить кластеризацию\",\n",
    "        button_style='success',\n",
    "        tooltip='Запустить процесс кластеризации'\n",
    "    )\n",
    "    \n",
    "    annotate_button = widgets.Button(\n",
    "        description=\"Начать разметку\",\n",
    "        button_style='primary',\n",
    "        disabled=True,\n",
    "        tooltip='Начать ручную разметку доменов'\n",
    "    )\n",
    "    \n",
    "    save_button = widgets.Button(\n",
    "        description=\"Сохранить результаты\",\n",
    "        button_style='info',\n",
    "        disabled=True,\n",
    "        tooltip='Сохранить разметку и обновить метаданные'\n",
    "    )\n",
    "    \n",
    "    enrich_button = widgets.Button(\n",
    "        description=\"Пополнить домены\",\n",
    "        button_style='warning',\n",
    "        disabled=True,\n",
    "        tooltip='Добавить датасеты в маленькие домены'\n",
    "    )\n",
    "    \n",
    "    domain_selector = widgets.SelectMultiple(\n",
    "        options=[],\n",
    "        description='Домен для пополнения:',\n",
    "        style={'description_width': 'initial'},\n",
    "        disabled=True,\n",
    "        layout={'height': '100px'}\n",
    "    )\n",
    "\n",
    "    output_area = widgets.Output()\n",
    "    results_output = widgets.Output()\n",
    "    \n",
    "    current_clusters = None\n",
    "    annotated_domains = None\n",
    "\n",
    "    def on_run_button_clicked(b):\n",
    "        nonlocal current_clusters\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "            print(\"Запуск кластеризации...\")\n",
    "            current_clusters = run_clustering(\n",
    "                embedding_selector.value,\n",
    "                clustering_selector.value\n",
    "            )\n",
    "            if current_clusters:\n",
    "                print(f\"Кластеризация завершена! Получено {len(current_clusters)} кластеров.\")\n",
    "                annotate_button.disabled = False\n",
    "                save_button.disabled = True\n",
    "                enrich_button.disabled = True\n",
    "                domain_selector.disabled = True\n",
    "            else:\n",
    "                print(\"Ошибка кластеризации. Проверьте логи.\")\n",
    "\n",
    "    def on_annotate_button_clicked(b):\n",
    "        nonlocal annotated_domains\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "            if current_clusters:\n",
    "                print(\"Начало ручной разметки...\")\n",
    "                \n",
    "                def on_annotation_complete(result):\n",
    "                    nonlocal annotated_domains\n",
    "                    annotated_domains = result\n",
    "                    print(f\"\\nРазметка завершена! Создано {len(annotated_domains)} доменов.\")\n",
    "                    \n",
    "                    with results_output:\n",
    "                        results_output.clear_output()\n",
    "                        print(\"\\nСозданные домены:\")\n",
    "                        for domain, tags in annotated_domains.items():\n",
    "                            print(f\"\\n{domain} ({len(tags)} тегов):\")\n",
    "                            print(\", \".join(tags[:5]) + (\"...\" if len(tags) > 5 else \"\"))\n",
    "                    \n",
    "                    save_button.disabled = False\n",
    "                    enrich_button.disabled = False\n",
    "                    domain_selector.options = list(annotated_domains.keys())\n",
    "                    domain_selector.disabled = False\n",
    "                \n",
    "                manual_annotate_clusters(current_clusters, on_annotation_complete)\n",
    "            else:\n",
    "                print(\"Сначала выполните кластеризацию!\")\n",
    "\n",
    "    def on_save_button_clicked(b):\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "            if annotated_domains:\n",
    "                print(\"Сохранение результатов...\")\n",
    "                save_domain_assignments(annotated_domains)\n",
    "                print(\"Результаты сохранены!\")\n",
    "                distribution, stats_file, domain_count = calculate_domain_distribution(annotated_domains)\n",
    "                print(distribution)\n",
    "                print(f\"\\nСтатистика сохранена в {stats_file}\")\n",
    "            else:\n",
    "                print(\"Сначала выполните разметку!\")\n",
    "\n",
    "    def on_enrich_button_clicked(b):\n",
    "        with output_area:\n",
    "            output_area.clear_output()\n",
    "            if annotated_domains:\n",
    "                selected_domains = list(domain_selector.value)\n",
    "                if not selected_domains:\n",
    "                    selected_domains = list(annotated_domains.keys())\n",
    "                print(f\"Пополнение доменов: {', '.join(selected_domains)}\")\n",
    "                distribution, stats_file, domain_count = calculate_domain_distribution(annotated_domains)\n",
    "                enrich_small_domains(\n",
    "                    domain_count,  \n",
    "                    annotated_domains,\n",
    "                    min_datasets=min_datasets_input.value,\n",
    "                    max_new_datasets_per_domain=max_new_datasets_per_domain_input.value,\n",
    "                    output_dir=OUTPUT_DIR_DATASETS\n",
    "                )\n",
    "                print(\"Пополнение завершено!\")\n",
    "                distribution, stats_file, domain_count = calculate_domain_distribution(annotated_domains)\n",
    "                print(distribution)\n",
    "                print(f\"\\nСтатистика сохранена в {stats_file}\")\n",
    "            else:\n",
    "                print(\"Сначала выполните разметку!\")\n",
    "\n",
    "    run_button.on_click(on_run_button_clicked)\n",
    "    annotate_button.on_click(on_annotate_button_clicked)\n",
    "    save_button.on_click(on_save_button_clicked)\n",
    "    enrich_button.on_click(on_enrich_button_clicked)\n",
    "\n",
    "    display(widgets.VBox([\n",
    "        embedding_selector,\n",
    "        clustering_selector,\n",
    "        min_datasets_input,\n",
    "        max_new_datasets_per_domain_input,\n",
    "        widgets.HBox([run_button, annotate_button, save_button, enrich_button]),\n",
    "        domain_selector,\n",
    "        output_area,\n",
    "        results_output\n",
    "    ]))\n",
    "\n",
    "def run_domain_mapping():\n",
    "    \"\"\"Запуск всего пайплайна создания доменов и разметки датасетов\"\"\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(\"Используйте элементы управления ниже\")\n",
    "    create_clustering_interface()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
